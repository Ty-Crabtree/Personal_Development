{"[Auto] Clock in sync with NTP": "Alert", "Pingdom Uptime": "OK", "Stan's test monitor": "OK", "Demo": "OK", "DataDod Agent Service | CRITICAL STATE |": "No Data", "General Disk Usage Alert--SCN": "No Data", "Check root partition diskspace--SCN": "No Data", "SSL Cert Expiration Warning": "Warn", "SqlServer blocking on {{host.name}}": "No Data", "Kafka Monitoring on AMT-DEMO": "No Data", "Disk Usage is high on {{host.name}}": "OK", "Cpu Utilization": "OK", "vSphere_Mem-Swapped Alert": "OK", "vSphere_Mem-Ballooned Alert": "OK", "vSphere_CPU-Ready Alert": "Alert", "vSphere_vCenter _ Pittock _ Connection Check Alert": "OK", "vSphere_CPU Usage AVG Outlier on Production Host": "Alert", "vSphere_MEM Outlier on Production Host": "OK", "[ProCloud Watchdog] {{event.title}}": "OK", "smfprocessor.service-sql-server p95 latency on env:production": "OK", "vSphere_CPU Ready AVG is High on Production Host": "Alert", "Storm topology -->  itm_primary_disaster_recovery_topology": "OK", "Storm topology  --> supervision_topology": "Alert", "Storm topology --> archive_export topology": "OK", "Storm topology --> job_processing_topology": "Alert", "Storm topology reindex_topology is down": "OK", "Storm topology -->  itm_ingestion_topology": "OK", "Disk utilization Schwab Prod:KAFKA Instances": "OK", "Mercury Service is NOT Running on {{host.name}}": "No Data", "RabbitMQ file descriptors on {{deployment.name}} too high": "OK", "RabbitMQ memory usage is high on Schwab UAT": "OK", "RabbitMQ CPU is high on Schwab UAT": "OK", "RabbitMQ ephemeral disk usage is high on Schwab UAT": "OK", "RabbitMQ persistent disk usage high on Schwab UAT": "OK", "RabbitMQ memory alert went off for {{deployment.name}}": "OK", "RabbitMQ CPU is high on Schwab Prod": "OK", "RabbitMQ ephemeral disk usage is high on Schwab Prod": "OK", "RabbitMQ memory usage is high on Schwab Prod": "OK", "RabbitMQ persistent disk usage high on Schwab Prod": "OK", "Kafka process times out of expected band for Schwab in {{deployment.name}}": "OK", "Disk Usage is High {{host.name}} in aws kafka dev space": "OK", "SQL Server Login Failures": "OK", "Memory Utilization for AWS-TEST SPACE in {{deployment.name}} on {{host.name}} is very high": "No Data", "Zookeepeer for AWS-TEST SPACE on {{host.name}}": "No Data", "Host Down {{host.name}}": "No Data", "AWS Availability Global": "OK", "AWS EC2 Service alert": "OK", "Memory space status on  for AWS-TEST in {{deployment.name}} on {{host.name}}": "No Data", "Solr Node data partition filling": "Warn", "Network - Cogent link latency": "OK", "Network - CTL 441373207 link latency": "OK", "Network - CTL 441373208 link latency": "OK", "jpmc-apac: Host or Datadog agent is down on {{host.name}}": "OK", "jpmc-apdr: Host or Datadog agent is down on {{host.name}}": "Alert", "jpmc-apuat: Host or Datadog agent is down on {{host.name}}": "OK", "jpmc-namdr: Host or Datadog agent is down on {{host.name}}": "OK", "jpmc-namuat: Host or Datadog agent is down on {{host.name}}": "OK", "jpmc-emea: Host or Datadog agent is down on {{host.name}}": "OK", "jpmc-emdr: Host or Datadog agent is down on {{host.name}}": "OK", "jpmc-emuat: Host or Datadog agent is down on {{host.name}}": "OK", "Index Lag jpus01": "OK", "Disk_Usages_data1_logs space status on  for JPMC in {{deployment.name}} on {{host.name}}": "No Data", "Disk usage is high  JPMC  Hosts {{host}}  env {{env}}": "Alert", "JPMC: Much time taking for writting/reading to disk {{host}} env {{env}} IP {{ip}}": "OK", "SmfProcessor Available Disk Space": "OK", "Kafka process times out of expected band for BNY in {{deployment.name}}": "Alert", "Kafka active controller count for aws-principal Customer": "No Data", "EGW: Schwab Prod - RabbitMQ Queue building up issue": "OK", "EGW: MT Prod - RabbitMQ Queue building up issue": "OK", "Inodes usage is high on Customer: {{customer}} | Host: {{host}} | Env: {{env}}": "OK", "pit-infragraf-01 Disk Usage": "OK", "[AMQ Store Usage Alert] {{host.bosh_name}} for role {{host.role}}": "Alert", "Low Internet Bandwidth Utilization": "OK", "CTL 441373208 Circuit Bandwidth Usage": "OK", "CTL 441373207 Circuit Bandwidth Usage": "OK", "Cogent Circuit Bandwidth Usage": "OK", "Resource ingestion-api has a high p90 latency on env:none": "No Data", "Resource ingestion-api has an abnormal change in throughput on env:none": "OK", "Resource ingestion-api has a high error rate on env:none": "OK", "Resource ingestion-api has a high average latency on env:none": "OK", "ProArchive Infrastructure: Quincy Link Bandwidth High": "OK", "PIT-CHATFS-03 Storage Anomoly Detection": "OK", "ProArch Database Monitor - Backup Alert": "OK", "Solr Node Master {{host.name}} failed to check in": "OK", "TEST  - Average Solr Internal Search Errors over 1/second": "OK", "TEST  - Average Solr Client Search Errors over 1/second": "OK", "ProArch Database Monitor - Host": "No Data", "ProArch Database Monitor - SQL Server Instance": "OK", "JPMC PROD UI is not 100% accessible on Env: {{env.name}} during last 24 hours": "Alert", "JPMC PROD UI response time is more on Env: {{env.name}} during last 24 hours": "OK", "Solr Node data partition filling (secondary slave)": "Warn", "Solr Node CPU spike": "OK", "ProArch Database Monitor - CPU Utilization on {{host.name}}": "No Data", "ProArch Database Monitor - Disk Space on {{host.name}} for {{device.name}}": "No Data", "ProArch Database Monitor - Available RAM on {{host.name}}": "No Data", "ProArch Database Monitor - SQL Server Agent Job Failed": "OK", "QCY-PIT Spine 1 Link Latency": "OK", "QCY-PIT Spine 2 Link Latency": "OK", "VCOR Email Processing Pipeline Alert": "OK", "VCOR IM Processing Pipeline Alert": "OK", "AMQ Store Increase Alert": "OK", "SMTP is NOT reachable on one or more linux journaling servers.": "OK", "[Synthetics] Test on shda-dev.cc.smarsh.cloud/manage/health": "OK", "PROD03 Disk Usage is  high  Env:{{env}}, Host:{{host}},  IP: {{ip}}, device: {{device}} hostGroup {{host_group}}": "No Data", "egw files setting older than 24 hrs {{value}} {{comparator}} {{threshold}} for env: {{env.name}} from last 4 hour": "Alert", "LFS  monitoring  {{value}} {{comparator}} {{threshold}} for env: {{env.name}}": "Alert", "PIT-CHATFS-03 Storage Forecast Alert": "OK", "Job Scheduler Disabled {{env.name}}": "No Data", "Job Scheduler is Disabled  {{event.tags.env}}": "OK", "Egw files sitting older than 24 hrs from last 4 hour": "Alert", "Files younger than 24 Hours sitting on EGW": "No Data", "WebArch UAT DB Host Monitor": "OK", "WebArch Database Monitor -MariaDB Instance": "OK", "Host CPU > 90% over 2 hours.": "OK", "Website Archiving Memcached Has Stopped!!": "No Data", "[Social] IM Ingestion Alert": "OK", "ProArch Database Monitor - SQL Server Instance Service SQL3 is down": "No Data", "ProArch Database Monitor - SQL Server Agent Service SQL3 is down": "No Data", "ProArch Database Monitor - SQL Server Instance Service COMPDB is down": "No Data", "ProArch Database Monitor - SQL Server Agent Service COMPDB is down": "No Data", "PIT-CHATFS-03 Has Low Storage Capacity!": "OK", "ProArch Database Monitor - SQL Server Instance Service SESSDB is down": "No Data", "ProArch Database Monitor - SQL Server Agent Service SESSDB is down": "No Data", "ProArch Database Monitor - SQL Server Instance Service {{host.instance}} is down on {{host.name}}": "No Data", "ProArch Database Monitor - SQL Server Agent Service {{host.instance}} is down on {{host.name}}": "No Data", "ProArch Database Monitor - Cluster Service down on {{host.name}}": "No Data", "Karaf service check": "OK", "SlackLive SMF Processor is down!": "OK", "ProArch Database Monitor - SQL Server Reporting Service {{host.instance}} is down on {{host.name}}": "No Data", "ProArch Database Monitor - SQL Server Integration Service {{host.instance}} is down on {{host.name}}": "No Data", "ProArchive IM Ingestion - Twitter Service": "Alert", "ProArchive IM Ingestion - Twitter": "OK", "Service supervision_topology has a high p90 latency on env:none": "No Data", "Service supervision_topology has an abnormal change in throughput on env:none": "No Data", "Service supervision_topology has a high error rate on env:none": "No Data", "Service supervision_topology has a high average latency on env:none": "No Data", "Storm CPU load is very High on {{host.ip}} {{env.name}}": "Alert", "Memory used  is very High on {{host.ip}} {{env.name}}": "OK", "Low Teams Ingestion": "OK", "AT&T Low Ingestion": "OK", "Low Chatter Ingestion": "OK", "Linkedin Low Ingestion": "OK", "Low TextGuard Ingestion!": "OK", "Terraform MongoDB Disk Usage is high Customer:{{customer.name}} Hosts:{{host}} env:{{env}} Clustername:{{cluster_name.name}} Device:{{device.name}}": "Alert", "EGW: MT Prod - RabbitMQ EGW-Incoming Queue building up": "OK", "EGW: MT Prod - RabbitMQ EGW-NewRelic Event Queue building up": "OK", "EGW: MT Prod - RabbitMQ EGW-AAC Event Queue building up": "OK", "AWS Instance Event Detected": "OK", "NFS HA Highly Available Cluster Health Check": "OK", "Service ea-ehaas has a high p90 latency on env:mrm-dev": "OK", "Bloomberg Archiver is not running on {{host.name}}": "OK", "CPU high on {{host.name}}  of Vantage Cloud Instance": "OK", "Concourse Prod Down": "Alert", "BNY workers are not reporting their state": "No Data", "[Synthetics] [ProArchive] - SMC - Basic Search Test": "OK", "Latency for non error response code is higher": "Alert", "Concourse: Disk Load is high on AWS-Prod {{host.name}}": "OK", "[ProArchive] [IM] ProArchive IM Platform component degraded/offline": "OK", "[ProArchive] [Email] ProArchive Email Platform component degraded/offline": "OK", "[ProArchive] [Bloomberg] ProArchive Bloomberg Platform component degraded/offline": "OK", "[ProArchive] [Exports] ProArchive Exports Platform component degraded/offline": "OK", "[ProArchive] [Search] ProArchive Search Platform component degraded/offline": "OK", "[ProArchive] [Internal Tools] ProArchive Internal Tools Platform component degraded/offline": "OK", "[ProArchive] [ActiveMQ] ProArchive ActiveMQ Platform component degraded/offline": "OK", "[ProArchive] [Zookeeper] ProArchive Zookeeper Platform component degraded/offline": "OK", "[ProArchive] [Infrastructure] ProArchive Infrastructure Platform component degraded/offline": "OK", "[ProArchive] [VCO] ProArchive VCO Platform component degraded/offline": "OK", "[ProArchive] Zookeeper node count is below normal.": "OK", "[ProArchive] Export host {{host.name}} has failed to checkin.": "OK", "Concourse: Usable Memory monitor in AWS-Prod cloud {{host.name}}": "OK", "Avg. CPU Usage on vSphere Host pit-vhost-01a.smarshinc.com is High": "OK", "JPMC NAM UAT SLO Performance MongoDB": "OK", "Avg. CPU Usage on vSphere Host pit-vhost-02a.smarshinc.com is High": "OK", "Avg. CPU Usage on vSphere Host pit-vhost-03a.smarshinc.com is High": "OK", "Avg. CPU Usage on vSphere Host pit-vhost-04a.smarshinc.com is High": "OK", "Avg. CPU Usage on vSphere Host pit-vhost-05a.smarshinc.com is High": "OK", "Avg. CPU Usage on vSphere Host pit-vhost-06a.smarshinc.com is High": "OK", "Avg. CPU Usage on vSphere Host pit-vhost-07a.smarshinc.com is High": "OK", "JPMC NAM UAT SLO Availability MongoDB": "OK", "Avg. CPU Usage on vSphere Host pit-vhost-08a.smarshinc.com is High": "OK", "Avg. CPU Usage on vSphere Host qcy-vhost-01a.smarshinc.com is High": "OK", "Avg. CPU Usage on vSphere Host qcy-vhost-02a.smarshinc.com is High": "OK", "Avg. CPU Usage on vSphere Host qcy-vhost-03a.smarshinc.com is High": "OK", "Avg. CPU Usage on vSphere Host qcy-vhost-04a.smarshinc.com is High": "OK", "Avg. CPU Usage on vSphere Host qcy-vhost-05a.smarshinc.com is High": "OK", "Avg. CPU Usage on vSphere Host qcy-vhost-06a.smarshinc.com is High": "OK", "Avg. CPU Usage on vSphere Host qcy-vhost-07a.smarshinc.com is High": "OK", "Avg. CPU Usage on vSphere Host qcy-vhost-08a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host pit-vhost-01a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host pit-vhost-02a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host pit-vhost-03a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host pit-vhost-04a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host pit-vhost-05a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host pit-vhost-06a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host pit-vhost-07a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host pit-vhost-08a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host qcy-vhost-01a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host qcy-vhost-02a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host qcy-vhost-03a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host qcy-vhost-04a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host qcy-vhost-05a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host qcy-vhost-06a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host qcy-vhost-07a.smarshinc.com is High": "OK", "Avg. MEM Usage on vSphere Host qcy-vhost-08a.smarshinc.com is High": "OK", "Uptime Warning on vSphere Host": "OK", "Concourse: Disk Load is high on AWS-BNY {{host.name}}": "No Data", "Ashok test ES thread bluk size rejection {{cluster_name.name}}": "OK", "Vcenter BNY  NAM PROD monitoring": "OK", "Vcenter BNY  EMEA PROD monitoring": "OK", "Vcenter BNY  EMEA DR monitoring": "OK", "Memory Load {{hostname}} of Vantage Cloud instance": "OK", "vSphere - Datadog.Agent Check Status Testing": "OK", "Exim is not running on {{host.name}}": "Alert", "{{process.name}} is not running on {{host.name}}": "OK", "Vcenter BNY  NAM DR monitoring": "OK", "Disk space on {{host.name}} of Vantage Cloud Instance": "OK", "Concourse: Usable Memory monitor on AWS-BNY cloud {{host.name}}": "No Data", "CS test-eng NFS server low on disk space: {{host.name}}": "OK", "Vcenter Monitoring test": "OK", "NFS volume vol01 on Linux Journaing pod {{pod}} is low on free disk space": "OK", "Logstash service monitoring check {{host.name}} {{process.name}}": "OK", "Filebeat service monitoring check {{host.name}} {{process.name}}": "Alert", "MobileGuard App03 DB CPU Test - TPW": "OK", "{{env.name}}  {{host.name}} cpu load alert!": "OK", "{{env.name}}  {{host.name}} memory uasge alert!": "OK", "bnynamproduat_firewall01 accessibility": "OK", "bnynamproduat_firewall02 accessibility": "OK", "{{env.name}}  {{host.name}} cpu load alert!!": "No Data", "{{env.name}}  {{host.name}} memory uasge alert!!": "OK", "bnynamdr_firewall01 accessibility": "OK", "bnynamdr_firewall02 accessibility": "OK", "bnyemeaproduat_firewall02 accessibility": "OK", "bnyemeaproduat_firewall01 accessibility": "OK", "bnyemeadr_firewall01 accessibility.": "OK", "bnyemeadr_firewall02 accessibility.": "OK", "bnynamproduat_ loadbalancer01 accessibility": "OK", "bnynamproduat_ loadbalancer02 accessibility": "OK", "bnynamdr_loadbalancer01 accessibility": "OK", "bnynamdr_loadbalancer02 accessibility": "OK", "bnyemeaproduat_loadbalancer01 accessibility": "OK", "bnyemeaproduat_loadbalancer02 accessibility": "OK", "bnyemeadr_loadbalancer01 accessibility.": "OK", "bnyemeadr_loadbalancer02 accessibility": "OK", "MessagesQueue_Teams queue has grown above {{threshold}}": "OK", "[ProArchive] Zookeeper CPU usage on {{host.name}} is at {{value}}": "OK", "DR Discrepancy is greater than 1": "No Data", "(DM Export Tool) SQL queries are taking a long time to return data (create_csv_files.py)": "OK", "DR Discrepancy for idoc - SLI Monitor": "No Data", "DR Discrepancy for Ediscovery - SLI Monitor": "No Data", "DR Discrepancy for Supervision - SLI Monitor": "No Data", "Supervision Queues Running after Timeline : {{event.tags.env}} {{event.title}}": "OK", "Ingestion success Percentage": "OK", "E (Mail) Drive over 90% full": "OK", "SMTP response time high": "OK", "Dig received success count": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role bloomberg": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role activemq": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role bloomberg": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role bloomberg": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{host.device}} for role bloomberg": "OK", "TEST - IGNORE DM Export Write CSV average is high": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role activemq": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role activemq": "OK", "TEST - IGNORE DM SQL max runtime is high": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role activemq": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role IcewarpJournaling": "OK", "{{env.name}}  {{name.name}}  cpu load alert! [jpapac]": "OK", "{{env.name}}  {{name.name}}  cpu load alert! [jpemea]": "OK", "{{env.name}}  {{name.name}}  cpu load alert! [jpnam]": "OK", "Memory usage env:{{env.name}} host:{{name.name}} alert! [jpap_pr_dr]": "OK", "Memory usage env:{{env.name}} host:{{name.name}} alert! [jpap_uat]": "No Data", "Memory usage env:{{env.name}} host:{{name.name}} alert! [jpem_uat]": "OK", "Memory usage env:{{env.name}} host:{{name.name}} alert! [jpem_pr_dr]": "OK", "Memory usage env:{{env.name}} host:{{name.name}} alert! [jpnam]": "Alert", "LB memory uasge env:{{env.name}} host:{{name.name}} alert! [jpapac]": "No Data", "LB memory uasge env:{{env.name}} host:{{name.name}} alert! [jpemea]": "No Data", "LB memory uasge env:{{env.name}} host:{{name.name}} alert! [jpnam]": "No Data", "LB cpu load env:{{env.name}} host:{name.name} alert! [jpapac]": "No Data", "LB cpu load env:{{env.name}} host:{name.name} alert! [jpnam]": "No Data", "LB cpu load env:{{env.name}} host:{name.name} alert! [jpemea]": "No Data", "{{region.name}} {{name.name}} {{interface.name}} bandwidth utilization alert! [bnyemea]": "OK", "{{region.name}} {{name.name}} {{interface.name}} bandwidth utilization alert! [bnynam]": "OK", "{{env.name}} {{name.name}}  cpu load alert! [bnynam]": "OK", "{{env.name}} {{name.name}}  cpu load alert! [bnyemea]": "OK", "Memory usage env:{{env.name}} host:{{name.name}} alert! [bnyemea]": "OK", "Memory usage env:{{env.name}} host:{{name.name}} alert! [bnynam]": "OK", "Load balancer cpu load env:{{env.name}} host:{name.name} alert! [bnynam]": "OK", "Load balancer memory uasge env:{{env.name}} host:{{name.name}} alert! [bnynam]": "OK", "Load balancer memory uasge env:{{env.name}} host:{{name.name}} alert! [bnyemea]": "OK", "Load balancer cpu load env:{{env.name}} host:{name.name} alert! [bnyemea]": "OK", "reachability of  jpsmtp 169.69.70.148 or the vpn tunnel status between namuat to jpdc!": "OK", "reachability of  jpsmtp 169.114.165.167 or the vpn tunnel status between namuat to jpdc!": "OK", "reachability of  jpsmtp 169.114.165.167 or the vpn tunnel status between namdr to jpdc!": "Alert", "reachability of  jpsmtp 169.69.70.148 or the vpn tunnel status between namdr to jpdc!": "Alert", "reachability of  jpsmtp 169.114.165.167 or the vpn tunnel status between namprod to jpdc!": "OK", "reachability of  jpsmtp 169.69.70.148 or the vpn tunnel status between namprod to jpdc!": "OK", "reachability of  jpsmtp 169.114.165.167 or the vpn tunnel status between apacdr to jpdc!": "Alert", "reachability of  jpsmtp 169.69.70.148 or the vpn tunnel status between apacdr to jpdc!": "Alert", "reachability of  jpsmtp 169.114.165.167 or the vpn tunnel status between apacprod to jpdc!": "OK", "reachability of  jpsmtp 169.114.165.167 or the vpn tunnel status between apacuat to jpdc!": "OK", "reachability of  jpsmtp 169.69.70.148 or the vpn tunnel status between apacprod to jpdc!": "OK", "reachability of  jpsmtp 169.69.70.148 or the vpn tunnel status between apacuat to jpdc!": "OK", "reachability of  jpsmtp 169.69.70.148 or the vpn tunnel status between emeauat to jpdc!": "OK", "reachability of  jpsmtp 169.69.70.148 or the vpn tunnel status between emeaprod to jpdc!": "OK", "reachability of  jpsmtp 169.69.70.148 or the vpn tunnel status between emeadr to jpdc!": "OK", "reachability of  jpsmtp 169.114.165.167 or the vpn tunnel status between emeauat to jpdc!": "OK", "reachability of  jpsmtp 169.114.165.167 or the vpn tunnel status between emeaprod to jpdc!": "OK", "reachability of  jpsmtp 169.114.165.167 or the vpn tunnel status between emeadr to jpdc!": "OK", "jpusuat_dmz_firewall01 accessibility": "OK", "jpusuat_internal_firewall01 accessibility": "OK", "jpnamuat_loadbalancer01 accessibility": "OK", "jpnam_internal_firewall01 accessibility": "OK", "jpnam_internal_firewall02 accessibility": "OK", "jpnam_dmz_firewall01 accessibility": "OK", "jpnam_dmz_firewall02 accessibility": "OK", "jpnamdr_dmz_firewall02 accessibility": "OK", "jpnamdr_internal_firewall01 accessibility": "OK", "jpnamdr_dmz_firewall01 accessibility": "OK", "jpnamdr_internal_firewall02 accessibility": "OK", "jpemeauat_internal_firewall01 accessibility": "OK", "jpemeauat_dmz_firewall01 accessibility": "OK", "jpemeauat_loadbalancer01 accessibility": "OK", "jpemeaprod_dmz_firewall01 accessibility": "OK", "jpemeaprod_dmz_firewall02 accessibility": "OK", "jpemeaprod_internal_firewall01 accessibility": "OK", "jpemeaprod_internal_firewall02 accessibility": "OK", "jpemeadr_loadbalancer01 accessibility": "OK", "jpemeadr_loadbalancer02 accessibility": "OK", "jpemeaprod_loadbalancer01 accessibility": "OK", "jpemeaprod_loadbalancer02 accessibility": "OK", "jpemeadr_dmz_firewall01 accessibility": "OK", "jpemeadr_dmz_firewall02 accessibility": "OK", "jpemeadr_internal_firewall01 accessibility": "OK", "jpemeadr_internal_firewall02 accessibility": "OK", "jpapacuat_internal_firewall01 accessibility": "OK", "jpapacuat_dmz_firewall01 accessibility": "OK", "jpapacdr_internal_firewall01 accessibility": "OK", "jpapacdr_internal_firewall02 accessibility": "OK", "jpapacdr_dmz_firewall01 accessibility": "OK", "jpapacdr_dmz_firewall02 accessibility": "OK", "jpapacprod_dmz_firewall01 accessibility": "OK", "jpapacprod_dmz_firewall02 accessibility": "OK", "{{region.name}} {{name.name}} {{interface.name}} bandwidth utilization alert! [nam]": "OK", "{{region.name}} {{name.name}} {{interface.name}} bandwidth utilization alert! [emea]": "OK", "{{region.name}} {{name.name}} {{interface.name}} bandwidth utilization alert! [apac]": "OK", "jpapacprod_internal_firewall01 accessibility": "OK", "jpapacprod_internal_firewall02 accessibility": "OK", "jpapacprod_loadbalancer01 accessibility": "OK", "jpapacprod_loadbalancer02 accessibility": "OK", "jpapacdr_loadbalancer01 accessibility": "OK", "jpapacdr_loadbalancer02 accessibility": "OK", "[Synthetics] [ProArchive] - SMC - Walkme": "Alert", "SMTP Port 25 is unreachable  Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role IcewarpJournaling": "OK", "SMTP Process is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role IcewarpJournaling": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role earealtime": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role earealtime": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{host.device}} for role earealtime": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role earealtime": "OK", "Earealtime is not running on {{host}}": "OK", "TEST Ashok Ceph Cluster Health status: {{env}} {{cluster_name.name}}   [Cluster-Alert]": "No Data", "Disk Usage High on Concourse BNY DB {{host.name}} {{host.ip}} {{device.name}}": "No Data", "[ProArchive] Zookeeper Latency is higher than normal!": "OK", "[ProArchive] Zookeeper JVM Memory usage is anomalous in {{host.env}} on host {{host.name}}": "OK", "[Proarchive] Zookeeper connection count is anomalous in {{env}}": "OK", "[Proarchive] Zookeeper pending sync count is anomalous in {{host.env}} on {{host.name}}": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role retrievalservice": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{host.device}} for role retrievalservice": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role batchprocessor": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role retrievalservice": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role batchprocessor": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{host.device}} for role batchprocessor": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role retrievalservice": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role batchprocessor": "OK", "Disk Usage High on Concourse AWS DB {{host.name}} {{host.ip}} {{device.name}}": "OK", "[ProArchive] Zookeeper host {{host.name}} has failed to check in": "OK", "[ProArchive] Zookeeper node count is below normal": "OK", "[ProArchive] Zookeeper host CPU usage on {{host.name}} is at {{value}}": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{host.device}} for role filepurgeservice": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role filepurgeservice": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role filepurgeservice": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role filepurgeservice": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role bulkexport": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role bulkexport": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{host.device}} for role bulkexport": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role bulkexport": "OK", "\"kafka-perf\" deployment CPU high usage alerts": "OK", "[ProArchive] - Zookeeper Disk Usage is high Env:{{host.env}}, Host:{{host}}": "OK", "[ProArchive] - Zookeeper Memory Usage is high Env:{{host.env}}, Host:{{host}}": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role vco": "No Data", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role vco": "No Data", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role vco": "No Data", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role vco": "OK", "[Synthetics] Test on concourse.prod.smarsh.cloud/": "OK", "[Synthetics] Test on concourse-opensource.bny-shared.smarsh.cloud/": "Alert", "EGW: Schwab RabbitMQ {{bosh_deployment.name}}  High Disk Water Mark": "OK", "EGW: MT Prod RabbitMQ  {{bosh_deployment.name}}  High Disk Water Mark": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role solr": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role solr": "Alert", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role solr": "Warn", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role solr": "OK", "test on conc state landing": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role pingfederate": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{host.device}} for role social": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role social": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role pingfederate": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role pingfederate": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{host.device}} for role pingfederate": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role social": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role social": "OK", "test": "OK", "Storm service monitoring check {{host.name}} {{process.name}} {{ip}}": "Alert", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}for role smf": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}} for role smf": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{device}} for role smf": "OK", "[Website Archiving] - Webarch Memory Usage is high Env:{{host.env}}, Host:{{host}}": "OK", "[Website Archiving] - Webarch Disk Usage is high Env:{{host.env}}, Host:{{host}}": "OK", "[Website Archiving] Webarch host CPU usage on {{host.name}} is at {{value}}": "OK", "[Website Archiving] - Low Concurrency Application Error % Monitoring - {{process}}": "No Data", "[Website Archiving] - Memcached Process Monitoring: Host:{{host}}": "No Data", "[Website Archiving] - Application Error % Monitoring - {{process}}": "No Data", "[Website Archiving] - ActiveMQ has not been emptied completely": "OK", "SUPERVISION QUEUES RUNNING {{env.name}}": "Alert", "SUPERVISION QUEUES QUEUED {{env.name}}": "Alert", "AAC Application Monitoring Alert dtccprod": "OK", "Concourse: BNY Host {{host.name}}": "No Data", "Concourse: AWS-Prod Worker {{host.name}}": "Alert", "Ceph OSD Full in {{env.name}}": "OK", "AAC Application Monitoring Alert bunge": "OK", "Ceph Luminous Cluster Capacity status: {{ceph_fsid}}": "Alert", "Disk Usage is high Env:{{env}}, Host:{{host}}, IP: {{ip}}, device: {{device}} hostGroup {{host_group}}": "Alert", "MongoDB Replicaset read tickets running low": "OK", "MongoDB Replicaset write tickets running low": "OK", "Concourse: AWS-Prod Host {{host.name}}": "OK", "Ceph PG degraded in {{env.name}}": "Warn", "Statuspage Admin AWS-Prod {{host.name}}": "OK", "Ceph Pool full in {{env.name}}": "OK", "Ceph stuck requests in {{env.name}}": "OK", "MongoDB replication lag {{replset_name}} in {{env.name}} is above normal": "OK", "Sonar AWS-Prod {{host.name}}": "OK", "Ceph Pool near full in {{env.name}}": "OK", "Ceph too many pgs in {{env.name}}": "OK", "Ceph OSD down in {{env.name}}": "OK", "Ceph OSD Near Full in {{env.name}}": "Warn", "Concourse workers PROD-AWS - fewer than 20": "Alert", "Monitor Concourse Home Page concourse-opensource.bny-shared.smarsh.cloud": "No Data", "Concourse canary pipeline performance on AWS-PROD env": "OK", "MongoDB connection failure in {{env.name}} on {{host.name}}/{{host.ip}}": "OK", "Ceph Cluster Health status: {{env}} [Cluster-Alert]": "OK", "Concourse: BNY Worker {{host.name}}": "No Data", "Ceph too few PGs in {{env.name}}": "OK", "Sonar AWS-Gov {{host.name}}": "OK", "MongoDB Replicaset status for {{replset_name.name}} in {{env.name}} is outside of normal range.": "Warn", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role facebookworkplace": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role facebookworkplace": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role facebookworkplace": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role facebookworkplace": "OK", "PCF Loggregator Firehose - Logs being dropped": "OK", "[ProArchive] - Solrweb TLS Certificate on {{host.name}} expires in {{value}} days": "OK", "Store Percent has increased by 20% Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role activemq": "OK", "Store Percent is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role activemq": "OK", "[Synthetics] EnterpriseArchive_SREAPPS_MTPROD_UI_LOGIN_CHECK": "OK", "[Synthetics] [ProArchive] Search: Basic all content, date/time, & calculate search": "OK", "[Synthetics] Test on sreappsmtprod.ca.smarsh.cloud": "OK", "[Synthetics] MTPRODTEST": "OK", "[Synthetics] Test-Browser Login": "OK", "Job stuck user_activity_log monitoring  -{{env.name}}": "OK", "[Synthetics] EnterpriseArchive_SREAPPS_BNYNAMPROD_UI_LOGIN_CHECK (cloned)": "OK", "[Synthetics] EnterpriseArchive_SREAPPS_SCHWABPROD_UI_LOGIN_CHECK": "OK", "[Synthetics] [ProArchive] Search: Search for keyword \"Hello\" - All content, all keyword fields checked": "OK", "[Synthetics] [ProArchive] Search: Calculate & Search for hierarchy entry, - all content": "OK", "[Synthetics] Test on sreappsmtprod.ca.smarsh.cloud/portola/login": "OK", "AAC Application Monitoring Alert schwabprod": "No Data", "[Synthetics] Test on sreappsbnyprod.bny.smarsh.cloud/portola": "OK", "[Synthetics] EnterpriseArchive_SREAPPS_BNYPRODNAM_UI_LOGIN_CHECK": "OK", "[Synthetics] EnterpriseArchive_SREAPPS_BNY_PRODEMEA_UI_LOGIN_CHECK": "OK", "\"storm-perf\" deployment CPU high usage alerts": "OK", "\"Elastic Search\" aws-perf space deployment CPU high usage alerts": "No Data", "\"ESSR\" deployment CPU high usage alerts": "No Data", "{{bosh_deployment.name}} | Supervisors | Open files descriptors count is higher than regular range for IP: {{bosh_ip.name}}": "No Data", "[Synthetics] [ProArchive] Search: Calculate & Search for hierarchy entry, - all content, keyword \"hello\"": "OK", "[Synthetics] [ProArchive] Search: Calculate & Search for manually entered identifier in people section, all content, all keywords checked": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role linuxjournaling": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role linuxjournaling": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{host.device}} for role linuxjournaling": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role linuxjournaling": "OK", "[Synthetics] EnterpriseArchive_SREAPPS_BNYUATNAM_UI_LOGIN_CHECK": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role msteamsarchiver": "OK", "MsTeams has stopped dequeueing Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role activemq": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role msteamsarchiver": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role msteamsarchiver": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role msteamsarchiver": "OK", "[Synthetics] EnterpriseArchive_SREAPPS_BNY_UATEMEA_UI_LOGIN_CHECK": "OK", "[Synthetics] [ProArchive] Search: Calculate & Search for manually entered identifier in people section, all content, keyword - \"hello\", all keywords checked": "OK", "[Synthetics] [ProArchive] Queues: Verify data added looks correct": "OK", "DBRE Infrastructure Monitor - Cluster registry checkpoint could not be restored to registry key EventID 1024 on {{event.host.name}}": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role ear-master": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role ear-master": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role ear-master": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role ear-master": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role ear-slave": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role ear-slave": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role ear-slave": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role ear-slave": "OK", "[Synthetics] [ProArchive] Search: Review Features - Not reviewed": "OK", "[Synthetics] [ProArchive] Search: Review Features - Further Review": "OK", "[Synthetics] [ProArchive] Search: Review Features -Reviewed": "OK", "[Synthetics] [ProArchive] Search: Review Features - Auto Reviewed": "OK", "[Synthetics] [ProArchive] Search: Review Features - Violation": "OK", "[Synthetics] [ProArchive] Search: Review Features - No Flag": "OK", "[Synthetics] [ProArchive] Search: Review Features - Privileged Only": "OK", "[Synthetics] [ProArchive] Search: Review Features - None Privileged": "OK", "[Synthetics] NEW_EnterpriseArchive_SREAPPS_BNYPRODNAM_UI_LOGIN_CHECK": "No Data", "[Synthetics] [ProArchive] Search: Review Features - add tag \"test\"": "OK", "[Synthetics] [ProArchive] Search: Review Features - Assigned (logged in user)": "OK", "DBRE Infrastructure Monitor - Node failed to join cluster EventID 1070 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster network connectivity problems EventID 1127 on {{event.host.name}}": "OK", "DBRE Application Monitor - Database file growth problem EventID 1105 on {{event.host.name}}": "OK", "DBRE Application Monitor - Database file growth problem EventID 1101 on {{event.host.name}}": "OK", "[Website Archiving] - Webarch Log Directory is Dangerously full!": "OK", "DBRE Infrastructure Monitor - Problems with cluster service EventID 1069 on {{event.host.name}}": "OK", "[Synthetics] [ProArchive] Discovery: Create Case": "OK", "DBRE Application Monitor - SQL Server Replication failures  EventID 14151 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster Quorum was lost EventID 1177 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster network connectivity problems EventID 1130 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Problems with cluster service EventID 1205 on {{event.host.name}}": "OK", "DBRE Application Monitor - SQL Server Replication failures EventID 14152 on {{event.host.name}}": "OK", "AT&T Has Not Successfully Delivered Batches!": "OK", "DBRE Application Monitor - Missing Quarterly Tables on PROD {{host.name}}": "OK", "DBRE Application Monitor - SQL Connection errors EventID 17187 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - System is not being responsive EventID 4870 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - System is not being responsive EventID 4869 on {{event.host.name}}": "OK", "DBRE Application Monitor - SQL Server Memory Insufficient Errors EventID 17131 on {{event.host.name}}": "OK", "DBRE Application Monitor - Transaction log full on SQL Server Database EventID 9002 on {{event.host.name}}": "OK", "DBRE Application Monitor - Handshake errors detected EventID 17806 on {{event.host.name}}": "OK", "DBRE Application Monitor - SUSPECT_PAGES NEW ENTRY in PROD on {{host.name}}": "OK", "DBRE Application Monitor - SQL Server Database failure detected EventID 17207 on {{event.host.name}}": "OK", "DBRE Application Monitor - SQL Connection closed EventID 18056 on {{event.host.name}}": "OK", "DBRE Application Monitor - FULL backup latency PROD {{host.name}}": "OK", "DBRE Application Monitor - Differential backup latency PROD {{host.name}}": "OK", "[Synthetics] [ProArchive] Search: Review Features - Policy Match": "OK", "[Synthetics] [ProArchive] Search: Review Features - Retention Policies": "OK", "[Synthetics] [ProArchive] Search: Review Features - Message Score": "OK", "[Synthetics] [ProArchive] Connections: Verify page loads and Social/Mobile connections are provisioned": "OK", "[Synthetics] [ProArchive] Authorize Channels: Verify page loads and Relevant Channel is fully Authorized": "OK", "[Synthetics] [ProArchive] Groups: Verify page loads and hierarchy looks correct": "OK", "[Synthetics] [ProArchive] Content Usage: Verify page loads and content usage users look correct": "OK", "[Synthetics] [ProArchive] Reports: Run Review Summery Report & Verify": "OK", "[Synthetics] [ProArchive] Reports: Run Review Queue by Queue & Verify": "OK", "[Synthetics] [ProArchive] Reports: Run Policy Analysis Report & Verify": "OK", "[Synthetics] [ProArchive] Reports: Policy Reports Scheduler - Create & Delete Report Schedule": "OK", "[Synthetics] [ProArchive] Web archiving: Verify  console is accessible": "Alert", "[Synthetics] [ProArchive] Reports: User Activity Loads": "OK", "[Synthetics] [ProArchive] Reports: Message Reconciliation - Email Messages Sent & Recieved": "OK", "[Synthetics] [ProArchive] Content Usage:  Load Page & Verify": "OK", "[Synthetics] [ProArchive] Policy Scanning: Further Review in Policies List, Search & confirm Policy Scanning": "OK", "[Synthetics] [ProArchive] Disable Walkme - Javascript": "OK", "Jobs disabled  {{env}}": "OK", "Service ingestion-topology has a high p90 latency on env:aws-mt-prod": "OK", "Service ingestion-topology has an abnormal change in throughput on env:aws-mt-prod": "OK", "Service ingestion-topology has a high error rate on env:aws-mt-prod": "OK", "Service ingestion-topology has a high average latency on env:aws-mt-prod": "OK", "[Synthetics] An Uptime test for sonar.apps.prod.smarsh.cloud": "OK", "VPN Status is in trouble  {{env.name}}": "Alert", "Disk Utilization-JPMC STORM": "OK", "Disk Utilization-JPMC Hazelcast": "OK", "Disk Utilization-JPMC EGW": "OK", "Disk Utilization-JPMC Karaf": "Warn", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role mailgw": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role mailgw": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{host.device}} for role mailgw": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role mailgw": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role IcewarpJournaling": "OK", "SplunkForwarder Process is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role IcewarpJournaling": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{host.device}} for role IcewarpJournaling": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role IcewarpJournaling": "OK", "[ProArchive UI] Search: Review Features Composite Check 1": "OK", "[ProArchive UI] Search: Review Features Composite Check 2": "OK", "[ProArchive UI] Search: Calculate and Hiearchy": "OK", "Exim queue on Mail Gateways is currently at {{value}}": "OK", "Exim queue on Exim Relays is currently at {{value}}": "OK", "Test Supervises service monitoring check {{process.name}} {{env.name}} {{host.name}}": "No Data", "Vantage-Demo-DB - MSSQLS.exe Service Alert": "OK", "[ProArchive] [ActiveMQ] Average Queue size threshold exceeded.": "OK", "Exim queue on Linux Journaling is currently at {{value}}": "OK", "[ProArchive] VCO BBC Backlog": "OK", "[ProArchive] VCO BBMessage Backlog": "OK", "{{bosh_deployment.name}} | Supervisors process check: {{bosh_ip.name}}": "No Data", "Storm topology -->  export_msgAndNonEmail-topology": "OK", "Storm topology --> lfs-topology": "OK", "Storm topology --> metadata-hold-topology": "Alert", "Storm topology  --> metadata-supervision-topology is down": "OK", "Storm topology -->nonemail-ingestion-topology": "OK", "Storm topology -->purge-topology": "OK", "Storm topology --> ediscovery export topology": "OK", "Storm topology --> Supervision export-topology": "OK", "Storm topology --> Supervision postprocessing topology is down": "OK", "Storm-Supervisor- process is down": "OK", "Storm-Nimbus- process is down": "OK", "Storm service monitoring check {{host.name}} {{process.name}} {{ip}}   Status": "Alert", "Storm Nimbus service monitoring check {{host.name}} {{process.name}} {{ip}} Status": "Alert", "Storm UI service monitoring check {{host.name}} {{process.name}} {{ip}} Status": "Alert", "Hazelcast service monitoring check {{host.name}} {{process.name}} {{ip}} Status": "OK", "Vantage-Demo-DB - Datadog Service Alert": "OK", "[Synthetics] login test": "No Data", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role smarshconnectionservice": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role smarshconnectionservice": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role smarshconnectionservice": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role smarshconnectionservice": "OK", "[Synthetics] rajeev.aminbhavi@smarsh.com": "Alert", "Hazelcast service monitor {{env.name}}  {{host_group.name}}  {{ip.name}}": "OK", "DBRE Database Monitor - SQL Defrag Service is down on {{host.name}}": "OK", "[Synthetics] Sanity-Synthetics": "OK", "[Synthetics] sanity-synthetics": "Alert", "DBRE Database Monitor - SQL Monitoring (DPA) is down on {{host.name}}": "OK", "DBRE Database Monitor - SQL Server Reporting  Service is down on {{host.name}}": "OK", "Solr Node Primary Slave {{host.name}} failed to check in": "OK", "Solr Node Secondary Slave {{host.name}} failed to check in": "OK", "ProArchive Composite Monitor - AT&T": "OK", "JPMC PROD EMEA: Latency for non error response code is higher": "OK", "[Synthetics] Vantage-Cloud-SRE-Test": "OK", "Test mongo router": "Alert", "VCO AMQ BBC Backlog Monitor": "OK", "VCO BBC Not Dequeuing from AMQ": "Alert", "Socialite Disk Usage is high Env:{{env}}, Host:{{host}}, IP: {{ip}}, device: {{device}} hostGroup {{host_group}}": "No Data", "Socialite Production Instance Disk Usage is : Host:{{host}}, IP: {{ip}}, device: {{device}}": "Warn", "NFS Threashold for Apac ReadOPS has reached 0": "OK", "NFS Threashold for JPMC Nam ReadOPS has reached 0": "OK", "NFS Threashold for JPMC Emea ReadOPS has reached 0": "OK", "[ProArchive] VCO IM Backlog": "OK", "EGW postfix service monitoring check {{host.env}} {{host.name}} {{process.name}}": "OK", "MongoDB Disk Usage is high Customer:{{customer.name}} Hosts:{{host}} env:{{env}} Clustername:{{cluster_name.name}} Device:{{device.name}}": "Alert", "Egw Jilter service monitoring check {{host.env}} {{host.name}} {{process.name}}": "Alert", "NFS karaf service monitoring check {{host.name}} {{process.name}}": "Alert", "NFS-HA Highly Available Cluster Health Check": "OK", "VCO AMQ BBM Backlog Monitor": "OK", "VCO AMQ BBC Composite Dequeue//Backlog Monitor": "OK", "VCO AMQ IM Backlog Monitor": "OK", "VCO BBM//IM Not Dequeuing from AMQ": "Alert", "VCO Email Not Dequeuing from AMQ": "Alert", "VCO AMQ Email Backlog Monitor": "Alert", "[Synthetics] Test on aac aws-demo application": "OK", "Anomaly Detection - MongoDB Disk Usage is high Customer:{{customer.name}} Hosts:{{host}} env:{{env}} Clustername:{{cluster_name.name}} Device:{{device.name}}": "No Data", "NFS Threshold for Write OPS is extremely Low": "OK", "NFS Write Threshold is Extremely Low!": "OK", "NFS Write Threshold for Emea Has reached extremely Low!": "Alert", "VCO AMQ Email Composite Dequeue//Backlog Monitor": "Alert", "VCO AMQ IM Composite Dequeue//Backlog Monitor": "OK", "VCO AMQ BBM Composite Dequeue//Backlog Monitor": "OK", "Zookeeper Cluster Capacity status: {{env}}": "OK", "DBRE Infrastructure Monitor - DB Logfile error. event_id 17053 occurred on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - DB Logfile Error. event_id 3314 occurred on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - DB Logfile Error. event_id 9001 occurred on {{event.host.name}}": "OK", "Number of {{processing_state.name}} processed documents exceeds 1000 for {{customer.name}} in {{environment.name}}": "OK", "DBRE Application Monitor - Database Sustained Blocking": "OK", "DBRE Application Monitor - Database Sessions": "OK", "DBRE Application Monitor - Database Transaction Rate per Second": "OK", "DBRE Application Monitor - Database Throughput": "OK", "[Synthetics] An Uptime test for Concourse.prod.smarsh.cloud": "OK", "ingestion Lag is greater than threshold from last 1 hour": "Alert", "ingestion Drain Rate is lower than threshold from last 1 hour": "Alert", "\"ingestion\" Pipeline Needs Attention": "Alert", "Need Attention: Jobs are stuck in Job_Request_config collection": "No Data", "Service ca-audit-center on resource_name:import-job has a high average latency on env:aws-schwab-prod": "OK", "ceph_sync_primary Drain Rate is lower than threshold from last 1 hour": "Alert", "supervision Drain Rate is lower than threshold from last 1 hour": "Alert", "journal03 Drain Rate is lower than threshold from last 1 hour": "Alert", "supervision_metadata Drain Rate is lower than threshold from last 1 hour": "Alert", "email Drain Rate is lower than threshold from last 1 hour": "Alert", "journal04 Drain Rate is lower than threshold from last 1 hour": "Alert", "purge Drain Rate is lower than threshold from last 1 hour": "Alert", "journal01 Drain Rate is lower than threshold from last 1 hour": "Alert", "journal02 Drain Rate is lower than threshold from last 1 hour": "Alert", "job_processing_topology Drain Rate is lower than threshold from last 1 hour": "Alert", "lfs02_primary Drain Rate is lower than threshold from last 1 hour": "Alert", "ceph_sync_dr Drain Rate is lower than threshold from last 1 hour": "Alert", "lfs_primary Drain Rate is lower than threshold from last 1 hour": "Alert", "case_management Drain Rate is lower than threshold from last 1 hour": "Alert", "vantage01 Drain Rate is lower than threshold from last 1 hour": "Alert", "index_dr Drain Rate is lower than threshold from last 1 hour": "Alert", "nonEmail Drain Rate is lower than threshold from last 1 hour": "Alert", "nonEmail Lag is greater than threshold from last 1 hour": "OK", "case_management Lag is greater than threshold from last 1 hour": "OK", "supervision_metadata Lag is greater than threshold from last 1 hour": "OK", "email Lag is greater than threshold from last 1 hour": "OK", "supervision Lag is greater than threshold from last 1 hour": "OK", "lfs02_primary Lag is greater than threshold from last 1 hour": "OK", "ceph_sync_dr Lag is greater than threshold from last 1 hour": "OK", "journal03 Lag is greater than threshold from last 1 hour": "OK", "vantage01 Lag is greater than threshold from last 1 hour": "OK", "job_processing_topology Lag is greater than threshold from last 1 hour": "OK", "purge Lag is greater than threshold from last 1 hour": "OK", "lfs_primary Lag is greater than threshold from last 1 hour": "OK", "ceph_sync_primary Lag is greater than threshold from last 1 hour": "OK", "journal04 Lag is greater than threshold from last 1 hour": "OK", "journal02 Lag is greater than threshold from last 1 hour": "OK", "index_dr Lag is greater than threshold from last 1 hour": "Alert", "journal01 Lag is greater than threshold from last 1 hour": "OK", "\"email\" Pipeline Needs Attention": "OK", "\"journal02\" Pipeline Needs Attention": "OK", "\"ceph_sync_primary\" Pipeline Needs Attention": "OK", "\"ceph_sync_dr\" Pipeline Needs Attention": "OK", "\"supervision_metadata\" Pipeline Needs Attention": "OK", "\"lfs_primary\" Pipeline Needs Attention": "OK", "\"nonEmail\" Pipeline Needs Attention": "OK", "\"vantage01\" Pipeline Needs Attention": "OK", "\"journal01\" Pipeline Needs Attention": "OK", "\"journal04\" Pipeline Needs Attention": "OK", "\"journal03\" Pipeline Needs Attention": "OK", "\"supervision\" Pipeline Needs Attention": "OK", "\"job_processing_topology\" Pipeline Needs Attention": "OK", "\"index_dr\" Pipeline Needs Attention": "Alert", "\"lfs02_primary\" Pipeline Needs Attention": "OK", "\"purge\" Pipeline Needs Attention": "OK", "\"case_management\" Pipeline Needs Attention": "OK", "Mongo Router Transfer High in {{env}}": "OK", "DBRE Infrastructure Monitor - Cluster startup problems EventID 1230 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster startup problems EventID 1146 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster startup problems EventID 1178 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster startup problems EventID 1561 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster startup problems EventID 1556 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster IP address resource availability problems EventID 1363 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster IP address resource availability problems EventID 1242 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster IP address resource availability problems EventID 1048 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster IP address resource availability problems EventID 1046 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster IP address resource availability problems EventID 1078 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster IP address resource availability problems EventID 1361 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster IP address resource availability problems EventID 1049 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster IP address resource availability problems EventID 1047 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster startup problems EventID 1000 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster startup problems EventID 1006 on {{event.host.name}}": "OK", "Squid Service status  {{host.name}} 1.0": "OK", "DBRE Infrastructure Monitor - Disk or Storage Errors EventID 39 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Disk or Storage Errors EventID 129 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Disk or Storage Errors EventID 7 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Disk or Storage Errors EventID 153 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Disk or Storage Errors EventID 9 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster startup problems EventID 1073 on {{event.host.name}}": "OK", "SFTP on pit-ftphost-02.smarshinc.com is having problems": "OK", "test: Latency for non error response code is higher": "Alert", "MongoDB Router connection failure for {{target_host.name}} {{port.name}}": "OK", "[Bloomberg Archiving] - Backlog is unreasonably high": "OK", "[Bloomberg Backlog] - Bloomberg has not burned down overnight": "OK", "test: ingestion Lag is greater than threshold from last 4 hour": "Alert", "[Jive (Openfire)] - *No Ingestion*": "OK", "Concourse canary pipeline SONAR performance on AWS-PROD env": "OK", "Concourse canary pipeline SONAR performance on AWS-GOV env": "No Data", "Squid Proxy service status {{host.name}}": "OK", "Solr IM Indexing Backlog": "OK", "Solr Email Attachment Indexing Backlog": "OK", "Solr Instant Bloomberg Attachment Indexing Backlog": "OK", "Solr Email User Content Indexing Backlog": "OK", "Solr Bloomberg Message User Content Indexing Backlog": "OK", "Solr Bloomberg Message Indexing Backlog": "OK", "Solr Bloomberg Message Attachment Indexing Backlog": "OK", "Solr Instant Bloomberg Indexing Backlog": "OK", "Solr Instant Bloomberg User Content Indexing Backlog": "OK", "Solr IM User Content Indexing Backlog": "OK", "Solr Case NonEmail Indexing Backlog": "OK", "Solr IM Attachment Indexing Backlog": "OK", "Solr Case Email Indexing Backlog": "OK", "Solr Email Indexing Backlog": "OK", "Solr Storage Forecast": "Alert", "[Synthetics] [ProArchive] QCY Journal 01 Relay Test": "OK", "[Synthetics] [ProArchive] QCY Journal 02 Relay Test": "OK", "[Synthetics] [ProArchive] QCY Journal 03 Relay Test": "OK", "[Synthetics] [ProArchive] QCY Journal 04 Relay Test": "OK", "[Synthetics] [ProArchive] QCY Journal 05 Relay Test": "OK", "[Synthetics] [ProArchive] QCY LB Journal 01 Relay Test": "OK", "[Synthetics] [ProArchive] QCY LB Journal 02 Relay Test": "OK", "[Synthetics] [ProArchive] QCY LB Journal 03 Relay Test": "OK", "[Synthetics] [ProArchive] QCY LB Journal 04 Relay Test": "OK", "[Synthetics] [ProArchive] QCY LB Journal 05 Relay Test": "OK", "[Synthetics] [ProArchive] QCY LB Journal 06 Relay Test": "OK", "[Synthetics] [ProArchive] QCY LB Journal 07 Relay Test": "OK", "[Synthetics] [ProArchive] QCY LB Journal 08 Relay Test": "OK", "[Synthetics] [ProArchive] QCY PNC Journal 03 Relay Test": "OK", "[Synthetics] [ProArchive] QCY PNC Journal 04 Relay Test": "OK", "[Synthetics] [ProArchive] QCY Sahara Journal 02 Relay Test": "OK", "[Synthetics] [ProArchive] QCY Stardust Journal 02 Relay Test": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{host.device}} for exim relay nodes": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for exim relay nodes": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for exim relay nodes": "OK", "[Synthetics] [ProArchive] Smarsh Partner Connection Service": "OK", "[Synthetics] [ProArchive] Internal Provisioning Service": "OK", "supervision queues job stuck start for more then 3 hours  {{environment.name}}": "Alert", "Jobs stuck or unscheduled in {{environment.name}}": "Alert", "Verizon Ingestion Monitoring": "OK", "[Synthetics] [ProArchive] SAS Archive Service": "OK", "[Synthetics] [ProArchive] SAS Facebook Callback Service": "OK", "[Synthetics] [ProArchive] SAS MS Teams Oauth Service": "OK", "[Synthetics] [ProArchive] SAS OAuth Service": "OK", "[Synthetics] [ProArchive] SAS Verizon Callback Service": "OK", "[Synthetics] [ProArchive] SAS Facebook Workplace Callback Service": "OK", "[Synthetics] [ProArchive] Case Management Service": "OK", "[Synthetics] [ProArchive] Export Service": "OK", "[Synthetics] [ProArchive] Identity Management Service": "OK", "[Synthetics] [ProArchive] Search Service": "OK", "[Synthetics] [ProArchive] LBJOURNAL03": "OK", "[Synthetics] [ProArchive] PIT Journal 01 Relay Test": "OK", "[Synthetics] [ProArchive] PIT Journal 02 Relay Test": "OK", "[Synthetics] [ProArchive] PIT Journal 03 Relay Test": "OK", "[Synthetics] [ProArchive] PIT Journal 04 Relay Test": "OK", "[Synthetics] [ProArchive] PIT Journal 05 Relay Test": "OK", "[Synthetics] [ProArchive] PIT LB Journal 01 Relay Test": "OK", "[Synthetics] [ProArchive] PIT LB Journal 03 Relay Test": "OK", "[Synthetics] [ProArchive] PIT LB Journal 04 Relay Test": "OK", "[Synthetics] [ProArchive] PIT LB Journal 05 Relay Test": "OK", "[Synthetics] [ProArchive] PIT LB Journal 06 Relay Test": "OK", "[Synthetics] [ProArchive] PIT PNC Journal 01 Relay Test": "OK", "[Synthetics] [ProArchive] PIT PNC Journal 02 Relay Test": "OK", "[Synthetics] [ProArchive] PIT Sahara Journal Relay Test": "OK", "[Synthetics] [ProArchive] PIT Stardust Relay Test": "OK", "[Synthetics] [ProArchive] PIT Pod01-PDX Relay Test": "OK", "[Synthetics] [ProArchive] QCY Pod-01-QCY Relay Test": "OK", "[Synthetics] [ProArchive] QCY ArchiveIM2 Relay Test": "OK", "[Synthetics] [ProArchive] PIT IM Archive Relay Test": "OK", "Need Attention: Jobs are stuck in Job_Request_config collection {{env.name}}": "No Data", "CC-TestMonitor_name": "No Data", "DBRE Database Monitor - SQL Server Instance Service is down on {{host.name}}": "OK", "DBRE Database Monitor - SQL Server Agent Service is down on {{host.name}}": "OK", "[Synthetics] [ProArchive] PIT LB Journal 02 Relay Test": "OK", "{{ host.name }} is down! Please investigate!": "OK", "[Synthetics] [ProArchive] Host01 SFTP TCP Check": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role employeetools": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role employeetools": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role employeetools": "OK", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role employeetools": "OK", "DBRE Application Monitor Database Transaction Throughput Change": "OK", "DBRE Application Monitor Database Active Connections Change": "OK", "DBRE Application Monitor Database Blocking Change Status": "Alert", "DNS Response Time - SmarshINC": "OK", "[Synthetics] Socialite-JefferiesAndCo": "OK", "[Synthetics] Socialite-CS": "OK", "[Synthetics] Socialite-Deutsche Bank": "OK", "[Synthetics] Socialite-StateStreet2": "OK", "[Synthetics] Socialite-Macquarie": "OK", "Vcenter NAM Sandbox monitoring - vcenter.sandbox.nam.jpmc.smarsh.cloud": "OK", "DBRE Database Monitor - SQL Server Instance Service for COMPDB is down on {{host.name}}": "OK", "DBRE Database Monitor - SQL Server Agent Service for COMPDB is down on {{host.name}}": "OK", "CPU Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role ftp": "OK", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role ftp": "Alert", "Memory Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role ftp": "OK", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role ftp": "OK", "reprocess Drain Rate is lower than threshold from last 1 hour": "Alert", "reprocess Lag is greater than threshold from last 1 hour": "OK", "\"reprocess\" Pipeline Needs Attention": "OK", "[Synthetics] Linux Journaling - TCP Check to Port 25": "OK", "[Bloomberg Archiving] - Ingestion is lower than acceptable": "OK", "3120: Latency for non error response code is higher": "Alert", "[Synthetics] Test on journal01.smarsh.com": "OK", "[Synthetics] Test on journal02.smarsh.com": "OK", "BNY NFS Disk Usage is high Env:{{env}}, Host:{{host}}, IP: {{ip}}, device: {{device}} hostGroup {{host_group}}": "Alert", "Vcenter APAC Sandbox monitoring - vcenter.sandbox.apac.jpmc.smarsh.cloud": "OK", "Vcenter EMEA Primary monitoring - vcenter.pri.emea.jpmc.smarsh.cloud": "OK", "Vcenter EMEA Sandbox monitoring - vcenter.sandbox.emea.jpmc.smarsh.cloud": "OK", "[Synthetics] Test on dtccprod.ca.smarsh.cloud/portola/": "OK", "JPMC: Latency for non-error response code is higher": "Alert", "lfs02_primary Pipeline is down from last 15 minutes": "Alert", "job_processing_topology Pipeline is down from last 15 minutes": "OK", "index_dr Pipeline is down from last 15 minutes": "Alert", "journal04 Pipeline is down from last 15 minutes": "OK", "purge Pipeline is down from last 15 minutes": "Alert", "journal03 Pipeline is down from last 15 minutes": "Alert", "reprocess Pipeline is down from last 15 minutes": "Alert", "ingestion Pipeline is down from last 15 minutes": "Alert", "case_management Pipeline is down from last 15 minutes": "Alert", "supervision_metadata Pipeline is down from last 15 minutes": "Alert", "ceph_sync_dr Pipeline is down from last 15 minutes": "Alert", "journal01 Pipeline is down from last 15 minutes": "Alert", "supervision Pipeline is down from last 15 minutes": "Alert", "ceph_sync_primary Pipeline is down from last 15 minutes": "Alert", "email Pipeline is down from last 15 minutes": "Alert", "nonEmail Pipeline is down from last 15 minutes": "Alert", "vantage01 Pipeline is down from last 15 minutes": "Alert", "journal02 Pipeline is down from last 15 minutes": "Alert", "lfs_primary Pipeline is down from last 15 minutes": "Alert", "DBRE Application Monitor - TLog backup latency on {{host.name}}": "OK", "DBRE Application Monitor - Missing FULL Database Backup(s) in PROD on {{host.name}}": "OK", "[Website Archiving] - Low Concurrency Application (low_006) Error % Monitoring - {{process}}": "No Data", "[Synthetics] Socialite-FlowTradersUS": "OK", "[Synthetics] Socialite-InvestecAssetManagementLtd": "OK", "[Synthetics] Socialite-MillenniumManagementLLC": "OK", "[Synthetics] Socialite-StrikeTechnologies": "OK", "[Synthetics] Socialite-BNPParibas": "OK", "[Synthetics] Socialite-MPGOperationsLLC-2nd": "OK", "[Synthetics] Socialite-FidelityINTERNATIONAL": "OK", "[Synthetics] Socialite-Aberdeen Asset management-via-Hootsuite Inc": "OK", "[Synthetics] Socialite-Fred Alger & Co.": "OK", "[Synthetics] Socialite-Bank of New York Mellon": "OK", "[Synthetics] Socialite-KeyBank National Association": "OK", "[Synthetics] Socialite-Natixis North America LLC": "OK", "[Synthetics] Socialite-Comerica Bank": "OK", "[Synthetics] Socialite-Jennison Associates": "OK", "[Synthetics] Socialite-Training": "OK", "[Synthetics] Socialite-Leerink Swann": "OK", "[Synthetics] Socialite-Hootsuite Integration and Demo": "OK", "[Synthetics] Socialite-State Street Bank and Trust": "OK", "DBRE Infrastructure Monitor - Cluster network connectivity or configuration problem EventID 4871 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster network connectivity or configuration problem EvendID 1554 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster network connectivity or configuration problem EventID 1553 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster network connectivity or configuration problem EventID 1289 on {{event.host.name}}": "OK", "[Bloomberg Archiving] - Composite - Ingest and Backlog are both in bad state": "OK", "Slack Low Ingestion": "OK", "[Synthetics] [ProArchive] OAuth Svc - QA": "OK", "DBRE Application Monitor - Cluster network connectivity or configuration problem": "OK", "DBRE Infrastructure Monitor -  Cluster generic service availability problem EventId 1041 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor -  Cluster generic service availability problem EventId 1040 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor -  Cluster generic service availability problem EventId 1042 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor -   Cluster witness problem EventId 1558 on {{host.name}}": "OK", "DBRE Infrastructure Monitor -   Cluster witness problem EventId 1573 on {{host.name}}": "OK", "DBRE Infrastructure Monitor -   Cluster witness problem EventId 1563 on {{host.name}}": "OK", "DBRE Infrastructure Monitor -   Cluster witness problem EventId 1562 on {{host.name}}": "OK", "DBRE Infrastructure Monitor -   Cluster witness problem EventId 1564 on {{host.name}}": "OK", "DBRE Infrastructure Monitor -   Cluster witness problem EventId 1557 on {{host.name}}": "OK", "[Synthetics] Daemon App": "OK", "[Synthetics] UAA App Health for Pwez Perf Envrionment": "OK", "[Synthetics] Test on concourse.pwez.smarsh.cloud/": "OK", "[Synthetics] Haraka App": "OK", "[Synthetics] Test on ea-perf-es-metrics-scb.pwez.smarsh.cloud": "OK", "[Synthetics] Test on 168.63.129.16": "OK", "[Synthetics] Tenant Check": "Alert", "[Synthetics] Test on Vault Load Balancer": "No Data", "[Synthetics] Test On Minio Gateway App": "OK", "[Synthetics] Test on UAA Prod": "OK", "Solr Cases Not Dequeuing from AMQ": "OK", "Service ca-archive-api has a high error rate on env:aws-mt-prod": "OK", "DBRE Infrastructure Monitor -  Cluster shared volume functionality problem EventId 5123  on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor -  Cluster shared volume functionality problem EventId 5135  on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor -  Cluster shared volume functionality problem EventId 5134  on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor -  Cluster shared volume functionality problem EventId 5142  on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor -  Cluster shared volume functionality problem EventId 5200  on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor -  Cluster shared volume functionality problem EventId 5121  on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor -  Cluster shared volume functionality problem EventId 5120  on {{event.host.name}}": "OK", "Solr AMQ Cases Composite Dequeue//Backlog Monitor": "OK", "Mongodb Network Peaked in {{environment}} on {{cluster_name}} cluster": "Alert", "Monitor Name: property not configured or property mis-configured": "No Data", "Cloud Capture RabbitMQ notification callback error queue depth monitor": "OK", "Cloud Capture RabbitMQ transformer error queue depth monitor": "OK", "Cloud Capture RabbitMQ scheduler callback error queue depth monitor": "OK", "Resource get-content-api has a high error rate on env:aws-mt-prod": "OK", "Resource get-content-api has an abnormal change in throughput on env:aws-mt-prod": "OK", "Participants CSV is not imported on Env:{{env.name}}  for Customer:{{customer.name}}": "OK", "Exports Running for more than 24 hours": "Alert", "Host is down Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for exim relay nodes": "OK", "Disk high on {{host.name}}": "OK", "Property does not exist": "OK", "Service exportstopology has a high p90 latency on env:aws-mt-prod": "OK", "[Synthetics] Test on smtp-mt.smarsh.cloud": "OK", "[Synthetics] [ProArchive] 3PP Provisioning Service": "OK", "Space Alert {{host.name}}": "OK", "[Synthetics] Test on journal03.smarsh.com": "OK", "[Synthetics] Test on journal04.smarsh.com": "OK", "DBRE Infrastructure Monitor -  Cluster Backup and restore functionality problem EventID 1541 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor -  Cluster Backup and restore functionality problem EventID 1542 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor -  Cluster Backup and restore functionality problem EventID 1543 on {{event.host.name}}": "OK", "Disk Usage is high for exim Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, device: {{host.device}} for exim relay nodes": "OK", "Service exportstopology has a high average latency on env:aws-perf": "OK", "Property : TempFilePath not configured Properly {{env}}": "No Data", "test case_management Lag is greater than threshold from last 1 hour": "OK", "Number of ERROR traces high in Export Topology": "OK", "DBRE Application Monitor - SQL Server Backup Failures EventID 3041 on {{event.host.name}}": "OK", "DBRE Application Monitor - SQL Server Backup Failures EventID 18204 on {{event.host.name}}": "OK", "Test alert": "OK", "DBRE Application Monitor : Space alert on {{instance.name}}": "OK", "test1": "No Data", "[Synthetics] schwab-prod : Test on pstexport-daemon-app": "OK", "[Synthetics] mt-prod : Test on pstexport-daemon-app": "OK", "[Synthetics] mt-prod : Test on export-daemon-app": "OK", "[Synthetics] schwab-prod : Test on export-daemon-app": "OK", "test-tracer": "No Data", "[Synthetics] [ProArchive] Identity.smarsh.com/PingFederate": "OK", "Parking documents failure - Mongo connection issue": "No Data", "Elasticsearch Cluster Health status - jpmc-pri-nam-data": "OK", "Elasticsearch Cluster Health status - jpmc-pri-nam-report": "OK", "Platform Cert Expiration Warning including non-configurable Certs": "Alert", "Elasticsearch indexing index total - jpmc-pri-nam-report": "Alert", "Elasticsearch pending tasks total - jpmc-pri-nam-report": "OK", "Elasticsearch thread pool write queue - jpmc-pri-nam-report": "OK", "Elasticsearch pending tasks priority urgent - jpmc-pri-nam-report": "OK", "Elasticsearch pending tasks priority high - jpmc-pri-nam-report": "OK", "Elasticsearch number of nodes - jpmc-pri-nam-report": "OK", "Elasticsearch thread pool search queue - jpmc-pri-nam-report": "OK", "Elasticsearch pending tasks total - jpmc-nam-sec-report": "OK", "Elasticsearch pending tasks priority high - jpmc-nam-sec-report": "OK", "Elasticsearch number of nodes - jpmc-nam-sec-report": "OK", "Elasticsearch thread pool search queue - jpmc-nam-sec-report": "OK", "Elasticsearch indexing index total - jpmc-nam-sec-report": "Alert", "Elasticsearch pending tasks priority urgent - jpmc-nam-sec-report": "OK", "Elasticsearch thread pool write queue - jpmc-nam-sec-report": "OK", "Elasticsearch Cluster Health status - jpmc-nam-sec-report": "OK", "Elasticsearch pending tasks total - jpmc-nam-sec-data": "OK", "Elasticsearch thread pool write queue - jpmc-nam-sec-data": "OK", "Elasticsearch thread pool search queue - jpmc-nam-sec-data": "OK", "Elasticsearch number of nodes - jpmc-nam-sec-data": "OK", "Elasticsearch pending tasks priority urgent - jpmc-nam-sec-data": "OK", "Elasticsearch Cluster Health status - jpmc-nam-sec-data": "OK", "Elasticsearch pending tasks priority high - jpmc-nam-sec-data": "OK", "Elasticsearch indexing index total - jpmc-nam-sec-data": "Alert", "Wrong configuration for purge max replay count": "OK", "APAC Sandbox Disk Usage is high  {{environment}} Host:{{host}}, IP: {{ip}} Device: {{device.name}} {{bosh_deployment.name}} {{deployment.name}}": "Alert", "Elasticsearch Cluster Health status - jpmc-sandbox-emea-data": "OK", "Elasticsearch Cluster Health status - jpmc-emea-pri-report": "OK", "Elasticsearch number of nodes - jpmc-emea-pri-report": "OK", "Elasticsearch indexing index total - jpmc-emea-pri-report": "Alert", "Elasticsearch thread pool write queue - jpmc-emea-pri-report": "OK", "Elasticsearch pending tasks priority high - jpmc-emea-pri-report": "OK", "Elasticsearch pending tasks total - jpmc-emea-pri-report": "OK", "Elasticsearch pending tasks priority urgent - jpmc-emea-pri-report": "OK", "Elasticsearch thread pool search queue - jpmc-emea-pri-report": "OK", "Elasticsearch indexing index total - jpmc-emea-pri-data": "Alert", "Elasticsearch pending tasks priority urgent - jpmc-emea-pri-data": "OK", "Elasticsearch thread pool search queue - jpmc-emea-pri-data": "OK", "Elasticsearch pending tasks priority high - jpmc-emea-pri-data": "OK", "Elasticsearch number of nodes - jpmc-emea-pri-data": "OK", "Elasticsearch thread pool write queue - jpmc-emea-pri-data": "OK", "Elasticsearch pending tasks total - jpmc-emea-pri-data": "OK", "Elasticsearch Cluster Health status - jpmc-emea-pri-data": "OK", "Elasticsearch number of nodes - jpmc-emea-sec-report": "OK", "Elasticsearch Cluster Health status - jpmc-emea-sec-report": "OK", "Elasticsearch thread pool write queue - jpmc-emea-sec-report": "OK", "Elasticsearch pending tasks priority high - jpmc-emea-sec-report": "OK", "Elasticsearch thread pool search queue - jpmc-emea-sec-report": "OK", "Elasticsearch indexing index total - jpmc-emea-sec-report": "Alert", "Elasticsearch pending tasks priority urgent - jpmc-emea-sec-report": "OK", "Elasticsearch pending tasks total - jpmc-emea-sec-report": "OK", "Elasticsearch number of nodes - jpmc-emea-sec-data": "OK", "Elasticsearch thread pool search queue - jpmc-emea-sec-data": "OK", "Elasticsearch Cluster Health status - jpmc-emea-sec-data": "OK", "Elasticsearch indexing index total - jpmc-emea-sec-data": "Alert", "Elasticsearch pending tasks total - jpmc-emea-sec-data": "OK", "Elasticsearch thread pool write queue - jpmc-emea-sec-data": "OK", "Elasticsearch pending tasks priority high - jpmc-emea-sec-data": "OK", "Elasticsearch pending tasks priority urgent - jpmc-emea-sec-data": "OK", "Purge drain rate - monitor": "Alert", "Exports with Large number of snapshots running": "OK", "Purge- lag monitor": "OK", "Purge_pipeline_Lag_and drain_rate_monitor": "OK", "Elasticsearch pending tasks total - jpmc-apac-sec-report": "OK", "Elasticsearch pending tasks priority high - jpmc-apac-sec-report": "OK", "Elasticsearch indexing index total - jpmc-apac-sec-report": "Alert", "Elasticsearch thread pool write queue - jpmc-apac-sec-report": "OK", "Elasticsearch Cluster Health status - jpmc-apac-sec-report": "OK", "Elasticsearch pending tasks priority urgent - jpmc-apac-sec-report": "OK", "Elasticsearch Cluster Health status - jpmc-apac-pri-report": "OK", "Elasticsearch indexing index total - jpmc-apac-pri-report": "Alert", "Elasticsearch pending tasks priority urgent - jpmc-apac-pri-report": "OK", "Elasticsearch thread pool write queue - jpmc-apac-pri-report": "OK", "Elasticsearch pending tasks total - jpmc-apac-pri-report": "OK", "Elasticsearch pending tasks priority high - jpmc-apac-pri-report": "OK", "Elasticsearch indexing index total - jpmc-apac-pri-data": "Alert", "Elasticsearch number of nodes - jpmc-apac-pri-data": "OK", "Elasticsearch Cluster Health status - jpmc-apac-pri-data": "OK", "Elasticsearch pending tasks priority urgent - jpmc-apac-pri-data": "OK", "Elasticsearch pending tasks total - jpmc-apac-pri-data": "OK", "Elasticsearch pending tasks priority high - jpmc-apac-pri-data": "OK", "Elasticsearch thread pool write queue - jpmc-apac-pri-data": "OK", "Elasticsearch thread pool search queue - jpmc-apac-pri-data": "OK", "Elasticsearch number of nodes - jpmc-apac-sec-data": "OK", "Elasticsearch pending tasks total - jpmc-apac-sec-data": "OK", "Elasticsearch Cluster Health status - jpmc-apac-sec-data": "OK", "Elasticsearch indexing index total - jpmc-apac-sec-data": "Alert", "Elasticsearch thread pool write queue - jpmc-apac-sec-data": "OK", "Elasticsearch pending tasks priority high - jpmc-apac-sec-data": "OK", "Elasticsearch thread pool search queue - jpmc-apac-sec-data": "OK", "Elasticsearch pending tasks priority urgent - jpmc-apac-sec-data": "OK", "Elasticsearch number of nodes - bny-nam-pri-supervision": "OK", "Elasticsearch pending tasks priority urgent - bny-nam-pri-supervision": "OK", "Elasticsearch Cluster Health status - bny-nam-pri-supervision": "OK", "Elasticsearch thread pool write queue - bny-nam-pri-supervision": "OK", "Elasticsearch pending tasks total - bny-nam-pri-supervision": "OK", "Elasticsearch thread pool search queue - bny-nam-pri-supervision": "OK", "Elasticsearch pending tasks priority high - bny-nam-pri-supervision": "OK", "Elasticsearch indexing index total - bny-nam-pri-supervision": "Alert", "Elasticsearch thread pool search queue - bny-nam-pri-data": "OK", "Elasticsearch pending tasks total - bny-nam-pri-data": "OK", "Elasticsearch indexing index total - bny-nam-pri-data": "Alert", "Elasticsearch pending tasks priority urgent - bny-nam-pri-data": "OK", "Elasticsearch Cluster Health status - bny-nam-pri-data": "OK", "Elasticsearch thread pool write queue - bny-nam-pri-data": "OK", "Elasticsearch number of nodes - bny-nam-pri-data": "OK", "Elasticsearch pending tasks priority high - bny-nam-pri-report": "OK", "Elasticsearch thread pool write queue - bny-nam-pri-report": "OK", "Elasticsearch pending tasks priority urgent - bny-nam-pri-report": "OK", "Elasticsearch thread pool search queue - bny-nam-pri-report": "OK", "Elasticsearch indexing index total - bny-nam-pri-report": "Alert", "Elasticsearch Cluster Health status - bny-nam-pri-report": "OK", "Elasticsearch pending tasks total - bny-nam-pri-report": "OK", "Elasticsearch number of nodes - bny-nam-pri-report": "OK", "Elasticsearch pending tasks priority high - bny-nam-pri-data": "OK", "Elasticsearch number of nodes - jpmc-emea-sandbox-report": "OK", "Elasticsearch Cluster Health status - jpmc-emea-sandbox-report": "OK", "Elasticsearch pending tasks priority high - jpmc-emea-sandbox-report": "OK", "Elasticsearch indexing index total - jpmc-emea-sandbox-report": "Alert", "Elasticsearch thread pool search queue - jpmc-emea-sandbox-report": "OK", "Elasticsearch pending tasks priority urgent - jpmc-emea-sandbox-report": "OK", "Elasticsearch pending tasks total - jpmc-emea-sandbox-report": "OK", "Elasticsearch thread pool write queue - jpmc-emea-sandbox-report": "OK", "Elasticsearch composite monitor by cluster - jpmc-emea-sandbox-report": "OK", "Elasticsearch number of nodes - jpmc-emea-sandbox-data": "OK", "Elasticsearch pending tasks priority urgent - jpmc-emea-sandbox-data": "OK", "Elasticsearch Cluster Health status - jpmc-emea-sandbox-data": "OK", "Elasticsearch thread pool search queue - jpmc-emea-sandbox-data": "OK", "Elasticsearch thread pool write queue - jpmc-emea-sandbox-data": "OK", "Elasticsearch indexing index total - jpmc-emea-sandbox-data": "Alert", "Elasticsearch pending tasks total - jpmc-emea-sandbox-data": "OK", "Elasticsearch pending tasks priority high - jpmc-emea-sandbox-data": "OK", "Elasticsearch composite monitor by cluster - jpmc-emea-sandbox-data": "OK", "Elasticsearch composite monitor by cluster - jpmc-emea-sec-report": "OK", "Elasticsearch composite monitor by cluster - jpmc-emea-sec-data": "OK", "Elasticsearch composite monitor by cluster - jpmc-emea-pri-report": "OK", "Elasticsearch composite monitor by cluster - jpmc-emea-pri-data": "OK", "Elasticsearch composite monitor by cluster - jpmc-apac-sec-report": "OK", "Elasticsearch composite monitor by cluster - jpmc-apac-pri-report": "OK", "Elasticsearch composite monitor by cluster - jpmc-apac-sec-data": "OK", "Elasticsearch composite monitor by cluster - jpmc-apac-pri-data": "OK", "Elasticsearch composite monitor by cluster - bny-nam-pri-report": "OK", "Elasticsearch composite monitor by cluster - bny-nam-pri-data": "OK", "Elasticsearch composite monitor by cluster - bny-nam-pri-supervision": "OK", "Elasticsearch composite monitor by cluster - jpmc-pri-nam-report": "OK", "Elasticsearch composite monitor by cluster - jpmc-nam-sec-report": "OK", "Elasticsearch composite monitor by cluster - jpmc-nam-sec-data": "OK", "Elasticsearch pending tasks priority high - jpmc-apac-sandbox-report": "OK", "Elasticsearch thread pool write queue - jpmc-apac-sandbox-report": "OK", "Elasticsearch pending tasks priority urgent - jpmc-apac-sandbox-report": "OK", "Elasticsearch indexing index total - jpmc-apac-sandbox-report": "Alert", "Elasticsearch Cluster Health status - jpmc-apac-sandbox-report": "OK", "Elasticsearch pending tasks total - jpmc-apac-sandbox-report": "OK", "Elasticsearch composite monitor by cluster - jpmc-apac-sandbox-report": "OK", "Elasticsearch number of nodes - jpmc-pri-nam-data": "OK", "Elasticsearch pending tasks priority high - jpmc-pri-nam-data": "OK", "Elasticsearch indexing index total - jpmc-pri-nam-data": "Alert", "Elasticsearch thread pool write queue - jpmc-pri-nam-data": "OK", "Elasticsearch pending tasks priority urgent - jpmc-pri-nam-data": "OK", "Elasticsearch pending tasks total - jpmc-pri-nam-data": "OK", "Elasticsearch thread pool search queue - jpmc-pri-nam-data": "OK", "Elasticsearch composite monitor by cluster - jpmc-pri-nam-data": "OK", "{{env.name}}: Purge pipeline : uptime status (-1 component)": "Alert", "{{env.name}}: Purge pipeline: uptime status(0 component)": "Alert", "{{env.name}} Purge pipeline: Uptime monitor": "Alert", "ThomsonReutersTransformer Service Monitor": "OK", "Count of Super Indices exceeds 3 for last 24 Hours {{env.name}}{{metric_tag.name}}": "OK", "PROD-EMEA-Primary : Low Bulkindexing throughput alert for last 4hour(Jithin/Anshu/Pradeep)": "OK", "PROD-NAM-Primary : Low Bulkindexing throughput alert for last 4hour (Vikram / Akshay / Puspendu)": "Alert", "PROD-EMEA-UAT : Low Bulkindexing throughput alert for last 4hour(Jithin/Anshu/Pradeep)": "OK", "\"[OpsGenie Testing-Do not delete] VCO Email Backlog\" ,{{#is_alert}} \"DOWN\" {{/is_alert}} , {{#is_alert}} \"cmp_[Cap_Email]:[degraded_performance]\" {{/is_alert}} {{#is_warning}} DOWN {{/is_warning}} {{#is_recovery}} UP , cmp_[Cap_Email]:[operational] {{/is_recovery}}\"": "Alert", "sdfsdfsd": "OK", "Elasticsearch Cluster Health status: apac01essreport": "OK", "Elasticsearch Cluster Health status: bnyemeaproddr-essreport-cluster": "No Data", "Elasticsearch Cluster Health status: jpmcuat01essreport": "OK", "Elasticsearch Cluster Health status: bnyusproddr-essreport-cluster": "No Data", "Elasticsearch Cluster Health status: jpdressreport": "OK", "Elasticsearch Cluster Health status: jpdressdata": "OK", "Elasticsearch Cluster Health status: bnyemeaprod-essdata-cluster": "OK", "Elasticsearch Cluster Health status: bnyusprod-essdata-cluster": "OK", "Elasticsearch Cluster Health status: jpusessdata": "OK", "Elasticsearch Cluster Health status: apac01essdata": "OK", "Elasticsearch Cluster Health status: apdr01essreport": "OK", "Elasticsearch Cluster Health status: bnyusprod-essreport-cluster": "OK", "Elasticsearch Cluster Health status: bnyemeaproddr-essdata-cluster": "No Data", "Elasticsearch Cluster Health status: bnyemeaprod-essreport-cluster": "OK", "Elasticsearch Cluster Health status: emeaessdata": "OK", "Elasticsearch Cluster Health status: bnyusproddr-essdata-cluster": "No Data", "Elasticsearch Cluster Health status: apdr01essdata": "OK", "Elasticsearch Cluster Health status: emeaessreports": "OK", "Elasticsearch Cluster Health status: jpmcuat01essdata": "OK", "Elasticsearch Cluster Health status: jpusessreport": "OK", "Ceph Composite Monitor": "Warn", "PROD-EMEA-DR : Low Bulkindexing throughput alert for last 4hour(Jithin/Anshu/Pradeep)": "OK", "PROD-APAC-PRI : Low Bulkindexing throughput alert for last 4hour(Puspendu/Pradeep)": "OK", "PROD-APAC-PRI : Low Reindex Metrics throughput alert for last 4hour(Puspendu/Pradeep)": "OK", "Needs Attention : Exports jobs stuck in Job request config": "No Data", "PROD-APAC-SECONDARY : Low Bulkindexing throughput alert for last 4hour(Puspendu/Pradeep)": "OK", "Elasticsearch number of nodes - jpmc-sandbox-report": "OK", "Elasticsearch Cluster Health status - jpmc-sandbox-report": "OK", "Elasticsearch thread pool search queue - jpmc-sandbox-report": "OK", "Elasticsearch indexing index total - jpmc-sandbox-report": "Alert", "Elasticsearch thread pool write queue - jpmc-sandbox-report": "OK", "Elasticsearch pending tasks priority high - jpmc-sandbox-report": "OK", "Elasticsearch pending tasks total - jpmc-sandbox-report": "OK", "Elasticsearch pending tasks priority urgent - jpmc-sandbox-report": "OK", "Elasticsearch composite monitor by cluster - jpmc-sandbox-report": "OK", "Elasticsearch thread pool search queue - jpmc-sandbox-data": "OK", "Elasticsearch indexing index total - jpmc-sandbox-data": "OK", "Elasticsearch pending tasks priority urgent - jpmc-sandbox-data": "OK", "Elasticsearch thread pool write queue - jpmc-sandbox-data": "OK", "Elasticsearch Cluster Health status - jpmc-sandbox-data": "Alert", "Elasticsearch pending tasks total - jpmc-sandbox-data": "OK", "Elasticsearch pending tasks priority high - jpmc-sandbox-data": "OK", "Elasticsearch number of nodes - jpmc-sandbox-data": "OK", "Elasticsearch composite monitor by cluster - jpmc-sandbox-data": "OK", "Export All Topology Down": "Alert", "Ediscovery Export Topology Down": "Alert", "Supervision Export Topology Down": "Alert", "Disk Usage is high Env:{{host.env}}, Host:{{host}}, IP: {{host.ip}}, for role kafka": "OK", "Archive Export Topology Down": "Alert", "MSG & Non Email Topology Down": "Alert", "[Synthetics] PaaS-Engineering - PCF Prod API": "OK", "[Synthetics] GET Test on stage.mobileguard.com": "OK", "[Synthetics] GET Test on dev.mobileguard.com": "OK", "[Synthetics] GET Test on qa.mobileguard.com": "OK", "[Synthetics] GET Test on wfg.mobileguard.com": "OK", "[Synthetics] GET Test on app.mobileguard.com": "OK", "[Synthetics] GET Test on app.textguard.com": "OK", "[Synthetics] GET Test on boa.mobileguard.com": "OK", "[Synthetics] GET Test on cib.mobileguard.com": "OK", "[Synthetics] GET Test on hartford.mobileguard.com": "OK", "[Synthetics] GET Test on smarsh.mobileguard.com": "OK", "[Synthetics] GET Test on stifel.mobileguard.com": "OK", "[Synthetics] GET Test on flowersfoods.mobileguard.com": "OK", "[Synthetics] GET Test on govcloud.mobileguard.com": "OK", "[Synthetics] GET Test on plic.mobileguard.com": "OK", "[Synthetics] GET Test on ocfl.mobileguard.com": "OK", "[Synthetics] GET Test on wadoc.mobileguard.com": "OK", "[Synthetics] GET Test on metlife.mobileguard.com": "OK", "[Synthetics] GET Test on mcq.mobileguard.com": "OK", "[Synthetics] GET Test on fmr.mobileguard.com": "OK", "[Synthetics] bnynam : Test on pstexport-daemon-app": "OK", "[Synthetics] bnynam : Test on export-daemon-app": "OK", "Average size  less than 100 MB": "No Data", "Export Docs count > 10": "No Data", "Exports Topology Native Size Rate  < 100 Bytes Native Size Processed > 1KB": "OK", "CC-prod-MongoDB-H1-CPU-utilization": "OK", "CC-prod-MongoDB-H2-CPU-utilization": "OK", "CC-prod-MongoDB-H3-CPU-utilization": "OK", "DBRE Infrastructure Monitor - WebArchive Host is down on {{host.name}}": "OK", "DBRE Infrastructure Monitor - WebArchive Instance is down on {{host.name}}": "OK", "DBRE - WebArchive Replication - Slave Running Monitor": "OK", "DNS Resolution - SmarshINC": "Alert", "Email Reports Monitor": "OK", "Elasticsearch Cluster Health status: mt-prod-supervision-cluster": "OK", "Elasticsearch Cluster Health status: schwab-prod-essdata-cluster": "OK", "Elasticsearch Cluster Health status: mt-prod-essdata-cluster": "OK", "Elasticsearch Cluster Health status: bnyemeauat-essreport-cluster": "OK", "Elasticsearch Cluster Health status: emdressdata": "OK", "Elasticsearch Cluster Health status: demo-essdata-cluster": "OK", "Elasticsearch Cluster Health status: emdressreports": "OK", "Elasticsearch Cluster Health status: emuat01essreport": "OK", "Elasticsearch Cluster Health status: emuat01essdata": "OK", "Elasticsearch Cluster Health status: mt-prod-aac-cluster": "OK", "Elasticsearch Cluster Health status: bnyemeauat-essdata-cluster": "OK", "Elasticsearch Cluster Health status: demo-essreport-cluster": "OK", "Elasticsearch Cluster Health status: mt-prod-essreport-cluster": "OK", "Elasticsearch Cluster Health status: schwab-prod-essreport-cluster": "OK", "Email Ingestion Rate": "Alert", "Email Backlog": "Alert", "[Synthetics] GET Test on vantage.mobileguard.com": "OK", "PROD-NAM-Secondary : Low Bulkindexing throughput alert for last 4hour (Vikram / Akshay / Puspendu)": "Alert", "[Synthetics] Synthetics Browser Test - vantage-demo-app.mobileguard.com:8443/ima/login.do": "OK", "[Synthetics] Synthetics Browser Test - vantage-demo-app02.mobileguard.com:8443/ima/login.mi": "OK", "host down": "OK", "Email Ingestion && Backlog Composite Monitor - TEST": "Alert", "Concourse: AWS-Prod Worker CPU load average 15 is very high on {{host.name}}": "Alert", "NatWest Disk Usage is high Env:{{env}}, Host:{{host}}, IP: {{ip}}, device: {{device}} hostGroup {{host_group}}": "OK", "Email Composite Ingestion//Backlog Monitor": "Alert", "EMEA Sandbox Disk Usage is high  {{environment}} Host:{{host}}, IP: {{ip}} Device: {{device.name}} {{bosh_deployment.name}} {{deployment.name}} {{env.name}}": "OK", "Elasticsearch Cluster Capacity status: bnyemeaprod-essreport-cluster": "OK", "Elasticsearch Cluster Capacity status: mt-prod-aac-cluster": "OK", "Elasticsearch Cluster Capacity status: bnyemeaproddr-essdata-cluster": "OK", "Elasticsearch Cluster Capacity status: jpmcuat01essdata": "OK", "Elasticsearch Cluster Capacity status: apac01essdata": "Alert", "Elasticsearch Cluster Capacity status: bnyemeauat-essdata-cluster": "OK", "Elasticsearch Cluster Capacity status: bnyusprod-essreport-cluster": "OK", "Elasticsearch Cluster Capacity status: apdr01essreport": "OK", "Elasticsearch Cluster Capacity status: emdressreports": "OK", "Elasticsearch Cluster Capacity status: jpdressdata": "Warn", "Elasticsearch Cluster Capacity status: emeaessdata": "Alert", "Elasticsearch Cluster Capacity status: emuat01essreport": "OK", "Elasticsearch Cluster Capacity status: bnyemeauat-essreport-cluster": "OK", "Elasticsearch Cluster Capacity status: schwab-prod-essdata-cluster": "OK", "Elasticsearch Cluster Capacity status: apdr01essdata": "Alert", "Elasticsearch Cluster Capacity status: apac01essreport": "OK", "Elasticsearch Cluster Capacity status: jpdressreport": "Warn", "Elasticsearch Cluster Capacity status: demo-essdata-cluster": "OK", "Elasticsearch Cluster Capacity status: jpusessreport": "Warn", "Elasticsearch Cluster Capacity status: jpusessdata": "OK", "Elasticsearch Cluster Capacity status: emeaessreports": "Warn", "Elasticsearch Cluster Capacity status: mt-prod-essreport-cluster": "OK", "Elasticsearch Cluster Capacity status: bnyemeaprod-essdata-cluster": "OK", "Elasticsearch Cluster Capacity status: bnyusprod-essdata-cluster": "OK", "Elasticsearch Cluster Capacity status: schwab-prod-essreport-cluster": "OK", "Elasticsearch Cluster Capacity status: mt-prod-supervision-cluster": "OK", "Elasticsearch Cluster Capacity status: bnyemeaproddr-essreport-cluster": "OK", "Elasticsearch Cluster Capacity status: bnyusproddr-essdata-cluster": "OK", "Elasticsearch Cluster Capacity status: emuat01essdata": "OK", "Elasticsearch Cluster Capacity status: demo-essreport-cluster": "OK", "Elasticsearch Cluster Capacity status: mt-prod-essdata-cluster": "Warn", "Elasticsearch Cluster Capacity status: emdressdata": "Warn", "Elasticsearch Cluster Capacity status: bnyusproddr-essreport-cluster": "OK", "Elasticsearch Cluster Capacity status: jpmcuat01essreport": "OK", "Vault Connection Failure on {{cluster_name}} on {{host}}": "No Data", "Vault Unsealed in {{cluster_name}} on {{host}}": "OK", "Vault is not Initialised in {{cluster_name}} on {{host}}": "OK", "Exports topology Native Rate is less than 100": "OK", "Exports Topology Snapshot Count > 100": "OK", "test monitor for oxygen": "OK", "Elasticsearch Cluster Capacity status: bny-nam-pri-supervision": "OK", "Elasticsearch Cluster Capacity status: bny-nam-pri-report": "OK", "Elasticsearch Cluster Capacity status: bny-nam-pri-data": "OK", "Test credhub": "OK", "Elasticsearch Cluster Capacity status: jpmc-emea-pri-data": "OK", "Elasticsearch Cluster Capacity status: jpmc-pri-nam-data": "OK", "Elasticsearch Cluster Capacity status: jpmc-pri-nam-report": "OK", "Elasticsearch Cluster Capacity status: jpmc-emea-sandbox-report": "OK", "Elasticsearch Cluster Capacity status: jpmc-emea-sandbox-data": "OK", "Elasticsearch Cluster Capacity status: jpmc-emea-pri-report": "OK", "Elasticsearch Cluster Capacity status: jpmc-emea-sec-report": "OK", "Elasticsearch Cluster Capacity status: jpmc-emea-sec-data": "OK", "Elasticsearch Cluster Health status: bosh_es": "OK", "Test for missing data to datadog.": "No Data", "Export Packager Process Size Rate < 100 Bytes": "Alert", "Exports Packager  Process Rate < 100 Bytes and Total Size > 10 KB": "OK", "Data Publisher Process Rate < 10 Docs/sec": "Alert", "Cloud Capture RabbitMQ joiner error queue depth monitor": "OK", "Cloud Capture RabbitMQ notification messages error queue depth monitor": "OK", "Cloud Capture RabbitMQ events error queue depth monitor": "OK", "Elasticsearch indexing index total - bny-apac-pri-supervision": "Alert", "Elasticsearch thread pool write queue - bny-apac-pri-supervision": "OK", "Elasticsearch thread pool search queue - bny-apac-pri-supervision": "OK", "Elasticsearch pending tasks total - bny-apac-pri-supervision": "OK", "Elasticsearch number of nodes - bny-apac-pri-supervision": "OK", "Elasticsearch Cluster Health status - bny-apac-pri-supervision": "OK", "Elasticsearch Cluster Capacity status: bny-apac-pri-supervision": "OK", "Elasticsearch pending tasks priority urgent - bny-apac-pri-supervision": "OK", "Elasticsearch pending tasks priority high - bny-apac-pri-supervision": "OK", "Elasticsearch composite monitor by cluster - bny-apac-pri-supervision": "OK", "Elasticsearch pending tasks priority high - bny-apac-pri-report": "OK", "Elasticsearch Cluster Health status - bny-apac-pri-report": "OK", "Elasticsearch thread pool search queue - bny-apac-pri-report": "OK", "Elasticsearch pending tasks priority urgent - bny-apac-pri-report": "OK", "Elasticsearch indexing index total - bny-apac-pri-report": "Alert", "Elasticsearch thread pool write queue - bny-apac-pri-report": "OK", "Elasticsearch Cluster Capacity status: bny-apac-pri-report": "OK", "Elasticsearch number of nodes - bny-apac-pri-report": "OK", "Elasticsearch pending tasks total - bny-apac-pri-report": "OK", "Elasticsearch composite monitor by cluster - bny-apac-pri-report": "OK", "Elasticsearch number of nodes - bny-apac-pri-data": "OK", "Elasticsearch thread pool search queue - bny-apac-pri-data": "OK", "Elasticsearch pending tasks priority high - bny-apac-pri-data": "OK", "Elasticsearch pending tasks priority urgent - bny-apac-pri-data": "OK", "Elasticsearch indexing index total - bny-apac-pri-data": "Alert", "Elasticsearch thread pool write queue - bny-apac-pri-data": "OK", "Elasticsearch pending tasks total - bny-apac-pri-data": "OK", "Elasticsearch Cluster Capacity status: bny-apac-pri-data": "OK", "Elasticsearch Cluster Health status - bny-apac-pri-data": "OK", "Elasticsearch composite monitor by cluster - bny-apac-pri-data": "OK", "Elasticsearch Cluster Health status - mt-ca-pri-supervision": "OK", "Elasticsearch thread pool write queue - mt-ca-pri-supervision": "OK", "Elasticsearch pending tasks priority high - mt-ca-pri-supervision": "OK", "Elasticsearch indexing index total - mt-ca-pri-supervision": "Alert", "Elasticsearch pending tasks priority urgent - mt-ca-pri-supervision": "OK", "Elasticsearch pending tasks total - mt-ca-pri-supervision": "OK", "Elasticsearch thread pool search queue - mt-ca-pri-supervision": "OK", "Elasticsearch number of nodes - mt-ca-pri-supervision": "OK", "Elasticsearch Cluster Capacity status: mt-ca-pri-supervision": "OK", "Elasticsearch composite monitor by cluster - mt-ca-pri-supervision": "OK", "Elasticsearch indexing index total - mt-ca-pri-report": "Alert", "Elasticsearch thread pool write queue - mt-ca-pri-report": "OK", "Elasticsearch thread pool search queue - mt-ca-pri-report": "OK", "Elasticsearch pending tasks priority high - mt-ca-pri-report": "OK", "Elasticsearch number of nodes - mt-ca-pri-report": "OK", "Elasticsearch pending tasks total - mt-ca-pri-report": "OK", "Elasticsearch pending tasks priority urgent - mt-ca-pri-report": "OK", "Elasticsearch Cluster Capacity status: mt-ca-pri-report": "OK", "Elasticsearch composite monitor by cluster - mt-ca-pri-report": "OK", "Elasticsearch Cluster Health status - mt-ca-pri-report": "OK", "Elasticsearch indexing index total - mt-ca-pri-data": "Alert", "Elasticsearch thread pool write queue - mt-ca-pri-data": "OK", "Elasticsearch pending tasks priority high - mt-ca-pri-data": "OK", "Elasticsearch Cluster Health status - mt-ca-pri-data": "OK", "Elasticsearch pending tasks total - mt-ca-pri-data": "OK", "Elasticsearch thread pool search queue - mt-ca-pri-data": "OK", "Elasticsearch number of nodes - mt-ca-pri-data": "OK", "Elasticsearch pending tasks priority urgent - mt-ca-pri-data": "OK", "Elasticsearch Cluster Capacity status: mt-ca-pri-data": "OK", "Elasticsearch composite monitor by cluster - mt-ca-pri-data": "OK", "Elasticsearch indexing index total - bny-emea-pri-supervision": "Alert", "Elasticsearch thread pool search queue - bny-emea-pri-supervision": "OK", "Elasticsearch pending tasks priority urgent - bny-emea-pri-supervision": "OK", "Elasticsearch pending tasks priority high - bny-emea-pri-supervision": "OK", "Elasticsearch thread pool write queue - bny-emea-pri-supervision": "OK", "Elasticsearch pending tasks total - bny-emea-pri-supervision": "OK", "Elasticsearch Cluster Health status - bny-emea-pri-supervision": "OK", "Elasticsearch number of nodes - bny-emea-pri-supervision": "OK", "Elasticsearch Cluster Capacity status: bny-emea-pri-supervision": "OK", "Elasticsearch composite monitor by cluster - bny-emea-pri-supervision": "OK", "Elasticsearch indexing index total - bny-emea-pri-report": "Alert", "Elasticsearch thread pool search queue - bny-emea-pri-report": "OK", "Elasticsearch pending tasks total - bny-emea-pri-report": "OK", "Elasticsearch thread pool write queue - bny-emea-pri-report": "OK", "Elasticsearch pending tasks priority urgent - bny-emea-pri-report": "OK", "Elasticsearch number of nodes - bny-emea-pri-report": "OK", "Elasticsearch Cluster Capacity status: bny-emea-pri-report": "OK", "Elasticsearch pending tasks priority high - bny-emea-pri-report": "OK", "Elasticsearch Cluster Health status - bny-emea-pri-report": "OK", "Elasticsearch composite monitor by cluster - bny-emea-pri-report": "OK", "Elasticsearch pending tasks priority high - bny-emea-pri-data": "OK", "Elasticsearch number of nodes - bny-emea-pri-data": "OK", "Elasticsearch thread pool write queue - bny-emea-pri-data": "OK", "Elasticsearch pending tasks total - bny-emea-pri-data": "OK", "Elasticsearch Cluster Capacity status: bny-emea-pri-data": "OK", "Elasticsearch Cluster Health status - bny-emea-pri-data": "OK", "Elasticsearch indexing index total - bny-emea-pri-data": "Alert", "Elasticsearch thread pool search queue - bny-emea-pri-data": "OK", "Elasticsearch pending tasks priority urgent - bny-emea-pri-data": "OK", "Elasticsearch composite monitor by cluster - bny-emea-pri-data": "OK", "EGW_RMQ_PENDING_MESSAGESS": "OK", "EGW_RMQ_PUBLISHED_RATE": "Alert", "JPMC_EMEA_SANDBOX_EGW_RMQ_INCOMING_QUEUE_depth": "OK", "JPMC_APAC_SANDBOX_EGW_RMQ_INCOMING_QUEUE_depth": "OK", "CC-prod-postgres-free-storage-prva-db": "OK", "CC-prod-postgres-free-storage-shda-db": "OK", "CC-prod-postgres-free-storage-kymc-db": "OK", "CC-prod-postgres-free-storage-cada-db": "OK", "CC-prod-postgres-free-storage-alca-db": "OK", "EGW: JPMC APAC UAT/SL RabbitMQ  {{bosh_deployment.name}}  High Disk Water Mark": "OK", "Cloud Capture RabbitMQ -  Free Disk Alert": "OK", "Cloud Capture RabbitMQ Heartbeat": "OK", "Cloud Capture RabbitMQ Alarm - Disk": "OK", "Elasticsearch Cluster Capacity status: jpmc-sandbox-report": "OK", "Retrieval Service Container Count{{#is_alert}} is low{{/is_alert}}{{#is_no_data}} not reporting{{/is_no_data}}{{#is_recovery}} recovered{{/is_recovery}}": "OK", "Retrieval Service Load Balancer (nginx){{#is_alert}} not available{{/is_alert}}{{#is_no_data}} not reporting{{/is_no_data}}": "OK", "Retrieval Service Redis{{#is_alert}} not available{{/is_alert}}{{#is_no_data}} not reporting{{/is_no_data}}": "OK", "Data Publisher document count > 100": "OK", "Data publisher Processed > 100 with rate < 10 doc/sec": "OK", "Exports Packager Total Raw file Size > 10 KB": "OK", "Exports Topology Native Size Rate < 100 Bytes": "Alert", "Exports Topology Native Size  > 1KB": "OK", "Elasticsearch Cluster Capacity status: jpmc-sandbox-data": "Alert", "Elasticsearch Cluster Capacity status: jpmc-nam-sec-data": "OK", "Elasticsearch Cluster Capacity status: jpmc-nam-sec-report": "OK", "Kafka made an unclean leader election in jpmc-nam-secondary-kafka": "OK", "Kafka made an unclean leader election in jpmc-nam-primary-kafka": "OK", "Kafka Cluster Capacity status: kafka-bnyusproddr": "OK", "Kafka active controller count for kafka-bnyusprod-primary": "OK", "Kafka active controller count for jpmc-nam-secondary-kafka": "OK", "Kafka active controller count for jpmc-nam-primary-kafka": "OK", "Kafka active controller count for bny-nam-pri-kafka": "No Data", "Kafka active controller count for kafka-emea-uat": "OK", "Kafka active controller count for kafka-mt-prod": "OK", "Kafka active controller count for kafka-prod": "OK", "Kafka active controller count for kafka-bnyusproddr": "OK", "Kafka active controller count for kafka-schwab-prod": "OK", "Kafka active controller count for kafka-bnyemeadr": "OK", "Kafka Cluster Capacity status: kafka-emea-uat": "OK", "Kafka Cluster Capacity status: kafka-bnyusprod-primary": "OK", "Kafka Cluster Capacity status: bny-nam-pri-kafka": "OK", "Kafka Cluster Capacity status: kafka-bnyemeadr": "OK", "Kafka Cluster Capacity status: kafka-schwab-prod": "OK", "Kafka Cluster Capacity status: kafka-prod": "OK", "Kafka Cluster Capacity status: kafka-mt-prod": "OK", "Kafka has an offline partition in kafka-prod": "OK", "Kafka has an offline partition in kafka-mt-prod": "OK", "Kafka has an offline partition in bny-nam-pri-kafka": "No Data", "Kafka has an offline partition in kafka-schwab-prod": "OK", "Kafka has an offline partition in kafka-bnyemeadr": "OK", "Kafka has an offline partition in kafka-bnyusprod-primary": "OK", "Kafka has an offline partition in kafka-emea-uat": "OK", "Kafka has an offline partition in kafka-bnyusproddr": "OK", "Kafka has under replicated partitions in jpmc-nam-secondary-kafka": "OK", "Kafka has under replicated partitions in kafka-mt-prod": "OK", "Kafka has under replicated partitions in kafka-bnyemeadr": "OK", "Kafka has under replicated partitions in jpmc-nam-primary-kafka": "OK", "Kafka has under replicated partitions in kafka-bnyusproddr": "OK", "Kafka has under replicated partitions in kafka-schwab-prod": "OK", "Kafka has under replicated partitions in kafka-bnyusprod-primary": "OK", "Kafka has under replicated partitions in kafka-prod": "OK", "Kafka has under replicated partitions in bny-nam-pri-kafka": "No Data", "Kafka has under replicated partitions in kafka-emea-uat": "OK", "Kafka made an unclean leader election in kafka-bnyusprod-primary": "OK", "Kafka made an unclean leader election in bny-nam-pri-kafka": "No Data", "Kafka made an unclean leader election in kafka-bnyemeadr": "OK", "Kafka made an unclean leader election in kafka-mt-prod": "OK", "Kafka made an unclean leader election in kafka-emea-uat": "OK", "Kafka made an unclean leader election in kafka-schwab-prod": "OK", "Kafka made an unclean leader election in kafka-bnyusproddr": "OK", "Kafka made an unclean leader election in kafka-prod": "OK", "Anomaly Detection - Kafka Cluster Capcaity status: {{env}}": "No Data", "Kafka has an offline partition in jpmc-nam-secondary-kafka": "OK", "Kafka has an offline partition in jpmc-nam-primary-kafka": "OK", "Kafka Cluster Capacity status: jpmc-nam-primary-kafka": "OK", "Kafka Cluster Capacity status: jpmc-nam-secondary-kafka": "OK", "csv import cron: failed file count = 0": "Alert", "Test alert for ediscovery bulk job | kafka to job pipeline": "OK", "Test alert for ediscovery bulk job | job pipeline to hazelcast": "OK", "Cloud Capture RabbitMQ - Memory Alert": "OK", "Cloud Capture RabbitMQ Reachability": "OK", "Cloud Capture RabbitMQ open file descriptors": "OK", "CC-prod-postgres-free-storage-cada": "OK", "CC-prod-postgres-free-storage-alca": "OK", "CC-prod-postgres-free-storage-shda": "OK", "CC-prod-postgres-free-storage-prva": "OK", "CC-prod-postgres-free-storage-kymc": "OK", "Elasticsearch Cluster Health status: bnyusuat-essreport-cluster": "No Data", "Elasticsearch Cluster Health status: bnyusuat-essdata-cluster": "No Data", "Elasticsearch Cluster Capacity status: bnyusuat-essdata-cluster": "No Data", "Elasticsearch Cluster Capacity status: bnyusuat-essreport-cluster": "No Data", "Cloud Capture RabbitMQ scheduler callback  queue depth monitor": "OK", "Cloud Capture Postgres CPU Utilization - PRVA": "OK", "Cloud Capture Postgres CPU Utilization - ALCA": "OK", "Cloud Capture Postgres Free Memory - SHDA": "OK", "Cloud Capture Postgres Swap Usage - CADA": "OK", "Cloud Capture Postgres CPU Utilization - SHDA": "OK", "Cloud Capture Postgres CPU Utilization - KYMC": "OK", "Cloud Capture Postgres CPU Utilization - CADA": "OK", "Cloud Capture Postgres Swap Usage - SHDA": "OK", "Cloud Capture Postgres Free Memory - KYMC": "OK", "Cloud Capture Postgres Free Memory - PRVA": "OK", "Cloud Capture Postgres Swap Usage - KYMC": "OK", "Cloud Capture Postgres Free Memory - ALCA": "OK", "Cloud Capture Postgres Free Memory - CADA": "OK", "Cloud Capture Postgres Swap Usage - PRVA": "OK", "Cloud Capture Postgres Swap Usage - ALCA": "OK", "Elasticsearch thread pool bulk rejected - jpmc-apac-sec-data": "No Data", "Elasticsearch Cluster Capacity status: jpmc-apac-sec-data": "OK", "Elasticsearch thread pool search queue - jpmc-apac-sec-report": "OK", "Elasticsearch thread pool bulk rejected - jpmc-apac-sec-report": "No Data", "Elasticsearch number of nodes - jpmc-apac-sec-report": "OK", "Elasticsearch Cluster Capacity status: jpmc-apac-sec-report": "OK", "Elasticsearch number of nodes - jpmc-apac-pri-report": "OK", "Elasticsearch thread pool search queue - jpmc-apac-pri-report": "OK", "Elasticsearch Cluster Capacity status: jpmc-apac-pri-report": "OK", "Elasticsearch thread pool bulk rejected - jpmc-apac-pri-report": "No Data", "Elasticsearch thread pool bulk rejected - jpmc-apac-sandbox-report": "No Data", "Elasticsearch number of nodes - jpmc-apac-sandbox-report": "OK", "Elasticsearch Cluster Capacity status: jpmc-apac-sandbox-report": "OK", "Elasticsearch thread pool search queue - jpmc-apac-sandbox-report": "OK", "Elasticsearch composite monitor by nodes - jpmc-apac-sandbox-report": "No Data", "Elasticsearch pending tasks total - jpmc-apac-sandbox-data": "OK", "Elasticsearch pending tasks priority urgent - jpmc-apac-sandbox-data": "OK", "Elasticsearch indexing index total - jpmc-apac-sandbox-data": "Alert", "Elasticsearch number of nodes - jpmc-apac-sandbox-data": "OK", "Elasticsearch thread pool search queue - jpmc-apac-sandbox-data": "OK", "Elasticsearch Cluster Health status - jpmc-apac-sandbox-data": "OK", "Elasticsearch pending tasks priority high - jpmc-apac-sandbox-data": "OK", "Elasticsearch thread pool bulk rejected - jpmc-apac-sandbox-data": "No Data", "Elasticsearch Cluster Capacity status: jpmc-apac-sandbox-data": "Warn", "Elasticsearch thread pool write queue - jpmc-apac-sandbox-data": "OK", "Elasticsearch composite monitor by cluster - jpmc-apac-sandbox-data": "OK", "Elasticsearch composite monitor by nodes - jpmc-apac-sandbox-data": "No Data", "Elasticsearch Cluster Health status - jpmc-apac-sec-data - showing nodes KGS": "OK", "Kafka has an offline partition in bny-apac-pri-kafka": "No Data", "Kafka made an unclean leader election in kafka-bnyemea-prod": "OK", "Kafka has an offline partition in kafka-bnyemea-prod": "OK", "Kafka has under replicated partitions in bny-apac-pri-kafka": "No Data", "Kafka has under replicated partitions in kafka-bnyemea-prod": "OK", "Kafka Cluster Capacity status: kafka-bnyemea-prod": "OK", "Kafka has under replicated partitions in bny-emea-pri-kafka": "No Data", "Kafka has under replicated partitions in mt-ca-pri-kafka": "No Data", "Kafka has an offline partition in bny-emea-pri-kafka": "No Data", "Kafka has an offline partition in mt-ca-pri-kafka": "No Data", "Kafka active controller count for mt-ca-pri-kafka": "No Data", "Kafka active controller count for bny-apac-pri-kafka": "No Data", "Kafka active controller count for bny-emea-pri-kafka": "No Data", "Kafka made an unclean leader election in bny-apac-pri-kafka": "No Data", "Kafka made an unclean leader election in mt-ca-pri-kafka": "No Data", "Kafka made an unclean leader election in bny-emea-pri-kafka": "No Data", "Kafka Cluster Capacity status: bny-emea-pri-kafka": "No Data", "Kafka Cluster Capacity status: bny-apac-pri-kafka": "No Data", "Kafka Cluster Capacity status: mt-ca-pri-kafka": "OK", "Kafka active controller count for kafka-bnyemea-prod": "OK", "Demo: Disk space is high": "Alert", "[Synthetics] [ProArchive] Retrieval Service Ping": "OK", "Elasticsearch Cluster Health status - mt-prod-essdata-cluster - showing nodes KGS": "OK", "Elasticsearch thread pool bulk rejected - jpmc-emea-sec-data": "No Data", "Elasticsearch composite monitor by nodes - jpmc-emea-sec-data": "No Data", "No request id is triggered from case": "OK", "Lead time for ISS Build and dev-deploy on AWS-PROD env": "OK", "Test: Lead time for ISS deployment on AWS-MT": "No Data", "[Synthetics] Test-SVP-EGW on email-gateway-demo.apps.prod.smarsh.cloud": "OK", "PROD-NAM-Primary : Low Bulkindexing throughput alert": "OK", "PROD-NAM-Secondary : Low Bulkindexing throughput alert": "Alert", "test ISS build job alone - Jino": "No Data", "test ISS dev-deploy job alone - Jino": "OK", "Elasticsearch thread pool bulk rejected - jpmc-emea-pri-data": "No Data", "Elasticsearch composite monitor by nodes - jpmc-emea-pri-data": "No Data", "Elasticsearch thread pool bulk rejected - jpmc-emea-pri-report": "No Data", "Elasticsearch composite monitor by nodes - jpmc-emea-pri-report": "No Data", "Cloud Capture Postgres Write Latency - ALCA": "OK", "Cloud Capture Postgres Read Latency - ALCA": "OK", "Cloud Capture Postgres Read Latency - KYMC": "OK", "Cloud Capture Postgres Write Latency - KYMC": "OK", "Cloud Capture Postgres Write Latency - PRVA": "OK", "Cloud Capture Postgres Write Latency - CADA": "OK", "Cloud Capture Postgres Read Latency - CADA": "OK", "Cloud Capture Postgres Read Latency - PRVA": "OK", "Cloud Capture Postgres Read Latency - SHDA": "OK", "Cloud Capture Postgres Write Latency - SHDA": "OK", "Elasticsearch thread pool bulk rejected - bny-nam-pri-data": "No Data", "Elasticsearch composite monitor by nodes - bny-nam-pri-data": "No Data", "Indexing & Search SST Test Alert": "Alert", "Solr Loader Memcached{{#is_alert}} not available{{/is_alert}}{{#is_no_data}} not reporting{{/is_no_data}}": "OK", "Elasticsearch Cluster Health status - robinhood-nam-pri-report": "OK", "Elasticsearch number of nodes - robinhood-nam-pri-report": "OK", "Elasticsearch pending tasks priority urgent - robinhood-nam-pri-report": "OK", "Elasticsearch thread pool bulk rejected - robinhood-nam-pri-report": "No Data", "Elasticsearch indexing index total - robinhood-nam-pri-report": "Alert", "Elasticsearch thread pool search queue - robinhood-nam-pri-report": "OK", "Elasticsearch pending tasks priority high - robinhood-nam-pri-report": "OK", "Elasticsearch Cluster Capacity status: robinhood-nam-pri-report": "OK", "Elasticsearch thread pool write queue - robinhood-nam-pri-report": "OK", "Elasticsearch pending tasks total - robinhood-nam-pri-report": "OK", "Elasticsearch composite monitor by cluster - robinhood-nam-pri-report": "OK", "Elasticsearch composite monitor by nodes - robinhood-nam-pri-report": "No Data", "Elasticsearch indexing index total - robinhood-nam-pri-data": "OK", "Elasticsearch pending tasks priority high - robinhood-nam-pri-data": "OK", "Elasticsearch thread pool bulk rejected - robinhood-nam-pri-data": "No Data", "Elasticsearch Cluster Health status - robinhood-nam-pri-data": "OK", "Elasticsearch pending tasks total - robinhood-nam-pri-data": "OK", "Elasticsearch thread pool search queue - robinhood-nam-pri-data": "OK", "Elasticsearch pending tasks priority urgent - robinhood-nam-pri-data": "OK", "Elasticsearch Cluster Capacity status: robinhood-nam-pri-data": "OK", "Elasticsearch thread pool write queue - robinhood-nam-pri-data": "OK", "Elasticsearch number of nodes - robinhood-nam-pri-data": "OK", "Elasticsearch composite monitor by cluster - robinhood-nam-pri-data": "OK", "Elasticsearch composite monitor by nodes - robinhood-nam-pri-data": "No Data", "Preservation policy cache not refreshed - cache refresh condition": "OK", "jpmc-sandbox-data testing anomaly query rate": "OK", "jpmc-sandbox-data testing anomaly indexing rate": "OK", "bny-nam-pri-data testing anomaly query rate": "OK", "bny-nam-pri-data testing anomaly indexing rate": "OK", "APAC-PRI [Data Cluster] [CONSUMER] : 0 throughput for last 24 hours": "OK", "APAC-DR [Data Cluster] [CONSUMER] : 0 throughput for last 24 hours": "OK", "EMEA-PRI [Data Cluster] [CONSUMER] : 0 throughput for last 24 hours": "OK", "NAM-PRI [Data Cluster] [CONSUMER] : Less than 400 docs/sec throughput for last 4 hours": "Alert", "APAC-PRI [Reporting Cluster] [CONSUMER] : 0 throughput for last 24 Hours": "OK", "NAM-PRI [Data Cluster] [CONSUMER] : Less than 450 docs/sec throughput for last 12 hours": "Alert", "NAM-PRI [Data Cluster] [CONSUMER] : Less than 500 docs/sec throughput for last 24 hours": "Alert", "NAM-PRI [Data Cluster] [CONSUMER] : Less than 200 docs/sec throughput for last 2 hours": "Alert", "APAC-DR [Reporting Cluster] [CONSUMER] : 0 throughput for last 24 Hours": "OK", "EMEA-PRI [Reporting Cluster] [CONSUMER] : 0 throughput for last 24 Hours": "OK", "NAM-DR [Data Cluster] [CONSUMER] : Less than 600 docs/sec throughput for last 4 hours": "Alert", "NAM-DR [Data Cluster] [CONSUMER] : Less than 650 docs/sec throughput for last 8 hours": "Alert", "NAM-DR [Data Cluster] [CONSUMER] : Less than 700 docs/sec throughput for last 24 hours": "Alert", "NAM-DR [Data Cluster] [CONSUMER] : Less than 200 docs/sec throughput for last 2 hours": "Alert", "NAM-PRI [Reporting Cluster] [CONSUMER] Less than 400 docs/sec throughput for last 4  Hours": "OK", "NAM-PRI [Reporting Cluster] [CONSUMER] Less than 450 docs/sec throughput for last 8  Hours": "OK", "APAC-PRI [Data Cluster] [PRODUCER] : 0 throughput for last 24 hours": "OK", "NAM-PRI [Reporting Cluster] [CONSUMER] Less than 500 docs/sec throughput for last 24 Hours": "OK", "APAC-DR [Data Cluster] [PRODUCER] : 0 throughput for last 24 hours": "OK", "EMEA-PRI [Data Cluster] [PRODUCER] : 0 throughput for last 24 hours": "OK", "NAM-DR  [Reporting Cluster] [CONSUMER] Less than 600 docs/sec throughput for last 4  Hours": "OK", "NAM-DR  [Reporting Cluster] [CONSUMER] Less than 650 docs/sec throughput for last 8  Hours": "OK", "NAM-PRI [Data Cluster] [PRODUCER] : 0 throughput for last 24 hours": "Alert", "NAM-DR  [Reporting Cluster] [CONSUMER] Less than 500 docs/sec throughput for last 24 Hours": "OK", "NAM-DR [Data Cluster] [PRODUCER] : 0 throughput for last 24 hours": "Alert", "APAC-PRI [Reporting Cluster] [PRODUCER] : 0 throughput for last 24 Hours": "OK", "APAC-DR[Reporting Cluster] [PRODUCER] : 0 throughput for last 24 Hours": "OK", "EMEA-PRI [Reporting Cluster] [PRODUCER] : 0 throughput for last 24 Hours": "OK", "NAM-PRI  [Reporting Cluster] [PRODUCER]  Less than 400 docs/sec throughput for last 4  Hours": "OK", "NAM-PRI [Reporting Cluster] [PRODUCER] Less than 500 docs/sec throughput for last 24 Hours": "OK", "NAM-PRI [Reporting Cluster] [PRODUCER] Less than 450 docs/sec throughput for last 8  Hours": "OK", "NAM-DR  [Reporting Cluster] [PRODUCER]  Less than 600 docs/sec throughput for last 4  Hours": "OK", "NAM-DR  [Reporting Cluster] [PRODUCER] Less than 650 docs/sec throughput for last 8  Hours": "OK", "NAM-DR  [Reporting Cluster] [PRODUCER]  Less than 500 docs/sec throughput for last 24 Hours": "OK", "APAC-PRI [STORM] [PRODUCER] : 40K open file-handles.": "OK", "APAC-DR [STORM] [PRODUCER] : 40K open file-handles.": "OK", "EMEA-PRI [STORM] [PRODUCER] : 40K open file-handles.": "OK", "NAM-PRI [STORM] [PRODUCER] : 40K open file-handles.": "OK", "NAM-DR [STORM] [PRODUCER] : 40K open file-handles.": "OK", "APAC-DR [STORM] [CONSUMER] : 40K open file-handles.": "OK", "APAC-PRI [STORM] [CONSUMER] : 40K open file-handles.": "OK", "EMEA-PRI [STORM] [CONSUMER] : 40K open file-handles.": "OK", "NAM-DR [STORM] [CONSUMER] : 40K open file-handles.": "OK", "NAM-PRI [STORM] [CONSUMER] : 40K open file-handles.": "OK", "NAM-PRI  [Reporting Cluster] [PRODUCER]  Less than 200 docs/sec throughput for last 2  Hours": "OK", "NAM-PRI [Reporting Cluster] [CONSUMER] Less than 200 docs/sec throughput for last 2  Hours": "OK", "NAM-DR [Reporting Cluster] [PRODUCER] Less than 200 docs/sec throughput for last 2  Hours": "OK", "NAM-DR [Reporting Cluster] [CONSUMER] Less than 200 docs/sec throughput for last 2  Hours": "OK", "Test Alert for Cloud Capture Staging Transformer Error Queue depth.": "OK", "Solr Loader Container Count{{#is_alert}} is low{{/is_alert}}{{#is_no_data}} not reporting{{/is_no_data}}": "OK", "Solr Loader ActiveMQ{{#is_alert}} is not running{{/is_alert}}{{#is_no_data}} not reporting{{/is_no_data}}": "No Data", "Test: Lead time for ISS deployment on JPMC APAC Sandbox - Jino": "OK", "This Test check RabbitMQ Incoming queue depth": "OK", "NAM-PRI [Data Cluster] [CONSUMER] : More than 550 docs/sec throughput for last 4 hours": "OK", "Elasticsearch Cluster Health status - jpmc-emea-pri-data - showing nodes KGS": "OK", "Lead time for ISS deployment on Schwab": "OK", "Lead time for ISS deployment on Azure PWEZ": "OK", "Lead time for ISS deployment on BNY nam sandbox": "OK", "Lead time for ISS deployment on BNY emea sandbox": "OK", "Lead time for ISS deployment on AWS-PROD": "OK", "Lead time for ISS deployment on BNY emea primary": "OK", "Lead time for ISS deployment on JPMC apac sandbox": "OK", "Lead time for ISS deployment on BNY emea secondary": "OK", "Lead time for ISS deployment on BNY nam primary": "OK", "Lead time for ISS deployment on BNY nam secondary": "OK", "Lead time for ISS deployment on JPMC emea sandbox": "OK", "Lead time for ISS deployment on AWS DEMO": "OK", "Lead time for ISS deployment on Schwab UAT": "OK", "Lead time for ISS deployment on BNY PROD": "OK", "Solr - Replication Failure": "Alert", "JPMC_EMEA_SECONDARY_EGW_RMQ_INCOMING_QUEUE_depth": "OK", "EMEA-DR [Data Cluster] [CONSUMER] : More than 0 throughput for last 1 hours": "OK", "NAM-PRI [Data Cluster] [CONSUMER] : More than 150 requests with ES failures": "OK", "NAM-SEC [Data Cluster] [CONSUMER] : More than 150 requests with ES failures": "OK", "EMEA-PRI [Data Cluster] [CONSUMER] : More than 150 requests with ES failures": "OK", "APAC-PRI [Data Cluster] [CONSUMER] : More than 150 requests with ES failures": "OK", "APAC-SEC[Data Cluster] [CONSUMER] : More than 150 requests with ES failures": "No Data", "Solr - Out Of Memory": "OK", "[Synthetics] Schwab-Prod : Synthetic EGW TPS API Test": "OK", "[Synthetics] MT-Prod : Synthetic EGW TPS API Test": "OK", "[Synthetics] AWS-Demo : Synthetic EGW TPS API Test": "OK", "[Synthetics] TPS Health Check": "OK", "Disposition pipeline throughput [150]  - lag condition": "OK", "DBRE Infrastructure Monitor - Cluster network connectivity problems EventID 1555 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster network connectivity problems EventID 1360 on {{event.host.name}}": "OK", "DBRE Infrastructure Monitor - Cluster network connectivity problems EventID 1129 on {{event.host.name}}": "OK", "Disposition pipeline throughput [150]  - throughput condition": "Alert", "[P3] Disposition Pipeline Throughput [150]": "OK", "[Synthetics] Socialite-Edfman": "OK", "Export Topology Down Alert": "Alert", "Cloud Capture Mongo CPU Utilization - D3": "OK", "Cloud Capture Query throuput": "OK", "Cloud Capture Node State - C1": "OK", "Cloud Capture Mongo Server Exceptions thrown - Config Cluster": "OK", "Cloud Capture Mongo Server Exceptions thrown - Data Cluster": "OK", "Cloud Capture GetMore throuput": "OK", "Cloud Capture Mongo CPU Utilization - D2": "OK", "Cloud Capture Mongo CPU Utilization - D1": "OK", "Cloud Capture Mongo CPU Utilization - C3": "OK", "Cloud Capture Write throughput - Deletes": "OK", "Cloud Capture Node State - C3": "OK", "Cloud Capture Mongo CPU Utilization - C2": "OK", "Cloud Capture Write throughput - Updates": "OK", "Cloud Capture Mongo Replication Lag - Data Cluster": "OK", "Cloud Capture Mongo CPU Utilization - C1": "OK", "Cloud Capture Write throughput - Inserts": "OK", "Cloud Capture Mongo Replication Lag - Config Cluster": "OK", "Cloud Capture Node State - C2": "OK", "Aws-Dev3 Hosts Health Check": "OK", "DEV3 Hosts are down for Monitoring": "OK", "[Synthetics] [EA] AWS - MT CAN - UI login, search, log out": "OK", "DBRE Database Monitor - Managed by TF test {{host.name}}": "OK", "Data Fetcher Document Count": "No Data", "Data Publisher Document Count": "No Data", "Miss Match Of Data Count In Hazelcast Producer and Fetcher Threads.": "No Data", "[Synthetics] [EA] AWS MT - Schwab - prod - login, seach, logout": "OK", "Lead time for EHMS deployment on JPMC nam secondary": "No Data", "Lead time for EHMS deployment on AWS-PROD": "No Data", "Lead time for EHMS Build and dev-deploy on AWS-PROD env": "OK", "Lead time for EHMS deployment on BNY nam sandbox": "No Data", "Lead time for EHMS deployment on BNY emea primary": "No Data", "Lead time for EHMS deployment on AWS BNY NAM": "No Data", "Lead time for EHMS deployment on Schwab UAT": "No Data", "Lead time for EHMS deployment on JPMC emea secondary": "No Data", "Lead time for EHMS deployment on JPMC emea primary": "No Data", "Lead time for EHMS deployment on JPMC emea sandbox": "No Data", "Lead time for EHMS deployment on BNY emea secondary": "No Data", "Lead time for EHMS deployment on BNY nam secondary": "No Data", "Lead time for EHMS deployment on Azure PWEZ": "No Data", "Lead time for EHMS deployment on JPMC apac primary": "No Data", "Lead time for EHMS deployment on JPMC nam primary": "No Data", "Lead time for EHMS deployment on JPMC nam sandbox": "No Data", "Lead time for EHMS deployment on BNY emea sandbox": "No Data", "Lead time for EHMS deployment on Schwab": "No Data", "Lead time for EHMS deployment on JPMC apac sandbox": "No Data", "Lead time for EHMS deployment on JPMC apac secondary": "No Data", "Lead time for EHMS deployment on BNY nam primary": "No Data", "Lead time for ISS deployment on JPMC emea secondary": "OK", "Lead time for ISS deployment on JPMC apac primary": "OK", "Lead time for ISS deployment on JPMC nam primary": "OK", "Lead time for ISS deployment on JPMC emea primary": "OK", "Lead time for ISS deployment on JPMC apac secondary": "OK", "Lead time for ISS deployment on JPMC nam sandbox": "OK", "Lead time for ISS deployment on JPMC nam secondary": "OK", "Preservation policy cache not refreshed - processing condition": "OK", "[P1] Preservation policy cache not refreshed": "OK", "Cloud Capture RabbitMQ Alarm - Memory": "OK", "Kafka made an unclean leader election in jpmc-nam-sandbox-kafka": "No Data", "Kafka Cluster Capacity status: jpmc-apac-primary-kafka": "No Data", "Kafka Cluster Capacity status: jpmc-emea-primary-kafka": "No Data", "Kafka Cluster Capacity status: jpmc-apac-secondary-kafka": "No Data", "Kafka Cluster Capacity status: jpmc-apac-sandbox-kafka": "No Data", "Kafka Cluster Capacity status: jpmc-emea-secondary-kafka": "No Data", "Kafka Cluster Capacity status: jpmc-emea-sandbox-kafka": "No Data", "Kafka Cluster Capacity status: jpmc-nam-sandbox-kafka": "No Data", "Kafka made an unclean leader election in jpmc-apac-secondary-kafka": "No Data", "Kafka made an unclean leader election in jpmc-apac-sandbox-kafka": "No Data", "Kafka made an unclean leader election in jpmc-apac-primary-kafka": "No Data", "Kafka made an unclean leader election in jpmc-emea-secondary-kafka": "No Data", "Kafka made an unclean leader election in jpmc-emea-primary-kafka": "No Data", "Kafka made an unclean leader election in jpmc-emea-sandbox-kafka": "No Data", "Kafka active controller count for jpmc-emea-primary-kafka": "No Data", "Kafka active controller count for jpmc-apac-sandbox-kafka": "No Data", "Kafka active controller count for jpmc-apac-secondary-kafka": "No Data", "Kafka active controller count for jpmc-emea-secondary-kafka": "No Data", "Kafka active controller count for jpmc-apac-primary-kafka": "No Data", "Kafka active controller count for jpmc-emea-sandbox-kafka": "No Data", "Kafka active controller count for jpmc-nam-sandbox-kafka": "No Data", "Kafka has under replicated partitions in jpmc-emea-sandbox-kafka": "No Data", "Kafka has under replicated partitions in jpmc-nam-sandbox-kafka": "No Data", "Kafka has under replicated partitions in jpmc-apac-secondary-kafka": "No Data", "Kafka has under replicated partitions in jpmc-apac-sandbox-kafka": "No Data", "Kafka has under replicated partitions in jpmc-apac-primary-kafka": "No Data", "Kafka has under replicated partitions in jpmc-emea-secondary-kafka": "No Data", "Kafka has under replicated partitions in jpmc-emea-primary-kafka": "No Data", "Kafka has an offline partition in jpmc-apac-secondary-kafka": "No Data", "Kafka has an offline partition in jpmc-apac-primary-kafka": "No Data", "Kafka has an offline partition in jpmc-emea-secondary-kafka": "No Data", "Kafka has an offline partition in jpmc-emea-primary-kafka": "No Data", "Kafka has an offline partition in jpmc-nam-sandbox-kafka": "No Data", "Kafka has an offline partition in jpmc-apac-sandbox-kafka": "No Data", "Kafka has an offline partition in jpmc-emea-sandbox-kafka": "No Data", "[Synthetics] [EA] AWS - MT CAN - UI login and log out [Stifel Awareness]": "OK", "CEPH HEALTH JPMC & BNY (Test Monitor)": "Warn", "export_all Drain Rate is lower than threshold from last 1 hour": "Alert", "archive_export Drain Rate is lower than threshold from last 1 hour": "Alert", "ediscovery_export Drain Rate is lower than threshold from last 1 hour": "Alert", "msg_and_nonemail_export Drain Rate is lower than threshold from last 1 hour": "Alert", "supervision_export Drain Rate is lower than threshold from last 1 hour": "Alert", "archive_export Lag is greater than threshold from last 1 hour": "OK", "ediscovery_export Lag is greater than threshold from last 1 hour": "OK", "supervision_export Lag is greater than threshold from last 1 hour": "OK", "export_all Lag is greater than threshold from last 1 hour": "OK", "msg_and_nonemail_export Lag is greater than threshold from last 1 hour": "OK", "\"export_all\" Pipeline Throughput SLO Alert": "OK", "\"supervision_export\" Pipeline Throughput SLO Alert": "OK", "\"msg_and_nonemail_export\" Pipeline Throughput SLO Alert": "OK", "\"archive_export\" Pipeline Throughput SLO Alert": "OK", "\"ediscovery_export\" Pipeline Throughput SLO Alert": "OK", "Ceph PG degraded in {{env.name}} (Test Monitor)": "Warn", "Ceph too few PGs in {{env.name}} (Test Monitor)": "OK", "Ceph too many pgs in {{env.name}} (Test Monitor)": "OK", "CEPH OSD DOWN in {env.name} (Test Monitor)": "OK", "[Synthetics] [P1] Disposition jobs app health check": "OK", "Cloud Capture Node State - D3": "OK", "Cloud Capture Node State - D2": "OK", "Cloud Capture Node State - D1": "OK", "JPMC_APAC-SANDBOX_EGW_RabbitMQ : INCOMING Queue depth Building up Need attention": "OK", "BNY NFS Disk Usage is high": "OK", "Service ea-smtp-storage has a high error rate on env:aws-mt-prod": "OK", "Service ea-smtp-storage has an abnormal change in throughput on env:aws-mt-prod": "OK", "Test Alert SVP-1": "Alert", "Test Alert SVP-2": "OK", "high queue lenght": "OK", "Name for monitor foo svp test egw": "OK", "[Synthetics] egw-perf : Synthetic_health_Check_for_Email_Gateway": "OK", "AWS-BNY-PRODNAM - RabbitMQ Heartbeat Failed on": "OK", "AWS-BNY-Canada - RabbitMQ Heartbeat Failed on": "OK", "EGW_Schwab_Prod - RabbitMQ Heartbeat Failed on": "OK", "JPMC_EMEA_Sandbox - RabbitMQ Heartbeat Failed on": "OK", "EGW_Perf - RabbitMQ Heartbeat Failed on": "OK", "EGW_MTPROD - RabbitMQ Heartbeat Failed on": "OK", "EGW-Schwab-UAT - RabbitMQ Heartbeat Failed on": "OK", "JPMC_APAC_Sandbox - RabbitMQ Heartbeat Failed on": "OK", "Need Attention: NR EVENT QUEUE depth not acking": "OK", "aws_bny_canada - EGW_Rabbitmq_Queue_depth_monitoring": "OK", "jpmc_emea_sandbox - EGW_Rabbitmq_Queue_depth_monitoring": "OK", "schwab_uat - EGW_Rabbitmq_Queue_depth_monitoring": "No Data", "jpmc_apac_sandbox - EGW_Rabbitmq_Queue_depth_monitoring": "OK", "aws_bny_emea - EGW-RabbitMQ_INCOMING_Queue_building_up_monitoring": "OK", "egw_perf - EGW-RabbitMQ_INCOMING_Queue_building_up_monitoring": "OK", "mt_prod - EGW-RabbitMQ_INCOMING_Queue_building_up_monitoring": "OK", "schwab_prod - EGW-RabbitMQ_INCOMING_Queue_building_up_monitoring": "OK", "jpmc_apac_sandbox - EGW-RabbitMQ_INCOMING_Queue_building_up_monitoring": "OK", "schwab_uat - EGW-RabbitMQ_INCOMING_Queue_building_up_monitoring": "No Data", "aws_bny_prod - EGW-RabbitMQ_INCOMING_Queue_building_up_monitoring": "OK", "jpmc_emea_sandbox - EGW-RabbitMQ_INCOMING_Queue_building_up_monitoring": "OK", "aws_bny_canada - EGW-RabbitMQ_INCOMING_Queue_building_up_monitoring": "OK", "jpmc_apac_sandbox - EGW-RabbitMQ_AAC_RECON_QUEUE_building_up_monitoring": "No Data", "schwab_uat - EGW-RabbitMQ_AAC_RECON_QUEUE_building_up_monitoring": "No Data", "egw_perf - EGW-RabbitMQ_AAC_RECON_QUEUE_building_up_monitoring": "OK", "aws_bny_canada - EGW-RabbitMQ_AAC_RECON_QUEUE_building_up_monitoring": "OK", "mt_prod - EGW-RabbitMQ_AAC_RECON_QUEUE_building_up_monitoring": "OK", "jpmc_emea_sandbox - EGW-RabbitMQ_AAC_RECON_QUEUE_building_up_monitoring": "No Data", "aws_bny_prod - EGW-RabbitMQ_AAC_RECON_QUEUE_building_up_monitoring": "OK", "aws_bny_emea - EGW-RabbitMQ_AAC_RECON_QUEUE_building_up_monitoring": "OK", "Service ea-idaas has a high error rate on env:jpmc-emea-secondary": "OK", "Template - Gorouter Latency is too high": "OK", "Service ea-idaas has a high error rate on env:aws-bny-nam-prod": "OK", "Service streaming-doc-extractor has a high error rate on env:aws-mt-prod": "OK", "Service streaming-doc-extractor has a high error rate on env:jpmc-emea-secondary": "OK", "Service streaming-doc-extractor has a high error rate on env:aws-bny-nam-prod": "OK", "Supervision SLO monitor": "OK", "Service ea-idaas has a high error rate on env:aws-mt-prod": "OK", "[Synthetics] [EA] AWS - BNY EMEA - Prod - login, search, logout": "No Data", "[P2] Proof of disposition Report Failure": "No Data", "Critical Alert Need Attention : No data received in Harak since last 30mins": "OK", "[Synthetics] Schwab-Prod : Synthetic EGW TPS API Testing from git hub actions-test2": "OK", "[Synthetics] MT-Prod : Synthetic EGW TPS API Testing from git hub actions-test2": "OK", "[Synthetics] AWS-Demo : Synthetic EGW TPS API Testing from git hub actions-test2": "OK", "[Synthetics] login to EA": "OK", "Service ea-idaas has a high p90 latency on env:aws-mt-prod": "OK", "Service ea-idaas has a high p90 latency on env:aws-bny-nam-prod": "OK", "Service ea-idaas has a high p90 latency on env:jpmc-emea-secondary": "OK", "Service streaming-doc-extractor has a high p90 latency on env:aws-mt-prod": "Alert", "Service streaming-doc-extractor has a high p90 latency on env:aws-bny-nam-prod": "OK", "Service streaming-doc-extractor has a high p90 latency on env:jpmc-emea-secondary": "OK", "[Synthetics] CC-PREPROD-NOTS synthetic-health-check": "OK", "Disposition jobs not publishing - jpuatnam": "OK", "Email Pipeline Indexing Bolt Execution Time is high on JPMC EMEA SEC": "Alert", "[Synthetics] CC-PREPROD-TSFA synthetic-health-check": "OK", "[Synthetics] CC-PREPROD-EVAGGR synthetic-health-check-CC": "OK", "[Synthetics] CC-PREPROD-EVLSNR synthetic-health-check": "OK", "[Synthetics] CC-PREPROD-EXPE synthetic-health-check": "OK", "[Synthetics] CC-PREPROD-CNTU synthetic-health-check-CC": "OK", "[Synthetics] CC-PREPROD-PRVA synthetic-health-check": "OK", "[Synthetics] CC-PREPROD-SHDA synthetic-health-check": "OK", "[Synthetics] CC-PREPROD-RTRA synthetic-health-check": "OK", "[Synthetics] CC-PREPROD-CADA synthetic-health-check": "OK", "AWS PROD - Remaining Memory is Low for BOSH Diego Cells": "OK", "AWS PROD - Remaining Disk is Low for BOSH Diego Cells": "OK", "AWS Opsman Availability Check": "OK", "AWS PROD - Gorouter Latency is too high": "OK", "Export Daemon App Local Queue Size > 0": "OK", "AWS Prod PCF VM's Persistent Disk Usage": "Warn", "Export Daemon Process Rate < 1 Bytes/Sec": "Alert", "Export Daemon Throughput SLO Alert": "OK", "AWS NAM (us-east-1) PCF VM's Disk Usage": "OK", "AWS NAM (us-west-2) PCF VM's Persistent Disk Usage": "OK", "AWS EU-CENTRAL-1 PCF VM's Persistent Disk Usage": "OK", "AWS AP-SOUTHEAST-1 PCF VM's Persistent Disk Usage": "OK", "AWS CA-CENTRAL-1 PCF VM's Persistent Disk Usage": "OK", "JPMC-NAM-PRI PCF VM's Persistent Disk Usage": "OK", "JPMC-NAM-SEC PCF VM's Persistent Disk Usage": "OK", "JPMC-NAM-SANDBOX PCF VM's Persistent Disk Usage": "OK", "JPMC-EMEA-SEC PCF VM's Persistent Disk Usage": "OK", "JPMC-EMEA-PRI PCF VM's Persistent Disk Usage": "OK", "JPMC-EMEA-SANDBOX PCF VM's Persistent Disk Usage": "OK", "JPMC-APAC-SANDBOX PCF VM's Persistent Disk Usage": "OK", "JPMC-APAC-PRI PCF VM's Persistent Disk Usage": "OK", "JPMC-APAC-SEC PCF VM's Persistent Disk Usage": "OK", "BNY-NAM-PRI PCF VM's Persistent Disk Usage": "OK", "BNY-NAM-SEC PCF VM's Persistent Disk Usage": "OK", "BNY-EMEA-SEC PCF VM's Persistent Disk Usage": "OK", "BNY-EMEA-PRI PCF VM's Persistent Disk Usage": "OK", "Nonemail ingestion lag": "OK", "Nonemail ingestion drain rate": "Alert", "Nonemail ingestion drain rate impacted": "OK", "Test-negative Disposition jobs not publishing - jpuatapac": "Alert", "Cloud Capture Mongo Oplog Window - Config Cluster": "OK", "Cloud Capture Mongo Oplog Window - Data Cluster": "OK", "Error Budget Alert on SLO: SLO for ISS error rate on env:jpmc-emea-secondary": "OK", "Error Budget Alert on SLO: SLO for ISS error rate on env: AWS-MT Prod": "OK", "Error Budget Alert on SLO: SLO for ISS error rate on env:aws-bny-nam-prod": "OK", "Error Budget Alert on SLO: ISS latency on env:aws-mt-prod": "OK", "Error Budget Alert on SLO: SLO for ISS latency on env:jpmc-emea-secondary": "OK", "Error Budget Alert on SLO: SLO for ISS latency on env:aws-bny-nam-prod": "OK", "Total Queue Execution Time": "OK", "Email ingestion pipeline Throughput (Schwab UAT) - lag condition": "OK", "Email ingestion pipeline Throughput (Schwab UAT)  - drain rate condition": "Alert", "[P3] Email ingestion pipeline Throughput [100]": "OK", "Elasticsearch Cluster Capacity status: schwab-uat-essdata-cluster": "Alert", "Elasticsearch Cluster Capacity status: schwab-uat-essreport-cluster": "OK", "Elasticsearch Cluster Health status: schwab-uat-essdata-cluster": "OK", "Elasticsearch Cluster Health status: schwab-uat-essreport-cluster": "OK", "Elasticsearch Cluster Health status: schwab-uat-essdata-cluster - showing nodes KGS": "OK", "Disposition Fabrics errors": "No Data", "Error Budget Alert on SLO: SLO for EHMS error rate on env:aws-bny-nam-prod": "OK", "Error Budget Alert on SLO: SLO for EHMS error rate on env:jpmc-emea-secondary": "OK", "Error Budget Alert on SLO: SLO for EHMS error rate on env:aws-mt-prod": "OK", "Error Budget Alert on SLO: SLO for EHMS latency on env:aws-mt-prod": "OK", "Error Budget Alert on SLO: SLO for EHMS latency on env:jpmc-emea-secondary": "OK", "Error Budget Alert on SLO: SLO for EHMS latency on env:aws-bny-nam-prod": "Alert", "JPMC_EMEA_SEC - RabbitMQ Heartbeat Failed on": "OK", "[Synthetics] [EA] AWS Demo| UI Portal - BrowserTest": "OK", "Service ISS has a high error rate on env:aws-jpmc-nam-prod": "OK", "[Synthetics] [EA] JPMC AWS PROD | UI Portal - BrowserTest": "OK", "[Synthetics] [EA] Schwab UAT| UI Portal - BrowserTest": "OK", "[Synthetics] [EA] BNY AWS NAM| UI Portal - BrowserTest": "OK", "[Synthetics] aws_ea_jpmc_prodnam : Test on export-daemon-app": "OK", "[Synthetics] aws_ea_jpmc_prodnam : Test on pstexport-daemon-app": "OK", "[Synthetics] [EA] Schwab Prod| UI Portal - BrowserTest": "OK", "Service streaming-doc-extractor has a high p90 latency on env:aws-schwab-prod": "OK", "Service streaming-doc-extractor has an abnormal change in throughput on env:aws-schwab-uat": "Alert", "Service streaming-doc-extractor has a high p90 latency on env:jpmc-apac-sandox": "OK", "Service ea-idaas has a high p90 latency on env:aws-schwab-uat": "OK", "[Synthetics] aws-dev2 ca-archive-api health check": "OK", "Service ea-idaas has a high p90 latency on env:aws-schwab-prod": "OK", "Service ea-idaas has a high p90 latency on env:bny-nam-prod": "OK", "Service ea-idaas has a high p90 latency on env:bny-emea-prod": "OK", "Service ea-idaas has a high p90 latency on env:jpmc-apac-sandbox": "OK", "Service ea-idaas has a high error rate on env:aws-schwab-uat": "OK", "[Synthetics] [EA] Schwab UAT | UI Portal - HealthCheck": "OK", "[Synthetics] [EA] Schwab Prod | UI Portal - HealthCheck": "OK", "[Synthetics] [EA] JPMC PROD - AWS | UI Portal - HealthCheck": "OK", "[Synthetics] [EA] BNY NAM PROD - AWS | UI Portal - HealthCheck": "OK", "[Synthetics] [EA] BNY AP-SOUTHEAST PROD - AWS | UI Portal - HealthCheck": "OK", "[Synthetics] [EA] JPMC EMEA PROD - Softlayer | UI Portal - HealthCheck": "OK", "[Synthetics] [EA] MT CA-CENTRAL PROD - AWS | UI Portal - HealthCheck": "OK", "[Synthetics] [EA] JPMC APAC UAT - Softlayer | UI Portal - HealthCheck": "OK", "[Synthetics] [EA] JPMC EMEA UAT - Softlayer | UI Portal - HealthCheck": "OK", "[Synthetics] [EA] BNY EU-CENTRAL PROD - AWS | UI Portal - HealthCheck": "OK", "Ceph OSD Full in JPMC NAM Secondary (Active Site)": "OK", "Ceph OSD NEAR Full in JPMC NAM Secondary (Active Site)": "OK", "[Synthetics] [EA] MT CA-CENTRAL PROD | UI Portal - BrowserTest": "OK", "Service ea-idaas has a high error rate on env:aws-schwab-prod": "OK", "Service ea-idaas has a high error rate on env:jpmc-apac-sandbox": "OK", "Service ea-idaas has a high error rate on env:aws-demo": "OK", "Service ea-idaas has a high error rate on env:jpmc-emea-sandbox": "OK", "Service ea-idaas has a high p90 latency on env:jpmc-emea-sandbox": "OK", "Need Attention: No data received in Harak since last 60 mins": "Alert", "Need Attention in aws_bny_canada: No data received in Harak since last 60 mins": "OK", "Need Attention in jpmc_emea_sec: No data received in Harak since last 60 mins": "OK", "Need Attention in jpmc_aws_prodnam: No data received in Harak since last 60 mins": "Alert", "Need Attention in aws_bny_emea: No data received in Harak since last 60 mins": "OK", "Need Attention in schwab_prod: No data received in Harak since last 60 mins": "OK", "Need Attention in schwab_uat: No data received in Harak since last 60 mins": "No Data", "jpmc_aws_prodnam - EGW-RabbitMQ_INCOMING_Queue_building_up_monitoring": "OK", "jpmc_emea_sec - EGW-RabbitMQ_INCOMING_Queue_building_up_monitoring": "OK", "[Synthetics] Supervision-jobs-app health check  - dev2": "OK", "EGW Retryable Queue is violated threshold limit and needs attention test1": "OK", "schwab_prod: EGW Retryable Queue is violated threshold limit and needs attention": "OK", "schwab_uat: EGW Retryable Queue is violated threshold limit and needs attention": "No Data", "aws_bny_emea: EGW Retryable Queue is violated threshold limit and needs attention": "OK", "jpmc_aws_prodnam: EGW Retryable Queue is violated threshold limit and needs attention": "OK", "aws_bny_canada: EGW Retryable Queue is violated threshold limit and needs attention": "OK", "jpmc_emea_sec: EGW Retryable Queue is violated threshold limit and needs attention": "OK", "[Synthetics] [EA] BNY EMEA UAT - Softlayer | UI Portal - HealthCheck": "Alert", "[Synthetics] [EA] BNY EMEA PROD - Softlayer | UI Portal - HealthCheck": "OK", "Service streaming-doc-extractor has a high p90 latency on env:jpmc-emea-uat": "OK", "Service streaming-doc-extractor has a high error rate on env:aws-schwab-prod": "OK", "Service streaming-doc-extractor has a high error rate on env:aws-schwab-uat": "OK", "[Synthetics] [EA] AWS MT Demo | UI Portal - HealthCheck": "OK", "[Synthetics] [EA] AWS MT PROD | UI Portal - HealthCheck": "OK", "Service streaming-doc-extractor has a high error rate on env:jpmc-apac-sandox": "OK", "[Synthetics] Test on keyservice.smarsh.com/keystoreservice/healthcheck": "OK", "Export stuck in daemon app (Packaging)": "No Data", "NFS Disk usage is high in JPMC Env {{env}} - Host {{host}}": "OK", "Test - Lag change percentage > 40%": "OK", "Test - Email ingestion pipeline Throughput - lag condition": "OK", "Test - Email Ingestion throughput needs to improve": "OK", "[Synthetics] Health Check of mlms-demo": "OK", "Schwab UAT | Email ingestion Pipeline | Lag for last 24h": "Alert", "P2 | Email ingestion Pipeline Lag is high | Schwab UAT": "OK", "TEST fabric errors git resource update": "OK", "AWS BNY NAM | Email ingestion Pipeline | Lag for last 1 hr": "OK", "AWS BNY NAM | Email ingestion Pipeline | Lag for last 24h": "Alert", "[Synthetics] Mlms-schwab-prod-Healthcheck": "OK", "concourse-example: Query Anomaly Detection": "OK", "concourse-example: Indexing Anomaly Detection": "OK", "Elasticsearch Cluster Health status - concourse-example": "OK", "Error Budget Alert on SLO: concourse-example: Cluster Health": "OK", "[Synthetics] Supervision-jobs-app health check  - mt-prod": "OK", "[Synthetics] Supervision-jobs-app health check  -schwab-prod": "OK", "schwab_prod - EGW_Rabbitmq_Queue_depth_monitoring": "OK", "JPMC_AWS_PRODNAM - RabbitMQ Heartbeat Failed on": "No Data", "jpmc_aws_prodnam - EGW_Rabbitmq_Queue_depth_monitoring": "OK", "jpmc_emea_sec - EGW_Rabbitmq_Queue_depth_monitoring": "OK", "aws_bny_emea - EGW_Rabbitmq_Queue_depth_monitoring": "OK", "Policy mismatch identification tool is not executed": "OK", "Anomaly detected in Lag for Schwab UAT": "OK", "DD Support 476128": "Alert", "Elasticsearch Cluster Health status - jpmc-apac-pri-supervision": "OK", "jpmc-apac-pri-supervision: Indexing Anomaly Detection": "OK", "jpmc-apac-pri-supervision: Query Anomaly Detection": "OK", "Elasticsearch Cluster Health status - jpmc-apac-sec-supervision": "OK", "jpmc-apac-sec-supervision: Query Anomaly Detection": "OK", "jpmc-apac-sec-supervision: Indexing Anomaly Detection": "OK", "[Synthetics] Mlms-schwab-uat Health Check": "OK", "jpmc-apac-sandbox-supervision: Query Anomaly Detection": "OK", "Elasticsearch Cluster Health status - jpmc-apac-sandbox-supervision": "OK", "jpmc-apac-sandbox-supervision: Indexing Anomaly Detection": "OK", "Elasticsearch Cluster Health status - jpmc-emea-pri-supervision": "OK", "jpmc-emea-pri-supervision: Indexing Anomaly Detection": "OK", "jpmc-emea-pri-supervision: Query Anomaly Detection": "OK", "Kafka has an offline partition in concourse-kafka-example": "OK", "Kafka active controller count for concourse-kafka-example": "OK", "Kafka made an unclean leader election in concourse-kafka-example": "OK", "Kafka has under replicated partitions in concourse-kafka-example": "OK", "Kafka Cluster Capacity status: concourse-kafka-example": "OK", "Elasticsearch Cluster Health status - jpmc-emea-sec-supervision": "OK", "jpmc-emea-sec-supervision: Indexing Anomaly Detection": "OK", "jpmc-emea-sec-supervision: Query Anomaly Detection": "OK", "jpmc-emea-sandbox-supervision: Indexing Anomaly Detection": "OK", "jpmc-emea-sandbox-supervision: Query Anomaly Detection": "OK", "Elasticsearch Cluster Health status - jpmc-emea-sandbox-supervision": "OK", "Non Email Pipeline Indexing Bolt Execution Time is high on JPMC EMEA SEC": "OK", "LFS Pipeline Indexing Bolt Execution Time is high on JPMC EMEA SEC": "OK", "Elasticsearch Cluster Health status - bny-emea-sandbox-data": "OK", "bny-emea-sandbox-data: Indexing Anomaly Detection": "OK", "bny-emea-sandbox-data: Query Anomaly Detection": "OK", "Elasticsearch Cluster Health status - bny-emea-sandbox-report": "OK", "bny-emea-sandbox-report: Indexing Anomaly Detection": "OK", "bny-emea-sandbox-report: Query Anomaly Detection": "OK", "Elasticsearch Cluster Health status - bny-emea-sandbox-supervision": "OK", "bny-emea-sandbox-supervision: Query Anomaly Detection": "OK", "bny-emea-sandbox-supervision: Indexing Anomaly Detection": "OK", "Email Pipeline Indexing Bolt Error Rate is high on JPMC EMEA SEC": "OK", "Lfs Pipeline Indexing Bolt Error Rate is high on JPMC EMEA SEC": "OK", "[Synthetics] egw-recon-jpmc-aws-prodnam : Synthetic_health_Check_test_for_EGW_Dev_RECON": "OK", "[Synthetics] Supervision-jobs-app health check  -jpmc-emea-sec": "OK", "us-west-2 Remaining Disk Diego Cells": "OK", "us-west-2 Remaining Memory Diego Cell": "OK", "us-west-2 Gorouter Latency": "OK", "us-west-2 PCF Loggregator Firehose Logs being dropped": "OK", "us-west-2 Opsman Availability Check": "OK", "us-west-2 PCF Director Availability Check": "OK", "[Synthetics] Supervision-jobs-app health check  -BNY-emea-uat-sl": "OK", "Schwab PROD| Email ingestion Pipeline | Lag for last 1hr": "OK", "AWS MT PROD| Email ingestion Pipeline | Lag for last 1hr": "OK", "[Synthetics] Supervision-jobs-app health check  -bny-prod-emea-sl": "Alert", "[Synthetics] Supervision-jobs-app health check  -BNY-nam-uat-sl": "OK", "[Synthetics] Supervision-jobs-app health check  -BNY-nam-prod-sl": "OK", "ap-southeast-1 Remaining Disk Diego Cells": "OK", "ap-southeast-1 Remaining Memory Diego Cell": "OK", "ap-southeast-1 Gorouter Latency": "OK", "ap-southeast-1 PCF Loggregator Firehose Logs being dropped": "OK", "ap-southeast-1 Opsman Availability Check": "OK", "ap-southeast-1 PCF Director Availability Check": "OK", "ca-central-1 Remaining Disk Diego Cells": "OK", "ca-central-1 Remaining Memory Diego Cell": "OK", "ca-central-1 Gorouter Latency": "OK", "ca-central-1 PCF Loggregator Firehose Logs being dropped": "OK", "ca-central-1 Opsman Availability Check": "OK", "ca-central-1 PCF Director Availability Check": "OK", "eu-central-1 Remaining Disk Diego Cells": "OK", "eu-central-1 Remaining Memory Diego Cell": "OK", "eu-central-1 Gorouter Latency": "OK", "eu-central-1 PCF Loggregator Firehose Logs being dropped": "OK", "eu-central-1 Opsman Availability Check": "OK", "eu-central-1 PCF Director Availability Check": "OK", "us-east-1 Remaining Disk Diego Cells": "OK", "us-east-1 Remaining Memory Diego Cell": "OK", "us-east-1 Gorouter Latency": "OK", "us-east-1 PCF Loggregator Firehose Logs being dropped": "OK", "us-east-1 Opsman Availability Check": "OK", "us-east-1 PCF Director Availability Check": "OK", "bny-apac-sandbox-supervision: Indexing Anomaly Detection": "OK", "Elasticsearch Cluster Health status - bny-apac-sandbox-supervision": "OK", "bny-apac-sandbox-supervision: Query Anomaly Detection": "OK", "Elasticsearch Cluster Health status - bny-apac-sandbox-data": "OK", "bny-apac-sandbox-data: Indexing Anomaly Detection": "OK", "bny-apac-sandbox-data: Query Anomaly Detection": "OK", "bny-apac-sandbox-report: Indexing Anomaly Detection": "OK", "Elasticsearch Cluster Health status - bny-apac-sandbox-report": "OK", "bny-apac-sandbox-report: Query Anomaly Detection": "OK", "Kafka Cluster Capacity status: bny-apac-sandbox-kafka": "Alert", "Service ea-smtp-transformer has a high error rate on env:aws-mt-prod": "OK", "Service ea-smtp-transformer has an abnormal change in throughput on env:aws-mt-prod": "OK", "us-west-2 CPU Load Average 5m": "OK", "us-west-2 CPU Load Average 5m Diego Cells": "OK", "us-west-2 MYSQL CF Availability Check": "OK", "us-west-2 Remaining Disk Pas Windows": "OK", "us-west-2 Remaining Memory Windows Diego Cells": "OK", "us-east-1 CPU Load Average 5m": "OK", "us-east-1 CPU Load Average 5m Diego Cells": "OK", "us-east-1 MYSQL CF Availability Check": "OK", "us-east-1 Remaining Disk Pas Windows": "OK", "us-east-1 Remaining Memory Windows Diego Cells": "OK", "ca-central-1 CPU Load Average 5m": "OK", "ca-central-1 CPU Load Average 5m Diego Cells": "OK", "ca-central-1 MYSQL CF Availability Check": "OK", "ca-central-1 Remaining Disk Pas Windows": "OK", "ca-central-1 Remaining Memory Windows Diego Cells": "OK", "ap-southeast-1 CPU Load Average 5m": "OK", "ap-southeast-1 CPU Load Average 5m Diego Cells": "OK", "ap-southeast-1 MYSQL CF Availability Check": "OK", "ap-southeast-1 Remaining Disk Pas Windows": "OK", "ap-southeast-1 Remaining Memory Windows Diego Cells": "OK", "eu-central-1 CPU Load Average 5m": "OK", "eu-central-1 CPU Load Average 5m Diego Cells": "OK", "eu-central-1 MYSQL CF Availability Check": "OK", "eu-central-1 Remaining Disk Pas Windows": "OK", "eu-central-1 Remaining Memory Windows Diego Cells": "OK", "jpmc-nam-sandbox CPU Load Average 5m": "OK", "jpmc-nam-sandbox CPU Load Average 5m Diego Cells": "OK", "jpmc-nam-sandbox Remaining Disk Diego Cells": "OK", "jpmc-nam-sandbox Remaining Memory Diego Cell": "OK", "jpmc-nam-sandbox Gorouter Latency": "OK", "jpmc-nam-sandbox PCF Loggregator Firehose Logs being dropped": "OK", "jpmc-nam-sandbox MYSQL CF Availability Check": "Alert", "jpmc-nam-sandbox PCF Director Availability Check": "OK", "jpmc-apac-sandbox CPU Load Average 5m": "OK", "jpmc-apac-sandbox CPU Load Average 5m Diego Cells": "OK", "jpmc-apac-sandbox Remaining Disk Diego Cells": "OK", "jpmc-apac-sandbox Remaining Memory Diego Cell": "OK", "jpmc-apac-sandbox Gorouter Latency": "OK", "jpmc-apac-sandbox PCF Loggregator Firehose Logs being dropped": "OK", "jpmc-apac-sandbox MYSQL CF Availability Check": "OK", "jpmc-apac-sandbox PCF Director Availability Check": "OK", "jpmc-emea-sandbox CPU Load Average 5m": "OK", "jpmc-emea-sandbox CPU Load Average 5m Diego Cells": "OK", "jpmc-emea-sandbox Remaining Disk Diego Cells": "OK", "jpmc-emea-sandbox Remaining Memory Diego Cell": "OK", "jpmc-emea-sandbox Gorouter Latency": "OK", "jpmc-emea-sandbox PCF Loggregator Firehose Logs being dropped": "OK", "jpmc-emea-sandbox MYSQL CF Availability Check": "OK", "jpmc-emea-sandbox PCF Director Availability Check": "OK", "jpmc-nam-pri CPU Load Average 5m": "OK", "jpmc-nam-pri CPU Load Average 5m Diego Cells": "OK", "jpmc-nam-pri Remaining Disk Diego Cells": "OK", "jpmc-nam-pri Remaining Memory Diego Cell": "OK", "jpmc-nam-pri Gorouter Latency": "OK", "jpmc-nam-pri PCF Loggregator Firehose Logs being dropped": "OK", "jpmc-nam-pri MYSQL CF Availability Check": "OK", "jpmc-nam-pri PCF Director Availability Check": "OK", "jpmc-nam-sec CPU Load Average 5m": "OK", "jpmc-nam-sec CPU Load Average 5m Diego Cells": "OK", "jpmc-nam-sec Remaining Disk Diego Cells": "OK", "jpmc-nam-sec Remaining Memory Diego Cell": "OK", "jpmc-nam-sec Gorouter Latency": "OK", "jpmc-nam-sec PCF Loggregator Firehose Logs being dropped": "OK", "jpmc-nam-sec MYSQL CF Availability Check": "OK", "jpmc-nam-sec PCF Director Availability Check": "OK", "bny-nam-dr CPU Load Average 5m": "Warn", "bny-nam-dr CPU Load Average 5m Diego Cells": "OK", "bny-nam-dr Remaining Disk Diego Cells": "OK", "bny-nam-dr Remaining Memory Diego Cell": "OK", "bny-nam-dr Gorouter Latency": "OK", "bny-nam-dr PCF Loggregator Firehose Logs being dropped": "OK", "bny-nam-dr MYSQL CF Availability Check": "OK", "bny-nam-dr PCF Director Availability Check": "OK", "jpmc-apac-sec CPU Load Average 5m": "OK", "jpmc-apac-sec CPU Load Average 5m Diego Cells": "OK", "jpmc-apac-sec Remaining Disk Diego Cells": "OK", "jpmc-apac-sec Remaining Memory Diego Cell": "OK", "jpmc-apac-sec Gorouter Latency": "OK", "jpmc-apac-sec PCF Loggregator Firehose Logs being dropped": "OK", "jpmc-apac-sec MYSQL CF Availability Check": "OK", "jpmc-apac-sec PCF Director Availability Check": "OK", "jpmc-emea-pri CPU Load Average 5m": "OK", "jpmc-emea-pri CPU Load Average 5m Diego Cells": "OK", "jpmc-emea-pri Remaining Disk Diego Cells": "OK", "jpmc-emea-pri Remaining Memory Diego Cell": "OK", "jpmc-emea-pri Gorouter Latency": "OK", "jpmc-emea-pri PCF Loggregator Firehose Logs being dropped": "OK", "jpmc-emea-pri MYSQL CF Availability Check": "OK", "jpmc-emea-pri PCF Director Availability Check": "OK", "bny-nam-pri CPU Load Average 5m": "OK", "bny-nam-pri CPU Load Average 5m Diego Cells": "OK", "bny-nam-pri Remaining Disk Diego Cells": "OK", "bny-nam-pri Remaining Memory Diego Cell": "OK", "bny-nam-pri Gorouter Latency": "OK", "bny-nam-pri PCF Loggregator Firehose Logs being dropped": "OK", "bny-nam-pri MYSQL CF Availability Check": "OK", "bny-nam-pri PCF Director Availability Check": "OK", "jpmc-emea-sec CPU Load Average 5m": "OK", "jpmc-emea-sec CPU Load Average 5m Diego Cells": "OK", "jpmc-emea-sec Remaining Disk Diego Cells": "OK", "jpmc-emea-sec Remaining Memory Diego Cell": "OK", "jpmc-emea-sec Gorouter Latency": "OK", "jpmc-emea-sec PCF Loggregator Firehose Logs being dropped": "OK", "jpmc-emea-sec MYSQL CF Availability Check": "OK", "jpmc-emea-sec PCF Director Availability Check": "OK", "Non Email Pipeline Indexing Bolt Error Rate is high on JPMC EMEA SEC": "OK", "Search Latency is high on JPMC EMEA SEC": "OK", "Indexing Latency is high on JPMC EMEA SEC": "OK", "Bulk Indexing Latency is high on JPMC EMEA SEC": "Alert", "Search Error Rate is high on JPMC EMEA SEC": "OK", "Index Error Rate is high on JPMC EMEA SEC": "OK", "Bulk Index Error Rate is high on JPMC EMEA SEC": "OK", "Email Reindex Pipeline Lag is high on JPMC EMEA SEC": "Alert", "Non Email Reindex Pipeline Lag is high on JPMC EMEA SEC": "OK", "Reprocess Reindex Pipeline Lag is high on JPMC EMEA SEC": "OK", "[Synthetics] [P1] ca-archive-api dig app health check": "OK", "Bulk Index Error Rate is high on AWS BNY NAM PROD": "OK", "Bulk Indexing Latency is high on AWS BNY NAM PROD": "OK", "Index Error Rate is high on AWS BNY NAM PROD": "OK", "Indexing Latency is high on AWS BNY NAM PROD": "OK", "Search Error Rate is high on AWS BNY NAM PROD": "OK", "Search Latency is high on AWS BNY NAM PROD": "OK", "Email Pipeline Indexing Bolt Error Rate is high on AWS BNY NAM PROD": "OK", "Email Pipeline Indexing Bolt Execution Time is high on AWS BNY NAM PROD": "OK", "Non Email Pipeline Indexing Bolt Execution Time is high on AWS BNY NAM PROD": "OK", "LFS Pipeline Indexing Bolt Execution Time is high on AWS BNY NAM PROD": "OK", "Lfs Pipeline Indexing Bolt Error Rate is high on AWS BNY NAM PROD": "OK", "Non Email Pipeline Indexing Bolt Error Rate is high on AWS BNY NAM PROD": "OK", "Workplace Callback URL online check": "Alert", "Facebook Callback URL online check": "Alert", "Twitter Callback URL online check": "Alert", "Search Latency is high on BNY APAC SOUTH EAST PROD": "OK", "Search Error Rate is high on BNY APAC SOUTH EAST PROD": "OK", "Indexing Latency is high on BNY APAC SOUTH EAST PROD": "OK", "Index Error Rate is high on BNY APAC SOUTH EAST PROD": "OK", "Bulk Indexing Latency is high on BNY APAC SOUTH EAST PROD": "OK", "Bulk Index Error Rate is high on BNY APAC SOUTH EAST PROD": "OK", "Non Email Pipeline Indexing Bolt Execution Time is high on BNY APAC SOUTH EAST PROD": "OK", "Non Email Pipeline Indexing Bolt Error Rate is high on BNY APAC SOUTH EAST PROD": "OK", "Lfs Pipeline Indexing Bolt Error Rate is high on BNY APAC SOUTH EAST PROD": "OK", "LFS Pipeline Indexing Bolt Execution Time is high on BNY APAC SOUTH EAST PROD": "OK", "Email Pipeline Indexing Bolt Execution Time is high on BNY APAC SOUTH EAST PROD": "OK", "Email Pipeline Indexing Bolt Error Rate is high on BNY APAC SOUTH EAST PROD": "OK", "Search Latency is high on BNY EMEA CENTRAL PROD": "OK", "Index Error Rate is high on BNY EMEA CENTRAL PROD": "OK", "Email Pipeline Indexing Bolt Execution Time is high on BNY EMEA CENTRAL PROD": "OK", "Non Email Pipeline Indexing Bolt Execution Time is high on BNY EMEA CENTRAL PROD": "OK", "Bulk Index Error Rate is high on BNY EMEA CENTRAL PROD": "OK", "LFS Pipeline Indexing Bolt Execution Time is high on BNY EMEA CENTRAL PROD": "OK", "Non Email Pipeline Indexing Bolt Error Rate is high on BNY EMEA CENTRAL PROD": "OK", "Bulk Indexing Latency is high on BNY EMEA CENTRAL PROD": "OK", "Lfs Pipeline Indexing Bolt Error Rate is high on BNY EMEA CENTRAL PROD": "OK", "Email Pipeline Indexing Bolt Error Rate is high on BNY EMEA CENTRAL PROD": "OK", "Search Error Rate is high on BNY EMEA CENTRAL PROD": "OK", "Indexing Latency is high on BNY EMEA CENTRAL PROD": "OK", "Search Latency is high on AWS DEMO": "OK", "Search Error Rate is high on AWS DEMO": "OK", "Indexing Latency is high on AWS DEMO": "OK", "Index Error Rate is high on AWS DEMO": "OK", "Bulk Indexing Latency is high on AWS DEMO": "No Data", "Bulk Index Error Rate is high on AWS DEMO": "OK", "Non Email Pipeline Indexing Bolt Execution Time is high on AWS DEMO": "OK", "Non Email Pipeline Indexing Bolt Error Rate is high on AWS DEMO": "OK", "Lfs Pipeline Indexing Bolt Error Rate is high on AWS DEMO": "OK", "LFS Pipeline Indexing Bolt Execution Time is high on AWS DEMO": "OK", "Email Pipeline Indexing Bolt Execution Time is high on AWS DEMO": "OK", "Email Pipeline Indexing Bolt Error Rate is high on AWS DEMO": "OK", "Testing Bosh System Health Check": "OK", "Email Pipeline Indexing Bolt Error Rate is high on AWS MT PROD": "Alert", "Email Pipeline Indexing Bolt Execution Time is high on AWS MT PROD": "Alert", "LFS Pipeline Indexing Bolt Execution Time is high on AWS MT PROD": "OK", "Lfs Pipeline Indexing Bolt Error Rate is high on AWS MT PROD": "OK", "Non Email Pipeline Indexing Bolt Error Rate is high on AWS MT PROD": "OK", "Non Email Pipeline Indexing Bolt Execution Time is high on AWS MT PROD": "Alert", "Bulk Index Error Rate is high on AWS MT PROD": "OK", "Bulk Indexing Latency is high on AWS MT PROD": "OK", "Index Error Rate is high on AWS MT PROD": "OK", "Indexing Latency is high on AWS MT PROD": "Alert", "Search Error Rate is high on AWS MT PROD": "OK", "Search Latency is high on AWS MT PROD": "OK", "Slow Reprocess Pipeline Indexing Bolt Execution Time is high on AWS MT PROD": "OK", "Slow Reprocess Pipeline Indexing Bolt Error Rate is high on AWS MT PROD": "OK", "Reprocess Pipeline Indexing Bolt Execution Time is high on AWS MT PROD": "Alert", "Reprocess Pipeline Indexing Bolt Error Rate is high on AWS MT PROD": "Alert", "Email Pipeline Indexing Bolt Error Rate is high on Schwab PROD": "OK", "Email Pipeline Indexing Bolt Execution Time is high on Schwab PROD": "OK", "LFS Pipeline Indexing Bolt Execution Time is high on Schwab PROD": "OK", "Lfs Pipeline Indexing Bolt Error Rate is high on Schwab PROD": "OK", "Non Email Pipeline Indexing Bolt Error Rate is high on Schwab PROD": "OK", "Non Email Pipeline Indexing Bolt Execution Time is high on Schwab PROD": "OK", "Slow Reprocess Pipeline Indexing Bolt Execution Time is high on Schwab PROD": "OK", "Slow Reprocess Pipeline Indexing Bolt Error Rate is high on Schwab PROD": "OK", "Bulk Index Error Rate is high on Schwab PROD": "OK", "Bulk Indexing Latency is high on Schwab PROD": "OK", "Index Error Rate is high on Schwab PROD": "OK", "Indexing Latency is high on Schwab PROD": "OK", "Search Error Rate is high on Schwab PROD": "OK", "Search Latency is high on Schwab PROD": "OK", "Email Pipeline Indexing Bolt Error Rate is high on Schwab UAT": "OK", "Email Pipeline Indexing Bolt Execution Time is high on Schwab UAT": "Warn", "LFS Pipeline Indexing Bolt Execution Time is high on Schwab UAT": "OK", "Lfs Pipeline Indexing Bolt Error Rate is high on Schwab UAT": "OK", "Non Email Pipeline Indexing Bolt Error Rate is high on Schwab UAT": "OK", "Non Email Pipeline Indexing Bolt Execution Time is high on Schwab UAT": "OK", "Bulk Index Error Rate is high on Schwab UAT": "OK", "Bulk Indexing Latency is high on Schwab UAT": "No Data", "Index Error Rate is high on Schwab UAT": "OK", "Indexing Latency is high on Schwab UAT": "OK", "Search Error Rate is high on Schwab UAT": "OK", "Search Latency is high on Schwab UAT": "OK", "Email Pipeline Indexing Bolt Error Rate is high on JPMC APAC Sandbox": "OK", "Email Pipeline Indexing Bolt Execution Time is high on JPMC APAC Sandbox": "Alert", "LFS Pipeline Indexing Bolt Execution Time is high on JPMC APAC Sandbox": "OK", "Lfs Pipeline Indexing Bolt Error Rate is high on JPMC APAC Sandbox": "OK", "Non Email Pipeline Indexing Bolt Error Rate is high on JPMC APAC Sandbox": "OK", "Non Email Pipeline Indexing Bolt Execution Time is high on JPMC APAC Sandbox": "OK", "Bulk Index Error Rate is high on JPMC APAC Sandbox": "OK", "Bulk Indexing Latency is high on JPMC APAC Sandbox": "OK", "Index Error Rate is high on JPMC APAC Sandbox": "OK", "Indexing Latency is high on JPMC APAC Sandbox": "OK", "Search Error Rate is high on JPMC APAC Sandbox": "OK", "Search Latency is high on JPMC APAC Sandbox": "Alert", "Email Pipeline Indexing Bolt Error Rate is high on JPMC EMEA Sandbox": "OK", "Email Pipeline Indexing Bolt Execution Time is high on JPMC EMEA Sandbox": "OK", "LFS Pipeline Indexing Bolt Execution Time is high on JPMC EMEA Sandbox": "OK", "Lfs Pipeline Indexing Bolt Error Rate is high on JPMC EMEA Sandbox": "OK", "Non Email Pipeline Indexing Bolt Error Rate is high on JPMC EMEA Sandbox": "OK", "Non Email Pipeline Indexing Bolt Execution Time is high on JPMC EMEA Sandbox": "OK", "Bulk Index Error Rate is high on JPMC EMEA Sandbox": "OK", "Bulk Indexing Latency is high on JPMC EMEA Sandbox": "OK", "Index Error Rate is high on JPMC EMEA  Sandbox": "OK", "Indexing Latency is high on JPMC EMEA Sandbox": "OK", "Search Error Rate is high on JPMC EMEA Sandbox": "Alert", "Search Latency is high on JPMC EMEA Sandbox": "Alert", "Email Pipeline Indexing Bolt Error Rate is high on MT CA CENTRAL PROD": "OK", "Email Pipeline Indexing Bolt Execution Time is high on MT CA CENTRAL PROD": "Alert", "LFS Pipeline Indexing Bolt Execution Time is high on MT CA CENTRAL PROD": "OK", "Lfs Pipeline Indexing Bolt Error Rate is high on MT CA CENTRAL PROD": "OK", "Non Email Pipeline Indexing Bolt Error Rate is high on MT CA CENTRAL PROD": "OK", "Non Email Pipeline Indexing Bolt Execution Time is high on MT CA CENTRAL PROD": "OK", "Bulk Index Error Rate is high on MT CA CENTRAL PROD": "OK", "Bulk Indexing Latency is high on MT CA CENTRAL PROD": "OK", "Index Error Rate is high on MT CA CENTRAL PROD": "OK", "Indexing Latency is high on MT CA CENTRAL PROD": "OK", "Search Error Rate is high on MT CA CENTRAL PROD": "OK", "Search Latency is high on MT CA CENTRAL PROD": "OK", "us-west-2 Bosh System Health Check": "Alert", "bny-nam-dr Bosh System Health Check": "Alert", "jpmc-nam-sandbox Bosh System Health Check": "Alert", "ap-southeast-1 RDS storage space": "OK", "ca-central-1 RDS storage space": "OK", "us-east-1 RDS storage space": "OK", "eu-central-1 RDS storage space": "OK", "jpmc-emea-sandbox Bosh System Health Check": "OK", "Kafka Cluster Capacity status: bny-emea-sandbox-kafka": "No Data", "jpmc-apac-sandbox Bosh System Health Check": "OK", "jpmc-nam-pri Bosh System Health Check": "OK", "jpmc-nam-sec Bosh System Health Check": "OK", "jpmc-emea-pri Bosh System Health Check": "OK", "Slow Reprocess Pipeline Indexing Bolt Error Rate is high on AWS BNY NAM PROD": "Alert", "Slow Reprocess Pipeline Indexing Bolt Execution Time is high on AWS BNY NAM PROD": "Alert", "jpmc-emea-sec Bosh System Health Check": "OK", "mt_prod - EGW_Rabbitmq_Queue_depth_monitoring": "OK", "aws_bny_prod - EGW_Rabbitmq_Queue_depth_monitoring": "OK", "Need Attention in jpmc_apac_sandbox: No data received in Harak since last 4 hrs": "OK", "Need Attention in jpmc_emea_sandbox: No data received in Harak since last 4 hrs": "OK", "Concourse Health Check": "OK", "[Synthetics] Disposition jobs app health check": "OK", "jpmc-apac-sec Bosh System Health Check": "OK", "bny-nam-pri Bosh System Health Check": "Alert", "bny-nam-dr Missing LRP's": "OK", "bny-nam-dr Auctioneer Latency": "OK", "bny-nam-dr BBS Latency": "OK", "bny-nam-dr Active Locks": "OK", "bny-nam-dr Rep Bulk Sync": "OK", "jpmc-nam-sandbox Missing LRP's": "OK", "jpmc-nam-sandbox Auctioneer Latency": "OK", "jpmc-nam-sandbox BBS Latency": "OK", "jpmc-emea-sandbox Missing LRP's": "OK", "jpmc-emea-sandbox Auctioneer Latency": "OK", "jpmc-emea-sandbox BBS Latency": "OK", "jpmc-nam-sandbox Active Locks": "OK", "jpmc-nam-sandbox Rep Bulk Sync": "OK", "jpmc-emea-sandbox Active Locks": "OK", "jpmc-emea-sandbox Rep Bulk Sync": "OK", "Vault is not Initialized in - concourse-vault-example-test": "OK", "Vault is Unsealed in - concourse-vault-example-test": "OK", "bny-emea-dr CPU Load Average 5m": "OK", "bny-emea-dr CPU Load Average 5m Diego Cells": "OK", "bny-emea-dr Remaining Disk Diego Cells": "OK", "bny-emea-dr Remaining Memory Diego Cell": "OK", "bny-emea-dr Gorouter Latency": "OK", "bny-emea-dr PCF Loggregator Firehose Logs being dropped": "OK", "bny-emea-dr MYSQL CF Availability Check": "OK", "bny-emea-dr PCF Director Availability Check": "OK", "jpmc-apac-pri Bosh System Health Check": "OK", "jpmc-apac-pri CPU Load Average 5m": "OK", "jpmc-apac-pri CPU Load Average 5m Diego Cells": "OK", "jpmc-apac-pri Remaining Disk Diego Cells": "OK", "jpmc-apac-pri Remaining Memory Diego Cell": "OK", "jpmc-apac-pri Gorouter Latency": "OK", "jpmc-apac-pri PCF Loggregator Firehose Logs being dropped": "OK", "jpmc-apac-pri MYSQL CF Availability Check": "OK", "jpmc-apac-pri PCF Director Availability Check": "OK", "bny-emea-dr Missing LRP's": "OK", "bny-emea-dr Auctioneer Latency": "OK", "bny-emea-dr BBS Latency": "OK", "bny-emea-dr Bosh System Health Check": "Alert", "us-west-2 Missing LRP's": "OK", "us-west-2 Active Locks": "OK", "us-west-2 Rep Bulk Sync": "OK", "us-west-2 Windows Diego Cell CPU": "OK", "bny-emea-dr Active Locks": "OK", "bny-emea-dr Rep Bulk Sync": "OK", "jpmc-emea-pri Missing LRP's": "OK", "jpmc-emea-pri Active Locks": "OK", "jpmc-emea-pri Rep Bulk Sync": "OK", "jpmc-nam-sec Missing LRP's": "OK", "jpmc-nam-sec Active Locks": "OK", "jpmc-nam-sec Rep Bulk Sync": "OK", "bny-nam-pri Missing LRP's": "OK", "bny-nam-pri Active Locks": "OK", "bny-nam-pri Rep Bulk Sync": "OK", "jpmc-emea-sec Missing LRP's": "OK", "jpmc-emea-sec Active Locks": "OK", "jpmc-emea-sec Rep Bulk Sync": "OK", "jpmc-nam-pri Missing LRP's": "OK", "jpmc-nam-pri Active Locks": "OK", "jpmc-nam-pri Rep Bulk Sync": "OK", "[Synthetics] Synthetic health Check test Failed in : egw-recon-JPMC_AWS_PRODNAM :": "OK", "[Synthetics] egw-tps-jpmc-aws-prodnam : Synthetic_health_Check_for_egw-tps-jpmc-aws-prodnam": "OK", "jpmc-apac-sandbox Missing LRP's": "OK", "jpmc-apac-sandbox Active Locks": "OK", "jpmc-apac-sandbox Rep Bulk Sync": "OK", "Error Budget Alert on SLO: Service streaming-doc-extractor has a high p90 latency on env:jpmc-emea-uat": "Alert", "Error Budget Alert on SLO: Service streaming-doc-extractor has a high error rate on env:aws-schwab-prod": "OK", "bny-emea-dr Remaining Ephermeral Disk CF VM": "OK", "bny-emea-dr Remaining Memory VM": "OK", "bny-emea-dr Remaining Persistent Disk CF VM": "OK", "bny-emea-dr System Disk Usage": "OK", "us-west-2 Remaining Ephermeral Disk CF VM": "OK", "us-west-2 Remaining Memory VM": "OK", "us-west-2 Remaining Persistent Disk CF VM": "OK", "us-west-2 System Disk Usage": "OK", "bny-nam-dr Remaining Ephermeral Disk CF VM": "OK", "bny-nam-dr Remaining Memory VM": "OK", "bny-nam-dr Remaining Persistent Disk CF VM": "OK", "bny-nam-dr System Disk Usage": "OK", "jpmc-nam-sandbox Remaining Ephermeral Disk CF VM": "OK", "jpmc-nam-sandbox Remaining Memory VM": "OK", "jpmc-nam-sandbox Remaining Persistent Disk CF VM": "OK", "jpmc-nam-sandbox System Disk Usage": "OK", "jpmc-emea-sandbox Remaining Ephermeral Disk CF VM": "OK", "jpmc-emea-sandbox Remaining Memory VM": "OK", "jpmc-emea-sandbox Remaining Persistent Disk CF VM": "OK", "jpmc-emea-sandbox System Disk Usage": "OK", "jpmc-apac-sandbox Remaining Ephermeral Disk CF VM": "OK", "jpmc-apac-sandbox Remaining Memory VM": "OK", "jpmc-apac-sandbox Remaining Persistent Disk CF VM": "OK", "jpmc-apac-sandbox System Disk Usage": "Warn", "jpmc-nam-pri Remaining Ephermeral Disk CF VM": "OK", "jpmc-nam-pri Remaining Memory VM": "OK", "jpmc-nam-pri Remaining Persistent Disk CF VM": "OK", "jpmc-nam-pri System Disk Usage": "OK", "jpmc-emea-pri Remaining Ephermeral Disk CF VM": "OK", "jpmc-emea-pri Remaining Memory VM": "OK", "jpmc-emea-pri Remaining Persistent Disk CF VM": "OK", "jpmc-emea-pri System Disk Usage": "OK", "jpmc-emea-sec Remaining Ephermeral Disk CF VM": "OK", "jpmc-emea-sec Remaining Memory VM": "OK", "jpmc-emea-sec Remaining Persistent Disk CF VM": "OK", "jpmc-nam-sec Remaining Ephermeral Disk CF VM": "OK", "jpmc-nam-sec Remaining Memory VM": "OK", "jpmc-nam-sec Remaining Persistent Disk CF VM": "OK", "jpmc-nam-sec System Disk Usage": "OK", "jpmc-emea-sec System Disk Usage": "OK", "bny-nam-pri Remaining Ephermeral Disk CF VM": "OK", "bny-nam-pri Remaining Memory VM": "OK", "bny-nam-pri Remaining Persistent Disk CF VM": "OK", "bny-nam-pri System Disk Usage": "OK", "Kafka Cluster Capacity status: robinhood-nam-pri-kafka": "No Data", "Kafka Cluster Capacity status: mt-eu-pri-kafka": "No Data", "jpmc-apac-pri Missing LRP's": "OK", "jpmc-apac-pri Remaining Ephermeral Disk CF VM": "OK", "jpmc-apac-pri Remaining Memory VM": "OK", "jpmc-apac-pri Remaining Persistent Disk CF VM": "OK", "jpmc-apac-pri Active Locks": "OK", "jpmc-apac-pri Rep Bulk Sync": "OK", "jpmc-apac-pri System Disk Usage": "OK", "jpmc-apac-sec Missing LRP's": "OK", "jpmc-apac-sec Remaining Ephermeral Disk CF VM": "OK", "jpmc-apac-sec Remaining Memory VM": "OK", "jpmc-apac-sec Remaining Persistent Disk CF VM": "OK", "jpmc-apac-sec Active Locks": "OK", "jpmc-apac-sec Rep Bulk Sync": "OK", "jpmc-apac-sec System Disk Usage": "OK", "Kafka Cluster Capacity status: jpmc-nam-aws-pri-kafka": "No Data", "Error Budget Alert on SLO: Service streaming-doc-extractor has a high error rate on env:aws-schwab-uat": "OK", "Error Budget Alert on SLO: Service streaming-doc-extractor has a high error rate on env:jpmc-apac-sandox": "OK", "Error Budget Alert on SLO: Service streaming-doc-extractor has a high p90 latency on env:aws-schwab-prod": "Alert", "Service streaming-doc-extractor has a high p90 latency on env:aws-schwab-uat": "OK", "Error Budget Alert on SLO: Service streaming-doc-extractor has a high p90 latency on env:aws-schwab-uat": "OK", "Error Budget Alert on SLO: Service streaming-doc-extractor has a high p90 latency on env:jpmc-apac-sandox": "Alert", "Error Budget Alert on SLO: Service streaming-doc-extractor has a high p90 latency on env:aws-bny-nam-prod": "Alert", "Error Budget Alert on SLO: Service streaming-doc-extractor has a high p90 latency on env:jpmc-emea-secondary": "OK", "Error Budget Alert on SLO: Service streaming-doc-extractor has a high p90 latency on env:aws-mt-prod": "OK", "Error Budget Alert on SLO: Service ea-idaas has a high error rate on env:aws-schwab-uat": "OK", "Error Budget Alert on SLO: Service ea-idaas has a high error rate on env:aws-schwab-prod": "OK", "Error Budget Alert on SLO: Service ea-idaas has a high error rate on env:aws-demo": "OK", "Error Budget Alert on SLO: Service ea-idaas has a high error rate on env:jpmc-emea-sandbox": "OK", "Error Budget Alert on SLO: Service ea-idaas has a high p90 latency on env:aws-schwab-uat": "OK", "Error Budget Alert on SLO: Service ea-idaas has a high p90 latency on env:aws-schwab-prod": "Alert", "Error Budget Alert on SLO: Service ea-idaas has a high p90 latency on env:bny-emea-prod": "OK", "Error Budget Alert on SLO: Service ea-idaas has a high p90 latency on env:jpmc-apac-sandbox": "OK", "Error Budget Alert on SLO: Service ea-idaas has a high p90 latency on env:jpmc-emea-sandbox": "OK", "Error Budget Alert on SLO: Service ea-idaas has a high error rate on env:jpmc-apac-sandbox": "OK", "us-west-2 Auctioneer Latency": "OK", "us-west-2 BBS Latency": "OK", "us-west-2 UAA Requests": "OK", "bny-emea-dr UAA Requests": "OK", "jpmc-emea-sandbox UAA Requests": "OK", "jpmc-apac-sandbox Auctioneer Latency": "OK", "jpmc-apac-sandbox BBS Latency": "OK", "jpmc-apac-sandbox UAA Requests": "OK", "jpmc-emea-pri Auctioneer Latency": "OK", "jpmc-emea-pri BBS Latency": "OK", "jpmc-emea-pri UAA Requests": "OK", "jpmc-emea-sec Auctioneer Latency": "OK", "jpmc-emea-sec BBS Latency": "OK", "jpmc-emea-sec UAA Requests": "OK", "jpmc-apac-sec Auctioneer Latency": "OK", "jpmc-apac-sec BBS Latency": "OK", "jpmc-apac-sec UAA Requests": "OK", "jpmc-apac-pri Auctioneer Latency": "OK", "jpmc-apac-pri BBS Latency": "OK", "jpmc-apac-pri UAA Requests": "OK", "Low Success Rate for Archive-Api - AWS MT PROD": "OK", "us-west-2 Apps Manager Availability Check": "OK", "us-west-2 CF CLI Availability Check": "OK", "bny-emea-dr Apps Manager Availability Check": "OK", "bny-emea-dr CF CLI Availability Check": "OK", "jpmc-apac-sandbox Apps Manager Availability Check": "OK", "jpmc-apac-sandbox CF CLI Availability Check": "OK", "jpmc-emea-sandbox Apps Manager Availability Check": "OK", "jpmc-emea-sandbox CF CLI Availability Check": "OK", "[Synthetics] Delete Test": "No Data", "jpmc-apac-sec Apps Manager Availability Check": "OK", "jpmc-apac-sec CF CLI Availability Check": "OK", "jpmc-apac-pri Apps Manager Availability Check": "OK", "jpmc-apac-pri CF CLI Availability Check": "OK", "jpmc-emea-pri Apps Manager Availability Check": "OK", "jpmc-emea-pri CF CLI Availability Check": "OK", "jpmc-emea-sec Apps Manager Availability Check": "OK", "jpmc-emea-sec CF CLI Availability Check": "OK", "eu-central-1 Missing LRP's": "OK", "eu-central-1 Apps Manager Availability Check": "OK", "eu-central-1 Auctioneer Latency": "OK", "eu-central-1 BBS Latency": "OK", "eu-central-1 Bosh System Health Check": "OK", "eu-central-1 CF CLI Availability Check": "OK", "eu-central-1 Remaining Ephermeral Disk CF VM": "OK", "eu-central-1 Remaining Memory VM": "OK", "eu-central-1 Remaining Persistent Disk CF VM": "OK", "eu-central-1 Active Locks": "OK", "eu-central-1 Rep Bulk Sync": "OK", "eu-central-1 System Disk Usage": "OK", "eu-central-1 UAA Requests": "OK", "eu-central-1 Windows Diego Cell CPU": "OK", "Cloud Capture Mongo EBS Burst Balance - C3": "OK", "Cloud Capture Mongo EBS Burst Balance - D2": "OK", "Cloud Capture Mongo EBS Burst Balance - C1": "OK", "Cloud Capture Mongo EBS Burst Balance - D1": "OK", "Cloud Capture Mongo EBS Burst Balance - C2": "OK", "Cloud Capture Mongo EBS Burst Balance - D3": "OK", "[Synthetics] Socialite-macquarie2-ELB": "OK", "us-west-2 Gorouter Rejections": "OK", "bny-emea-dr Gorouter Rejections": "OK", "jpmc-apac-sandbox Gorouter Rejections": "OK", "eu-central-1 Gorouter Rejections": "OK", "jpmc-nam-sandbox Apps Manager Availability Check": "OK", "jpmc-nam-sandbox CF CLI Availability Check": "OK", "jpmc-nam-sandbox Gorouter Rejections": "OK", "jpmc-nam-sandbox UAA Requests": "OK", "jpmc-apac-sec Gorouter Rejections": "OK", "jpmc-apac-pri Gorouter Rejections": "OK", "jpmc-nam-pri Apps Manager Availability Check": "OK", "jpmc-nam-pri Auctioneer Latency": "OK", "jpmc-nam-pri BBS Latency": "OK", "jpmc-nam-pri CF CLI Availability Check": "OK", "jpmc-nam-pri Gorouter Rejections": "OK", "jpmc-nam-pri UAA Requests": "OK", "jpmc-nam-sec Apps Manager Availability Check": "OK", "jpmc-nam-sec Auctioneer Latency": "OK", "jpmc-nam-sec BBS Latency": "OK", "jpmc-nam-sec CF CLI Availability Check": "OK", "jpmc-nam-sec Gorouter Rejections": "OK", "jpmc-nam-sec UAA Requests": "OK", "ap-southeast-1 Missing LRP's": "OK", "ap-southeast-1 Apps Manager Availability Check": "OK", "ap-southeast-1 Auctioneer Latency": "OK", "ap-southeast-1 BBS Latency": "OK", "ap-southeast-1 Bosh System Health Check": "OK", "ap-southeast-1 CF CLI Availability Check": "OK", "ap-southeast-1 Remaining Ephermeral Disk CF VM": "OK", "ap-southeast-1 Remaining Memory VM": "OK", "ap-southeast-1 Remaining Persistent Disk CF VM": "OK", "ap-southeast-1 Active Locks": "OK", "ap-southeast-1 Rep Bulk Sync": "OK", "ap-southeast-1 System Disk Usage": "OK", "ap-southeast-1 UAA Requests": "OK", "ap-southeast-1 Windows Diego Cell CPU": "OK", "ap-southeast-1 Gorouter Rejections": "OK", "jpmc-emea-sandbox Gorouter Rejections": "OK", "jpmc-emea-pri Gorouter Rejections": "OK", "jpmc-emea-sec Gorouter Rejections": "OK", "Elasticsearch Cluster Health status - bny-nam-uat-data": "OK", "bny-nam-uat-data: Indexing Anomaly Detection": "OK", "bny-nam-uat-data: Query Anomaly Detection": "OK", "bny-nam-uat-report: Query Anomaly Detection": "OK", "bny-nam-uat-report: Indexing Anomaly Detection": "OK", "Elasticsearch Cluster Health status - bny-nam-uat-report": "OK", "bny-nam-uat-supervision: Query Anomaly Detection": "OK", "Elasticsearch Cluster Health status - bny-nam-uat-supervision": "OK", "bny-nam-uat-supervision: Indexing Anomaly Detection": "OK", "Kafka Cluster Capacity status: bny-nam-uat-kafka": "OK", "Prod RDS storage space": "OK", "ca-central-1 Missing LRP's": "OK", "ca-central-1 Auctioneer Latency": "OK", "ca-central-1 BBS Latency": "OK", "ca-central-1 Bosh System Health Check": "OK", "ca-central-1 CF CLI Availability Check": "OK", "ca-central-1 Remaining Ephermeral Disk CF VM": "OK", "ca-central-1 Remaining Memory VM": "OK", "ca-central-1 Remaining Persistent Disk CF VM": "OK", "ca-central-1 Gorouter Rejections": "OK", "ca-central-1 Active Locks": "OK", "ca-central-1 Rep Bulk Sync": "OK", "ca-central-1 System Disk Usage": "OK", "ca-central-1 UAA Requests": "OK", "ca-central-1 Windows Diego Cell CPU": "OK", "TCP Router Network Bandwidth; Maximum of 10Gbps": "No Data", "bny-nam-pri-data: Indexing Anomaly Detection": "OK", "bny-nam-pri-data: Query Anomaly Detection": "OK", "bny-nam-pri-report: Query Anomaly Detection": "OK", "bny-nam-pri-report: Indexing Anomaly Detection": "OK", "bny-nam-pri-supervision: Query Anomaly Detection": "OK", "bny-nam-pri-supervision: Indexing Anomaly Detection": "OK", "robinhood-nam-pri-data: Query Anomaly Detection": "OK", "robinhood-nam-pri-data: Indexing Anomaly Detection": "OK", "robinhood-nam-pri-report: Query Anomaly Detection": "OK", "robinhood-nam-pri-report: Indexing Anomaly Detection": "OK", "robinhood-nam-pri-supervision: Query Anomaly Detection": "No Data", "Elasticsearch Cluster Health status - robinhood-nam-pri-supervision": "No Data", "robinhood-nam-pri-supervision: Indexing Anomaly Detection": "No Data", "ca-central-1 Apps Manager Availability Check": "OK", "us-east-1 Missing LRP's": "OK", "us-east-1 Apps Manager Availability Check": "OK", "us-east-1 Auctioneer Latency": "OK", "us-east-1 BBS Latency": "OK", "us-east-1 Bosh System Health Check": "OK", "us-east-1 CF CLI Availability Check": "OK", "us-east-1 Remaining Ephermeral Disk CF VM": "OK", "us-east-1 Remaining Memory VM": "OK", "us-east-1 Remaining Persistent Disk CF VM": "OK", "us-east-1 Gorouter Rejections": "OK", "us-east-1 Active Locks": "OK", "us-east-1 Rep Bulk Sync": "OK", "us-east-1 System Disk Usage": "OK", "us-east-1 UAA Requests": "OK", "us-east-1 Windows Diego Cell CPU": "OK", "bny-apac-pri-data: Indexing Anomaly Detection": "OK", "bny-apac-pri-data: Query Anomaly Detection": "OK", "bny-apac-pri-report: Indexing Anomaly Detection": "OK", "bny-apac-pri-report: Query Anomaly Detection": "OK", "bny-emea-pri-kafka: Broker Connectivity": "No Data", "bny-emea-pri-kafka: Offline Partition Count": "No Data", "bny-emea-pri-kafka: Under Replicated Partitions": "No Data", "bny-emea-pri-kafka: TotalTimeMS": "No Data", "bny-emea-pri-kafka: Active Controller Count": "No Data", "bny-emea-pri-kafka: Broker Availability Composite Monitor": "No Data", "jpmc-nam-aws-pri-kafka: Broker Connectivity": "No Data", "jpmc-nam-aws-pri-kafka: Offline Partition Count": "No Data", "jpmc-nam-aws-pri-kafka: Active Controller Count": "No Data", "jpmc-nam-aws-pri-kafka: Under Replicated Partitions": "No Data", "jpmc-nam-aws-pri-kafka: TotalTimeMS": "No Data", "jpmc-nam-aws-pri-kafka: Broker Availability Composite Monitor": "No Data", "robinhood-nam-pri-kafka: Offline Partition Count": "No Data", "robinhood-nam-pri-kafka: Active Controller Count": "No Data", "robinhood-nam-pri-kafka: Under Replicated Partitions": "No Data", "robinhood-nam-pri-kafka: TotalTimeMS": "No Data", "robinhood-nam-pri-kafka: Broker Connectivity": "No Data", "robinhood-nam-pri-kafka: Broker Availability Composite Monitor": "No Data", "bny-nam-pri-kafka: Broker Connectivity": "OK", "bny-nam-pri-kafka: Under Replicated Partitions": "No Data", "bny-nam-pri-kafka: Offline Partition Count": "No Data", "bny-nam-pri-kafka: Active Controller Count": "No Data", "bny-nam-pri-kafka: TotalTimeMS": "No Data", "bny-nam-pri-kafka: Broker Availability Composite Monitor": "No Data", "mt-eu-pri-kafka: Broker Connectivity": "No Data", "mt-eu-pri-kafka: Under Replicated Partitions": "No Data", "mt-eu-pri-kafka: Offline Partition Count": "No Data", "mt-eu-pri-kafka: Active Controller Count": "No Data", "mt-eu-pri-kafka: TotalTimeMS": "No Data", "mt-eu-pri-kafka: Broker Availability Composite Monitor": "No Data", "bny-apac-pri-kafka: Broker Connectivity": "No Data", "bny-apac-pri-kafka: Active Controller Count": "No Data", "bny-apac-pri-kafka: Under Replicated Partitions": "No Data", "bny-apac-pri-kafka: Offline Partition Count": "No Data", "bny-apac-pri-kafka: TotalTimeMS": "No Data", "bny-apac-pri-kafka: Broker Availability Composite Monitor": "No Data", "mt-ca-pri-kafka: Broker Connectivity": "OK", "mt-ca-pri-kafka: Under Replicated Partitions": "No Data", "mt-ca-pri-kafka: Active Controller Count": "No Data", "mt-ca-pri-kafka: TotalTimeMS": "No Data", "mt-ca-pri-kafka: Offline Partition Count": "No Data", "mt-ca-pri-kafka: Broker Availability Composite Monitor": "No Data", "bny-emea-sandbox-kafka: Broker Connectivity": "No Data", "bny-emea-sandbox-kafka: Active Controller Count": "No Data", "bny-emea-sandbox-kafka: Under Replicated Partitions": "No Data", "bny-emea-sandbox-kafka: TotalTimeMS": "No Data", "bny-emea-sandbox-kafka: Offline Partition Count": "No Data", "bny-emea-sandbox-kafka: Broker Availability Composite Monitor": "No Data", "Kafka Cluster Capacity status: rbn-nam-smtp-kafka": "OK", "bny-apac-pri-supervision: Indexing Anomaly Detection": "OK", "bny-apac-pri-supervision: Query Anomaly Detection": "OK", "bny-emea-pri-data: Indexing Anomaly Detection": "Warn", "bny-emea-pri-data: Query Anomaly Detection": "OK", "bny-emea-pri-report: Indexing Anomaly Detection": "Warn", "bny-emea-pri-report: Query Anomaly Detection": "OK", "bny-emea-pri-supervision: Indexing Anomaly Detection": "Warn", "bny-emea-pri-supervision: Query Anomaly Detection": "OK", "mt-eu-pri-data: Indexing Anomaly Detection": "No Data", "Elasticsearch Cluster Health status - mt-eu-pri-data": "No Data", "mt-eu-pri-data: Query Anomaly Detection": "No Data", "mt-eu-pri-report: Query Anomaly Detection": "No Data", "Elasticsearch Cluster Health status - mt-eu-pri-report": "No Data", "mt-eu-pri-report: Indexing Anomaly Detection": "No Data", "mt-eu-pri-supervision: Indexing Anomaly Detection": "No Data", "Elasticsearch Cluster Health status - mt-eu-pri-supervision": "No Data", "mt-eu-pri-supervision: Query Anomaly Detection": "No Data", "bny-nam-dr Apps Manager Availability Check": "OK", "bny-nam-dr CF CLI Availability Check": "OK", "bny-nam-dr Gorouter Rejections": "OK", "bny-nam-dr UAA Requests": "OK", "jpmc-apac-pri-data: Indexing Anomaly Detection": "OK", "jpmc-apac-pri-data: Query Anomaly Detection": "OK", "jpmc-apac-pri-report: Indexing Anomaly Detection": "OK", "jpmc-apac-pri-report: Query Anomaly Detection": "OK", "jpmc-apac-sec-data: Indexing Anomaly Detection": "OK", "jpmc-apac-sec-data: Query Anomaly Detection": "OK", "jpmc-apac-sec-report: Indexing Anomaly Detection": "OK", "jpmc-apac-sec-report: Query Anomaly Detection": "OK", "jpmc-apac-sandbox-data: Indexing Anomaly Detection": "OK", "jpmc-apac-sandbox-data: Query Anomaly Detection": "OK", "jpmc-apac-sandbox-report: Indexing Anomaly Detection": "OK", "jpmc-apac-sandbox-report: Query Anomaly Detection": "OK", "bny-emea-dr TCP Router Received Bandwidth": "OK", "bny-emea-dr TCP Router Sent Bandwidth": "OK", "bny-nam-dr TCP Router Received Bandwidth": "OK", "bny-nam-dr TCP Router Sent Bandwidth": "OK", "jpmc-nam-sandbox TCP Router Received Bandwidth": "OK", "jpmc-nam-sandbox TCP Router Sent Bandwidth": "OK", "jpmc-apac-sandbox TCP Router Received Bandwidth": "OK", "jpmc-apac-sandbox TCP Router Sent Bandwidth": "OK", "bny-nam-pri Apps Manager Availability Check": "OK", "bny-nam-pri Auctioneer Latency": "OK", "jpmc-emea-sandbox TCP Router Received Bandwidth": "OK", "bny-nam-pri BBS Latency": "OK", "jpmc-emea-sandbox TCP Router Sent Bandwidth": "OK", "bny-nam-pri CF CLI Availability Check": "OK", "bny-nam-pri Gorouter Rejections": "OK", "bny-nam-pri UAA Requests": "OK", "jpmc-nam-pri TCP Router Received Bandwidth": "OK", "jpmc-nam-pri TCP Router Sent Bandwidth": "OK", "jpmc-apac-sec TCP Router Received Bandwidth": "OK", "jpmc-apac-sec TCP Router Sent Bandwidth": "OK", "bny-nam-pri TCP Router Received Bandwidth": "OK", "bny-nam-pri TCP Router Sent Bandwidth": "OK", "jpmc-nam-sec TCP Router Received Bandwidth": "OK", "jpmc-nam-sec TCP Router Sent Bandwidth": "OK", "jpmc-emea-pri TCP Router Received Bandwidth": "OK", "jpmc-emea-pri TCP Router Sent Bandwidth": "OK", "jpmc-apac-pri TCP Router Received Bandwidth": "OK", "jpmc-apac-pri TCP Router Sent Bandwidth": "OK", "jpmc-emea-sec TCP Router Received Bandwidth": "OK", "jpmc-emea-sec TCP Router Sent Bandwidth": "OK", "jpmc-emea-pri-data: Indexing Anomaly Detection": "OK", "jpmc-emea-pri-data: Query Anomaly Detection": "OK", "jpmc-emea-pri-report: Indexing Anomaly Detection": "OK", "jpmc-emea-pri-report: Query Anomaly Detection": "OK", "jpmc-emea-sec-data: Indexing Anomaly Detection": "Warn", "jpmc-emea-sec-data: Query Anomaly Detection": "Warn", "jpmc-emea-sec-report: Query Anomaly Detection": "OK", "jpmc-emea-sec-report: Indexing Anomaly Detection": "Warn", "jpmc-emea-sandbox-data: Query Anomaly Detection": "OK", "jpmc-emea-sandbox-data: Indexing Anomaly Detection": "OK", "bny-emea-dr Diego Container Count": "OK", "bny-nam-dr Diego Container Count": "OK", "bny-nam-pri Diego Container Count": "OK", "mt-ca-pri-data: Query Anomaly Detection": "OK", "mt-ca-pri-data: Indexing Anomaly Detection": "OK", "mt-ca-pri-report: Query Anomaly Detection": "OK", "mt-ca-pri-report: Indexing Anomaly Detection": "OK", "mt-ca-pri-supervision: Indexing Anomaly Detection": "OK", "mt-ca-pri-supervision: Query Anomaly Detection": "OK", "jpmc-pri-nam-data: Indexing Anomaly Detection": "OK", "jpmc-pri-nam-data: Query Anomaly Detection": "OK", "jpmc-pri-nam-report: Query Anomaly Detection": "OK", "jpmc-pri-nam-report: Indexing Anomaly Detection": "OK", "jpmc-nam-sec-data: Query Anomaly Detection": "OK", "jpmc-nam-sec-data: Indexing Anomaly Detection": "OK", "jpmc-nam-sec-report: Indexing Anomaly Detection": "OK", "jpmc-nam-sec-report: Query Anomaly Detection": "OK", "Archive Export Topology worker count is 0": "OK", "jpmc-sandbox-data: Indexing Anomaly Detection": "OK", "jpmc-sandbox-data: Query Anomaly Detection": "OK", "jpmc-sandbox-report: Query Anomaly Detection": "OK", "jpmc-sandbox-report: Indexing Anomaly Detection": "OK", "Worker is crashing in {{env.name}}": "Alert", "Elasticsearch Cluster Health status - csuisse-nam-aws-poc-data": "OK", "csuisse-nam-aws-poc-data: Query Anomaly Detection": "OK", "csuisse-nam-aws-poc-data: Indexing Anomaly Detection": "OK", "csuisse-nam-aws-poc-report: Indexing Anomaly Detection": "OK", "Elasticsearch Cluster Health status - csuisse-nam-aws-poc-report": "OK", "csuisse-nam-aws-poc-report: Query Anomaly Detection": "OK", "Elasticsearch Cluster Health status - csuisse-nam-aws-poc-supervision": "OK", "csuisse-nam-aws-poc-supervision: Indexing Anomaly Detection": "OK", "csuisse-nam-aws-poc-supervision: Query Anomaly Detection": "OK", "mt-prod-supervision: Indexing Anomaly Detection": "No Data", "mt-prod-supervision: Query Anomaly Detection": "No Data", "Elasticsearch Cluster Health status - mt-prod-supervision": "No Data", "csuisse-nam-aws-poc-kafka: Broker Connectivity": "OK", "Kafka Cluster Capacity status: csuisse-nam-aws-poc-kafka": "OK", "csuisse-nam-aws-poc-kafka: Active Controller Count": "OK", "csuisse-nam-aws-poc-kafka: Under Replicated Partitions": "OK", "csuisse-nam-aws-poc-kafka: Offline Partition Count": "OK", "csuisse-nam-aws-poc-kafka: TotalTimeMS": "OK", "csuisse-nam-aws-poc-kafka: Broker Availability Composite Monitor": "No Data", "demo-essdata: Query Anomaly Detection": "No Data", "Elasticsearch Cluster Health status - demo-essdata": "No Data", "demo-essdata: Indexing Anomaly Detection": "No Data", "demo-essreport: Query Anomaly Detection": "No Data", "Elasticsearch Cluster Health status - demo-essreport": "No Data", "demo-essreport: Indexing Anomaly Detection": "No Data", "Elasticsearch Cluster Health status - mt-prod-essreport": "No Data", "mt-prod-essreport: Query Anomaly Detection": "No Data", "mt-prod-essreport: Indexing Anomaly Detection": "No Data", "schwab-uat-essdata: Indexing Anomaly Detection": "No Data", "Elasticsearch Cluster Health status - schwab-uat-essdata": "No Data", "schwab-uat-essdata: Query Anomaly Detection": "No Data", "Elasticsearch Cluster Health status - schwab-prod-essreport": "No Data", "schwab-prod-essreport: Query Anomaly Detection": "No Data", "schwab-prod-essreport: Indexing Anomaly Detection": "No Data", "mt-prod-essdata: Query Anomaly Detection": "No Data", "mt-prod-essdata: Indexing Anomaly Detection": "No Data", "Elasticsearch Cluster Health status - mt-prod-essdata": "No Data", "schwab-prod-essdata: Query Anomaly Detection": "No Data", "Elasticsearch Cluster Health status - schwab-prod-essdata": "No Data", "schwab-prod-essdata: Indexing Anomaly Detection": "No Data", "bny-emea-pri-site: Replciation Lag (testing)": "OK", "bny-emea-pri-site: Replicaset Health (testing)": "OK", "bny-emea-pri-site: Host Connectivity": "OK", "bny-emea-pri-site: Read Tickets (testing)": "OK", "bny-emea-pri-site: Write Tickets (testing)": "OK", "bny-emea-pri-site: Concurrent Client Connections (testing)": "OK", "jpmc-emea-sandbox Diego Container Count": "OK", "jpmc-nam-sandbox Diego Container Count": "OK", "Example Monitor for API Test": "OK", "jpmc-apac-sandbox Diego Container Count": "OK", "Failure Count is high from last 30 mins": "OK", "eDiscovery Export Topology worker count is 0": "OK", "MsgAndNonEmail Export Topology worker count is 0": "OK", "JPMC EMEA Secondary | Error rate for last 5 mins is increasing for ca-archive-api": "Alert", "Supervision Export Topology worker count is 0": "OK", "Export-All Export Topology worker count is 0": "Alert", "jpmc-nam-pri Diego Container Count": "OK", "jpmc-nam-pri Gorouter Received Bandwidth": "OK", "jpmc-nam-pri Gorouter Sent Bandwidth": "OK", "us-west-2 Diego Container Count": "OK", "us-west-2 Gorouter Received Bandwidth": "OK", "us-west-2 Gorouter Sent Bandwidth": "OK", "us-west-2 TCP Router Received Bandwidth": "OK", "us-west-2 TCP Router Sent Bandwidth": "OK", "jpmc-emea-pri Diego Container Count": "OK", "jpmc-emea-pri Gorouter Received Bandwidth": "OK", "jpmc-emea-pri Gorouter Sent Bandwidth": "OK", "jpmc-nam-sec Diego Container Count": "OK", "jpmc-nam-sec Gorouter Received Bandwidth": "OK", "jpmc-nam-sec Gorouter Sent Bandwidth": "OK", "jpmc-emea-sec Diego Container Count": "OK", "jpmc-emea-sec Gorouter Received Bandwidth": "OK", "jpmc-emea-sec Gorouter Sent Bandwidth": "OK", "bny-nam-dr Garden Health Check": "OK", "bny-nam-dr Gorouter Received Bandwidth": "OK", "bny-nam-dr Gorouter Sent Bandwidth": "OK", "jpmc-apac-sandbox Garden Health Check": "OK", "jpmc-apac-sandbox Gorouter Received Bandwidth": "OK", "jpmc-apac-sandbox Gorouter Sent Bandwidth": "OK", "us-west-2 Garden Health Check": "OK", "jpmc-emea-sandbox Garden Health Check": "OK", "jpmc-emea-sandbox Gorouter Received Bandwidth": "OK", "jpmc-emea-sandbox Gorouter Sent Bandwidth": "OK", "jpmc-apac-pri Diego Container Count": "OK", "jpmc-apac-pri Gorouter Received Bandwidth": "OK", "jpmc-apac-pri Gorouter Sent Bandwidth": "OK", "jpmc-nam-sandbox Garden Health Check": "OK", "jpmc-nam-sandbox Gorouter Received Bandwidth": "OK", "jpmc-nam-sandbox Gorouter Sent Bandwidth": "OK", "bny-emea-dr Garden Health Check": "OK", "bny-emea-dr Gorouter Received Bandwidth": "OK", "bny-emea-dr Gorouter Sent Bandwidth": "OK", "jpmc-apac-sec Diego Container Count": "OK", "jpmc-apac-sec Gorouter Received Bandwidth": "OK", "jpmc-apac-sec Gorouter Sent Bandwidth": "OK", "jpmc-nam-pri Garden Health Check": "OK", "jpmc-apac-pri Garden Health Check": "OK", "ca-central-1 Diego Container Count": "OK", "ca-central-1 Gorouter Received Bandwidth": "OK", "ca-central-1 Gorouter Sent Bandwidth": "OK", "ca-central-1 TCP Router Received Bandwidth": "OK", "ca-central-1 TCP Router Sent Bandwidth": "OK", "jpmc-apac-sec Garden Health Check": "OK", "us-east-1 Diego Container Count": "OK", "us-east-1 Gorouter Received Bandwidth": "OK", "us-east-1 Gorouter Sent Bandwidth": "OK", "us-east-1 TCP Router Received Bandwidth": "OK", "us-east-1 TCP Router Sent Bandwidth": "OK", "ca-central-1 Garden Health Check": "OK", "us-east-1 Garden Health Check": "OK", "jpmc-nam-sec Garden Health Check": "OK", "eu-central-1 Diego Container Count": "OK", "eu-central-1 Gorouter Received Bandwidth": "OK", "eu-central-1 Gorouter Sent Bandwidth": "OK", "eu-central-1 TCP Router Received Bandwidth": "OK", "eu-central-1 TCP Router Sent Bandwidth": "OK", "jpmc-emea-pri Garden Health Check": "OK", "jpmc-emea-sec Garden Health Check": "OK", "eu-central-1 Garden Health Check": "OK", "bny-nam-pri Garden Health Check": "OK", "bny-nam-pri Gorouter Received Bandwidth": "OK", "bny-nam-pri Gorouter Sent Bandwidth": "OK", "CPU Utilization: CPU Load is very High": "OK", "Service perf-snowball-emailextractor has a high error rate on env:egw-perf": "OK", "Service perf-snowball-emailextractor has an abnormal change in Apdex on env:egw-perf": "OK", "Service perf-snowball-emailextractor has a high average latency on env:egw-perf": "OK", "Service perf-snowball-emailextractor has a high p90 latency on env:egw-perf": "OK", "Service perf-snowball-emailextractor has an abnormal change in throughput on env:egw-perf": "OK", "[Synthetics] [EA] UI Portal - ArchiveManagement Smoke test": "Alert", "[Synthetics] [EA] UI Portal - Supervision Smoke test": "OK", "[Synthetics] [EA] UI Portal - Case management Smoke test": "No Data", "[Synthetics] Synthetic health Check test Failed in : egw-recon-BNY_AWS_EMEA :": "OK", "ap-southeast-1 Diego Container Count": "OK", "ap-southeast-1 Garden Health Check": "OK", "ap-southeast-1 Gorouter Received Bandwidth": "OK", "ap-southeast-1 Gorouter Sent Bandwidth": "OK", "ap-southeast-1 TCP Router Received Bandwidth": "OK", "ap-southeast-1 TCP Router Sent Bandwidth": "OK", "[Synthetics] [ProArchive] ProvisioningService - QA": "OK", "Monitor Name: Unverified Documents for Tenant [{{tenant}}] on [{{space_name}}] space": "Alert", "[Synthetics] test dd-synthetics on js": "No Data", "concourse-mongodb-shared-example: Replicaset Health": "OK", "concourse-mongodb-shared-example: Host Connectivity": "OK", "concourse-mongodb-shared-example: Write Tickets": "OK", "concourse-mongodb-shared-example: Read Tickets": "OK", "concourse-mongodb-shared-example: Concurrent Client Connections": "OK", "concourse-mongodb-shared-example: Query Availability Composite Monitor": "OK", "concourse-mongodb-shared-example: Replciation Lag": "No Data", "concourse-mongodb-shared-example: Data Availability Composite Monitor": "No Data", "concourse-mongodb-site-example: Read Tickets": "OK", "concourse-mongodb-site-example: Host Connectivity": "OK", "concourse-mongodb-site-example: Write Tickets": "OK", "concourse-mongodb-site-example: Concurrent Client Connections": "OK", "concourse-mongodb-site-example: Replciation Lag": "No Data", "concourse-mongodb-site-example: Query Availability Composite Monitor": "OK", "concourse-mongodb-site-example: Replicaset Health": "OK", "concourse-mongodb-site-example: Data Availability Composite Monitor": "No Data", "MinIO Cluster Disks Offline - concourse-example-minio": "OK", "MinIO Cluster Nodes Offline - concourse-example-minio": "OK", "Kafka active controller count for bny-apac-sandbox-kafka": "No Data", "Kafka has under replicated partitions in bny-apac-sandbox-kafka": "No Data", "Kafka has an offline partition in bny-apac-sandbox-kafka": "No Data", "Kafka made an unclean leader election in bny-apac-sandbox-kafka": "No Data", "Kafka has under replicated partitions in bny-nam-uat-kafka": "OK", "Kafka has an offline partition in bny-nam-uat-kafka": "OK", "Kafka made an unclean leader election in bny-nam-uat-kafka": "OK", "Kafka active controller count for bny-nam-uat-kafka": "OK", "rbn-nam-smtp-kafka: Broker Connectivity": "OK", "rbn-nam-smtp-kafka: Offline Partition Count": "OK", "rbn-nam-smtp-kafka: Under Replicated Partitions": "OK", "rbn-nam-smtp-kafka: Active Controller Count": "OK", "rbn-nam-smtp-kafka: TotalTimeMS": "OK", "rbn-nam-smtp-kafka: Broker Availability Composite Monitor": "No Data", "[Synthetics] Test on ea-ingestion-jobs-app-dev2.apps.prod.smarsh.cloud/actuator/health": "OK", "bny-emea-dr Swap Usage VM": "Alert", "bny-emea-dr Swap Usage Diego Cell": "OK", "bny-nam-dr Swap Usage VM": "Alert", "bny-nam-dr Swap Usage Diego Cell": "OK", "jpmc-nam-sandbox Swap Usage VM": "Alert", "jpmc-nam-sandbox Swap Usage Diego Cell": "OK", "us-west-2 Swap Usage VM": "OK", "us-west-2 Swap Usage Diego Cell": "OK", "jpmc-emea-sandbox Swap Usage VM": "OK", "jpmc-emea-sandbox Swap Usage Diego Cell": "OK", "jpmc-apac-sandbox Swap Usage VM": "Alert", "jpmc-apac-sandbox Swap Usage Diego Cell": "OK", "bny-nam-pri Swap Usage VM": "Alert", "bny-nam-pri Swap Usage Diego Cell": "OK", "jpmc-emea-pri Swap Usage VM": "Alert", "jpmc-emea-pri Swap Usage Diego Cell": "OK", "ca-central-1 Swap Usage VM": "OK", "ca-central-1 Swap Usage Diego Cell": "OK", "ap-southeast-1 Swap Usage VM": "OK", "ap-southeast-1 Swap Usage Diego Cell": "OK", "eu-central-1 Swap Usage VM": "Alert", "eu-central-1 Swap Usage Diego Cell": "OK", "us-east-1 Swap Usage VM": "OK", "us-east-1 Swap Usage Diego Cell": "OK", "jpmc-apac-pri Swap Usage VM": "OK", "jpmc-apac-pri Swap Usage Diego Cell": "OK", "jpmc-emea-sec Swap Usage VM": "OK", "jpmc-emea-sec Swap Usage Diego Cell": "OK", "robinhood-nam-pri-shared: Host Connectivity": "OK", "robinhood-nam-pri-shared: Replicaset Health": "OK", "robinhood-nam-pri-shared: Write Tickets": "OK", "robinhood-nam-pri-shared: Read Tickets": "OK", "robinhood-nam-pri-shared: Replciation Lag": "No Data", "robinhood-nam-pri-shared: Concurrent Client Connections": "OK", "robinhood-nam-pri-shared: Data Availability Composite Monitor": "No Data", "robinhood-nam-pri-shared: Query Availability Composite Monitor": "OK", "robinhood-nam-pri-site: Host Connectivity": "OK", "robinhood-nam-pri-site: Replicaset Health": "OK", "robinhood-nam-pri-site: Read Tickets": "OK", "robinhood-nam-pri-site: Write Tickets": "OK", "robinhood-nam-pri-site: Replciation Lag": "No Data", "robinhood-nam-pri-site: Concurrent Client Connections": "OK", "robinhood-nam-pri-site: Data Availability Composite Monitor": "No Data", "robinhood-nam-pri-site: Query Availability Composite Monitor": "OK", "bny-nam-pri-shared: Replicaset Health": "OK", "bny-nam-pri-shared: Host Connectivity": "OK", "bny-nam-pri-shared: Write Tickets": "OK", "bny-nam-pri-shared: Read Tickets": "OK", "bny-nam-pri-shared: Concurrent Client Connections": "OK", "bny-nam-pri-shared: Replciation Lag": "No Data", "bny-nam-pri-shared: Query Availability Composite Monitor": "OK", "bny-nam-pri-shared: Data Availability Composite Monitor": "No Data", "bny-nam-pri-site: Replicaset Health": "OK", "bny-nam-pri-site: Read Tickets": "OK", "bny-nam-pri-site: Host Connectivity": "OK", "bny-nam-pri-site: Write Tickets": "OK", "bny-nam-pri-site: Replciation Lag": "No Data", "bny-nam-pri-site: Concurrent Client Connections": "OK", "bny-nam-pri-site: Data Availability Composite Monitor": "No Data", "bny-nam-pri-site: Query Availability Composite Monitor": "OK", "bny-nam-uat-shared: Host Connectivity": "OK", "bny-nam-uat-shared: Replicaset Health": "OK", "bny-nam-uat-shared: Read Tickets": "OK", "bny-nam-uat-shared: Write Tickets": "OK", "bny-nam-uat-shared: Concurrent Client Connections": "OK", "bny-nam-uat-shared: Replciation Lag": "No Data", "bny-nam-uat-shared: Query Availability Composite Monitor": "OK", "bny-nam-uat-shared: Data Availability Composite Monitor": "No Data", "bny-nam-uat-site: Replicaset Health": "OK", "bny-nam-uat-site: Host Connectivity": "OK", "bny-nam-uat-site: Read Tickets": "OK", "bny-nam-uat-site: Write Tickets": "OK", "bny-nam-uat-site: Concurrent Client Connections": "OK", "bny-nam-uat-site: Query Availability Composite Monitor": "OK", "bny-nam-uat-site: Replciation Lag": "No Data", "bny-nam-uat-site: Data Availability Composite Monitor": "No Data", "bny-apac-pri-shared: Host Connectivity": "OK", "bny-apac-pri-shared: Replicaset Health": "OK", "bny-apac-pri-shared: Read Tickets": "OK", "bny-apac-pri-shared: Write Tickets": "OK", "bny-apac-pri-shared: Concurrent Client Connections": "OK", "bny-apac-pri-shared: Replciation Lag": "OK", "bny-apac-pri-shared: Query Availability Composite Monitor": "OK", "bny-apac-pri-shared: Data Availability Composite Monitor": "No Data", "bny-apac-pri-site: Replicaset Health": "OK", "bny-apac-pri-site: Write Tickets": "OK", "bny-apac-pri-site: Host Connectivity": "OK", "bny-apac-pri-site: Read Tickets": "OK", "bny-apac-pri-site: Concurrent Client Connections": "OK", "bny-apac-pri-site: Replciation Lag": "OK", "bny-apac-pri-site: Query Availability Composite Monitor": "OK", "bny-apac-pri-site: Data Availability Composite Monitor": "No Data", "bny-apac-sandbox-shared: Host Connectivity": "OK", "bny-apac-sandbox-shared: Replicaset Health": "OK", "bny-apac-sandbox-shared: Read Tickets": "OK", "bny-apac-sandbox-shared: Write Tickets": "OK", "bny-apac-sandbox-shared: Concurrent Client Connections": "OK", "bny-apac-sandbox-shared: Replciation Lag": "OK", "bny-apac-sandbox-shared: Query Availability Composite Monitor": "OK", "bny-apac-sandbox-shared: Data Availability Composite Monitor": "No Data", "bny-apac-sandbox-site: Host Connectivity": "OK", "bny-apac-sandbox-site: Replicaset Health": "OK", "bny-apac-sandbox-site: Read Tickets": "OK", "bny-apac-sandbox-site: Write Tickets": "OK", "bny-apac-sandbox-site: Concurrent Client Connections": "OK", "bny-apac-sandbox-site: Replciation Lag": "Warn", "bny-apac-sandbox-site: Query Availability Composite Monitor": "OK", "bny-apac-sandbox-site: Data Availability Composite Monitor": "No Data", "bny-emea-pri-shared: Host Connectivity": "OK", "bny-emea-pri-shared: Replicaset Health": "OK", "bny-emea-pri-shared: Write Tickets": "OK", "bny-emea-pri-shared: Read Tickets": "OK", "bny-emea-pri-shared: Replciation Lag": "OK", "bny-emea-pri-shared: Concurrent Client Connections": "OK", "bny-emea-pri-shared: Data Availability Composite Monitor": "No Data", "bny-emea-pri-shared: Query Availability Composite Monitor": "OK", "bny-emea-pri-site: Replicaset Health": "OK", "bny-emea-pri-site: Read Tickets": "OK", "bny-emea-pri-site: Write Tickets": "OK", "bny-emea-pri-site: Concurrent Client Connections": "OK", "bny-emea-pri-site: Replciation Lag": "OK", "bny-emea-pri-site: Query Availability Composite Monitor": "OK", "bny-emea-pri-site: Data Availability Composite Monitor": "No Data", "Cloud Capture Deletes throughput Monitor - CC-PROD_MONGODB_SITE_RTRA": "OK", "Cloud Capture Mongo Replication Lag Monitor - CC-PROD_MONGODB_SITE_RTRA": "OK", "Cloud Capture Inserts throughput Monitor - CC-PROD_MONGODB_SITE_RTRA": "OK", "Cloud Capture Mongo EBS Burst Balance Monitor - CC-PROD_MONGODB_SITE_RTRA": "OK", "Cloud Capture Mongo CPU Utilization Monitor - CC-PROD_MONGODB_SITE_RTRA": "No Data", "Cloud Capture Query throuput Monitor - CC-PROD_MONGODB_SITE_RTRA": "OK", "Cloud Capture Updates throughput Monitor - CC-PROD_MONGODB_SITE_RTRA": "OK", "Cloud Capture GetMore throuput Monitor - CC-PROD_MONGODB_SITE_RTRA": "OK", "[Synthetics] aws-dev2 : ca-archive-api synthetic health check": "OK", "[Synthetics] aws-mt-prod : ca-archive-api synthetic health check": "OK", "bny-emea-sandbox-shared: Replicaset Health": "OK", "bny-emea-sandbox-shared: Host Connectivity": "OK", "bny-emea-sandbox-shared: Read Tickets": "OK", "bny-emea-sandbox-shared: Write Tickets": "OK", "bny-emea-sandbox-shared: Replciation Lag": "OK", "bny-emea-sandbox-shared: Data Availability Composite Monitor": "No Data", "bny-emea-sandbox-shared: Concurrent Client Connections": "OK", "bny-emea-sandbox-shared: Query Availability Composite Monitor": "OK", "bny-emea-sandbox-site: Replicaset Health": "OK", "bny-emea-sandbox-site: Host Connectivity": "OK", "bny-emea-sandbox-site: Write Tickets": "OK", "bny-emea-sandbox-site: Read Tickets": "OK", "bny-emea-sandbox-site: Concurrent Client Connections": "OK", "bny-emea-sandbox-site: Replciation Lag": "OK", "bny-emea-sandbox-site: Query Availability Composite Monitor": "OK", "bny-emea-sandbox-site: Data Availability Composite Monitor": "No Data", "jpmc-apac-sec Swap Usage VM": "OK", "jpmc-apac-sec Swap Usage Diego Cell": "OK", "mt-eu-pri-shared: Write Tickets": "OK", "mt-eu-pri-shared: Replicaset Health": "OK", "mt-eu-pri-shared: Read Tickets": "OK", "mt-eu-pri-shared: Replciation Lag": "No Data", "mt-eu-pri-shared: Host Connectivity": "OK", "mt-eu-pri-shared: Concurrent Client Connections": "OK", "mt-eu-pri-shared: Data Availability Composite Monitor": "No Data", "mt-eu-pri-shared: Query Availability Composite Monitor": "OK", "mt-eu-pri-site: Host Connectivity": "OK", "mt-eu-pri-site: Read Tickets": "OK", "mt-eu-pri-site: Replicaset Health": "OK", "mt-eu-pri-site: Write Tickets": "OK", "mt-eu-pri-site: Concurrent Client Connections": "OK", "mt-eu-pri-site: Query Availability Composite Monitor": "OK", "mt-eu-pri-site: Replciation Lag": "No Data", "mt-eu-pri-site: Data Availability Composite Monitor": "No Data", "jpmc-nam-aws-pri-shared: Host Connectivity": "OK", "jpmc-nam-aws-pri-shared: Replicaset Health": "OK", "jpmc-nam-aws-pri-shared: Concurrent Client Connections": "OK", "jpmc-nam-aws-pri-shared: Write Tickets": "OK", "jpmc-nam-aws-pri-shared: Read Tickets": "OK", "jpmc-nam-aws-pri-shared: Replciation Lag": "No Data", "jpmc-nam-aws-pri-shared: Query Availability Composite Monitor": "OK", "jpmc-nam-aws-pri-shared: Data Availability Composite Monitor": "No Data", "swez Remaining Disk Diego Dells": "OK", "swez Remaining Memory Diego Cells": "OK", "swez Gorouter Latency": "OK", "swez PCF Loggregator Firehose Logs being dropped": "OK", "swez PCF Director Availability Check": "OK", "jpmc-nam-aws-pri-site: Host Connectivity": "OK", "jpmc-nam-aws-pri-site: Replicaset Health": "OK", "jpmc-nam-aws-pri-site: Write Tickets": "OK", "jpmc-nam-aws-pri-site: Read Tickets": "OK", "jpmc-nam-aws-pri-site: Replciation Lag": "No Data", "jpmc-nam-aws-pri-site: Concurrent Client Connections": "OK", "jpmc-nam-aws-pri-site: Data Availability Composite Monitor": "No Data", "jpmc-nam-aws-pri-site: Query Availability Composite Monitor": "OK", "mt-ca-pri-shared: Replicaset Health": "OK", "mt-ca-pri-shared: Host Connectivity": "OK", "mt-ca-pri-shared: Read Tickets": "OK", "mt-ca-pri-shared: Write Tickets": "OK", "mt-ca-pri-shared: Concurrent Client Connections": "OK", "mt-ca-pri-shared: Replciation Lag": "OK", "mt-ca-pri-shared: Query Availability Composite Monitor": "OK", "mt-ca-pri-shared: Data Availability Composite Monitor": "No Data", "mt-ca-pri-site: Host Connectivity": "OK", "mt-ca-pri-site: Read Tickets": "OK", "mt-ca-pri-site: Write Tickets": "OK", "mt-ca-pri-site: Replicaset Health": "OK", "mt-ca-pri-site: Concurrent Client Connections": "OK", "mt-ca-pri-site: Query Availability Composite Monitor": "OK", "mt-ca-pri-site: Replciation Lag": "OK", "mt-ca-pri-site: Data Availability Composite Monitor": "No Data", "[Synthetics] mt-prod : Test on ediscovery-job-app": "OK", "[Synthetics] schwab-prod : Test on ediscovery-job-app": "OK", "[Synthetics] aws-demo : Test on ediscovery-job-app": "OK", "[Synthetics] bnynam : Test on ediscovery-job-app": "OK", "[Synthetics] schwab-uat : Test on ediscovery-job-app": "OK", "jpmc-nam-sec Swap Usage VM": "Alert", "jpmc-nam-sec Swap Usage Diego Cell": "OK", "AWS MT PROD Email ingestion Pipeline Lag is high since 1 hour": "OK", "AWS JPMC NAM PROD Email ingestion Pipeline Lag is high since 1 hour": "Alert", "AWS MT CA CENTRAL PROD Email ingestion Pipeline Lag is high since 1 hour": "OK", "AWS BNY EU CENTRAL PROD Email ingestion Pipeline Lag is high since 1 hour": "OK", "AWS BNY NAM PROD Email ingestion Pipeline Lag is high since 1 hour": "OK", "AWS SCHWAB UAT Email ingestion Pipeline Lag is high since 1 hour": "OK", "AWS SCHWAB PROD Email ingestion Pipeline Lag is high since 1 hour": "OK", "AWS BNY AP SOUTHEAST PROD Email ingestion Pipeline Lag is high since 1 hour": "OK"}